"""
ü¶ô Ollama AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤ –ø–µ—Å–µ–Ω (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–ª–∞–±—ã—Ö –ü–ö)

–û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:
- –õ–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏ (qwen2.5:1.5b, phi3:mini)
- –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø—Ä–æ–º–ø—Ç—ã
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
- –ë–∞—Ç—á–∏–Ω–≥ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
- –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥ –∂–µ–ª–µ–∑–æ

–ù–û–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –≤–∞—à–µ–≥–æ –ü–ö
- –†–µ–∂–∏–º "—ç–∫–æ–Ω–æ–º" —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ–º —Ä–µ—Å—É—Ä—Å–æ–≤
- –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–µ

–ê–í–¢–û–†: AI Assistant (Optimized Version)
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""
import json
import time
import logging
import requests
import psutil
import threading
from typing import Dict, Any, List, Optional
from datetime import datetime

from interfaces.analyzer_interface import BaseAnalyzer, AnalysisResult, register_analyzer

logger = logging.getLogger(__name__)


@register_analyzer("ollama_optimized")
class OptimizedOllamaAnalyzer(BaseAnalyzer):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞ –±–∞–∑–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama.
    
    –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Å–ª–∞–±—ã—Ö –ü–ö:
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏ (1.5B-3B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
    - –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ –Ω–∞–≥—Ä—É–∑–∫—É —Å–∏—Å—Ç–µ–º—ã
    - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–µ–∂–∏–º —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
    """
    
    # –õ–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è (–æ—Ç —Å–∞–º–æ–π –ª–µ–≥–∫–æ–π –∫ –±–æ–ª–µ–µ —Ç—è–∂–µ–ª–æ–π)
    LIGHTWEIGHT_MODELS = [
        "qwen2.5:1.5b",     # –°–∞–º–∞—è –ª–µ–≥–∫–∞—è, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è
        "phi3:mini",        # Microsoft, –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è  
        "llama3.2:1b",      # –£–ª—å—Ç—Ä–∞-–ª–µ–≥–∫–∞—è Meta
        "gemma2:2b",        # Google, –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è
        "llama3.2:3b",      # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
    ]
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ Ollama –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        super().__init__(config)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.economy_mode = self.config.get('economy_mode', True)
        self.max_cpu_usage = self.config.get('max_cpu_usage', 70)  # –ú–∞–∫—Å–∏–º—É–º 70% CPU
        self.base_url = self.config.get('base_url', 'http://localhost:11434')
        self.timeout = self.config.get('timeout', 30)  # –°–æ–∫—Ä–∞—â–µ–Ω —Å 60 –¥–æ 30 —Å–µ–∫
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–±—É–¥—É—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        self.model_name = None
        self.temperature = 0.0  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.max_tokens = 800   # –°–æ–∫—Ä–∞—â–µ–Ω–æ —Å 1500
        self.context_window = 2048  # –°–æ–∫—Ä–∞—â–µ–Ω–æ —Å 4096
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
        self.resource_monitor = ResourceMonitor()
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        self.available = self._initialize_best_model()
        
        if self.available:
            logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Ollama –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤: {self.model_name}")
            logger.info(f"üîß –†–µ–∂–∏–º —ç–∫–æ–Ω–æ–º–∏–∏: {self.economy_mode}, –º–∞–∫—Å CPU: {self.max_cpu_usage}%")
        else:
            logger.warning("‚ö†Ô∏è –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Ollama –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    def _initialize_best_model(self) -> bool:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ü–ö"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
            if not self._check_ollama_server():
                return False
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            system_info = self._get_system_specs()
            logger.info(f"üíª –°–∏—Å—Ç–µ–º–∞: RAM {system_info['ram_gb']:.1f}GB, CPU {system_info['cpu_percent']:.1f}%")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            installed_models = self._get_installed_models()
            logger.info(f"üì¶ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: {installed_models}")
            
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
            best_model = self._select_optimal_model(system_info, installed_models)
            
            if best_model:
                self.model_name = best_model
                self._adjust_settings_for_model(best_model, system_info)
                
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                if self._test_model_performance():
                    logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {self.model_name}")
                    return True
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ, –ø—ã—Ç–∞–µ–º—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å
            return self._install_lightweight_model()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            return False
    
    def _check_ollama_server(self) -> bool:
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            response = requests.get(
                f"{self.base_url}/api/tags", 
                timeout=3,  # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                proxies={"http": "", "https": ""}
            )
            return response.status_code == 200
        except:
            logger.warning("‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve")
            return False
    
    def _get_system_specs(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            # –ü–∞–º—è—Ç—å
            memory = psutil.virtual_memory()
            ram_gb = memory.total / (1024**3)
            
            # –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä
            cpu_percent = psutil.cpu_percent(interval=1)
            cpu_cores = psutil.cpu_count()
            
            # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            cpu_freq = psutil.cpu_freq()
            max_freq = cpu_freq.max if cpu_freq else 0
            
            return {
                'ram_gb': ram_gb,
                'cpu_percent': cpu_percent,
                'cpu_cores': cpu_cores,
                'max_freq': max_freq,
                'available_ram_gb': memory.available / (1024**3)
            }
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã: {e}")
            return {'ram_gb': 8, 'cpu_percent': 50, 'cpu_cores': 4, 'max_freq': 2000}
    
    def _get_installed_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=5,
                proxies={"http": "", "https": ""}
            )
            
            if response.status_code == 200:
                models_data = response.json().get('models', [])
                return [model['name'] for model in models_data]
            
            return []
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    def _select_optimal_model(self, system_info: Dict, installed_models: List[str]) -> Optional[str]:
        """–í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–∏—Å—Ç–µ–º—ã"""
        ram_gb = system_info['ram_gb']
        cpu_percent = system_info['cpu_percent']
        
        logger.info(f"üîç –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è {ram_gb:.1f}GB RAM, CPU {cpu_percent:.1f}%")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å–∏—Å—Ç–µ–º—ã
        if ram_gb < 4 or cpu_percent > 80:
            # –û—á–µ–Ω—å —Å–ª–∞–±–∞—è —Å–∏—Å—Ç–µ–º–∞ - —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ –ª–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏
            preferred_models = ["qwen2.5:1.5b", "llama3.2:1b"]
        elif ram_gb < 8 or cpu_percent > 60:
            # –°–ª–∞–±–∞—è —Å–∏—Å—Ç–µ–º–∞ - –ª–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏
            preferred_models = ["qwen2.5:1.5b", "phi3:mini", "llama3.2:1b", "gemma2:2b"]
        elif ram_gb < 16:
            # –°—Ä–µ–¥–Ω—è—è —Å–∏—Å—Ç–µ–º–∞ - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 3B –º–æ–¥–µ–ª–∏
            preferred_models = self.LIGHTWEIGHT_MODELS
        else:
            # –ú–æ—â–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ - –º–æ–∂–Ω–æ –≤—Å–µ –ª–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏ + –ø—Ä–æ–≤–µ—Ä–∏–º –±–æ–ª–µ–µ —Ç—è–∂–µ–ª—ã–µ
            preferred_models = self.LIGHTWEIGHT_MODELS + ["llama3.2:7b", "qwen2.5:7b"]
        
        # –ò—â–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã—Ö
        for model in preferred_models:
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ –ø–æ –±–∞–∑–æ–≤–æ–º—É –∏–º–µ–Ω–∏
            for installed in installed_models:
                if model == installed or model.split(':')[0] in installed:
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –ø–æ–¥—Ö–æ–¥—è—â–∞—è –º–æ–¥–µ–ª—å: {installed}")
                    return installed
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å
        for model in self.LIGHTWEIGHT_MODELS:
            for installed in installed_models:
                if model.split(':')[0] in installed:
                    logger.info(f"üì¶ –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å: {installed}")
                    return installed
        
        return None
    
    def _adjust_settings_for_model(self, model_name: str, system_info: Dict):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥ –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Å–∏—Å—Ç–µ–º—É"""
        ram_gb = system_info['ram_gb']
        
        # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ –º–æ–¥–µ–ª–∏
        if "1b" in model_name.lower():
            # –°–∞–º—ã–µ –ª–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏
            self.context_window = 1024
            self.max_tokens = 400
            self.temperature = 0.0
        elif "1.5b" in model_name.lower():
            # –õ–µ–≥–∫–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏  
            self.context_window = 1536
            self.max_tokens = 600
            self.temperature = 0.1
        elif any(x in model_name.lower() for x in ["2b", "mini"]):
            # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏
            self.context_window = 2048
            self.max_tokens = 800
            self.temperature = 0.1
        else:
            # 3B+ –º–æ–¥–µ–ª–∏
            self.context_window = 2048
            self.max_tokens = 1000
            self.temperature = 0.1
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è —Å–ª–∞–±—ã—Ö —Å–∏—Å—Ç–µ–º
        if ram_gb < 6:
            self.context_window = min(self.context_window, 1024)
            self.max_tokens = min(self.max_tokens, 400)
        elif ram_gb < 8:
            self.context_window = min(self.context_window, 1536)
            self.max_tokens = min(self.max_tokens, 600)
        
        # –†–µ–∂–∏–º —ç–∫–æ–Ω–æ–º–∏–∏ - –µ—â–µ –±–æ–ª—å—à–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        if self.economy_mode:
            self.context_window = int(self.context_window * 0.75)
            self.max_tokens = int(self.max_tokens * 0.75)
            self.timeout = min(self.timeout, 20)
        
        logger.info(f"‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è {model_name}: –∫–æ–Ω—Ç–µ–∫—Å—Ç {self.context_window}, —Ç–æ–∫–µ–Ω—ã {self.max_tokens}")
    
    def _test_model_performance(self) -> bool:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            test_prompt = "What is rap music? Answer in one sentence."
            
            start_time = time.time()
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": test_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.0,
                        "num_ctx": 512,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ—Å—Ç–∞
                        "num_predict": 50  # –ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
                    }
                },
                timeout=15,  # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
                proxies={"http": "", "https": ""}
            )
            
            test_time = time.time() - start_time
            
            if response.status_code == 200 and test_time < 10:  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –±—ã—Å—Ç—Ä–æ
                logger.info(f"‚úÖ –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ {self.model_name} –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ ({test_time:.1f}—Å)")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {self.model_name} —Ä–∞–±–æ—Ç–∞–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ ({test_time:.1f}—Å)")
                return False
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ {self.model_name} –Ω–µ –ø—Ä–æ—à–µ–ª: {e}")
            return False
    
    def _install_lightweight_model(self) -> bool:
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∞–º–æ–π –ª–µ–≥–∫–æ–π –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"""
        target_model = "qwen2.5:1.5b"  # –õ—É—á—à–∏–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å —Ä–∞–∑–º–µ—Ä/–∫–∞—á–µ—Å—Ç–≤–æ
        
        try:
            logger.info(f"üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å {target_model} (—ç—Ç–æ –∑–∞–π–º–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)...")
            
            response = requests.post(
                f"{self.base_url}/api/pull",
                json={"name": target_model},
                timeout=600,  # 10 –º–∏–Ω—É—Ç –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É
                proxies={"http": "", "https": ""}
            )
            
            if response.status_code == 200:
                self.model_name = target_model
                system_info = self._get_system_specs()
                self._adjust_settings_for_model(target_model, system_info)
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {target_model} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                return True
            else:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å {target_model}: {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def analyze_song(self, artist: str, title: str, lyrics: str) -> AnalysisResult:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Å–Ω–∏ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–µ—Å—É—Ä—Å–æ–≤"""
        start_time = time.time()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")
        
        if not self.available:
            raise RuntimeError("Optimized Ollama analyzer is not available")
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
        if self.resource_monitor.is_system_overloaded(self.max_cpu_usage):
            logger.warning(f"‚ö†Ô∏è –°–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–π —Ä–µ–∂–∏–º")
            self._switch_to_economy_mode()
        
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ–º
            processed_lyrics = self._preprocess_lyrics_optimized(lyrics)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Å–∂–∞—Ç–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
            prompt = self._create_minimal_prompt(artist, title, processed_lyrics)
            
            # –ó–∞–ø—Ä–æ—Å —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": self.temperature,
                        "top_p": 0.8,  # –°—É–∂–µ–Ω –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                        "num_ctx": self.context_window,
                        "num_predict": self.max_tokens,
                        "repeat_penalty": 1.1,  # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–æ–≤
                        "num_thread": min(2, psutil.cpu_count())  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ—Ç–æ–∫–∏
                    }
                },
                timeout=self.timeout,
                proxies={"http": "", "https": ""}
            )
            
            if response.status_code != 200:
                raise RuntimeError(f"Ollama request failed: {response.status_code}")
            
            result = response.json()
            analysis_text = result.get('response', '')
            
            # –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å fallback
            analysis_data = self._parse_response_fast(analysis_text)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            confidence = self._calculate_confidence_optimized(analysis_data)
            
            processing_time = time.time() - start_time
            
            return AnalysisResult(
                artist=artist,
                title=title,
                analysis_type="ollama_optimized",
                confidence=confidence,
                metadata={
                    "model_name": self.model_name,
                    "optimization_level": "high",
                    "economy_mode": self.economy_mode,
                    "context_window": self.context_window,
                    "max_tokens": self.max_tokens,
                    "system_load": self.resource_monitor.get_current_load(),
                    "processing_date": datetime.now().isoformat()
                },
                raw_output=analysis_data,
                processing_time=processing_time,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"‚ùå –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è –¥–ª—è {artist} - {title}: {e}")
            raise RuntimeError(f"Optimized analysis failed: {e}") from e
    
    def _preprocess_lyrics_optimized(self, lyrics: str) -> str:
        """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
        processed = self.preprocess_lyrics(lyrics)
        
        # –°–∏–ª—å–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–ª–∞–±—ã—Ö –ü–ö
        max_length = 800 if self.economy_mode else 1200
        
        if len(processed) > max_length:
            # –ë–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ + –∫–æ–Ω—Ü–æ–≤–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            mid_point = max_length // 2
            processed = processed[:mid_point] + " ... " + processed[-mid_point:]
        
        return processed
    
    def _create_minimal_prompt(self, artist: str, title: str, lyrics: str) -> str:
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        return f"""Analyze this rap song briefly in JSON format:

Artist: {artist}
Title: {title}  
Lyrics: {lyrics}

Return only this JSON structure:
{{
  "genre": "rap/trap/drill",
  "mood": "aggressive/calm/energetic",
  "quality": 0.0-1.0,
  "themes": ["topic1", "topic2"],
  "skill_level": 0.0-1.0
}}

Only JSON, no text!"""
    
    def _parse_response_fast(self, response_text: str) -> Dict[str, Any]:
        """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –ø—Ä–æ—Å—Ç—ã–º fallback"""
        try:
            # –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON
            start = response_text.find('{')
            end = response_text.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = response_text[start:end]
                return json.loads(json_str)
            else:
                raise ValueError("No JSON found")
                
        except:
            # –ü—Ä–æ—Å—Ç–æ–π fallback –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            return {
                "genre": "rap",
                "mood": "neutral", 
                "quality": 0.5,
                "themes": ["general"],
                "skill_level": 0.5,
                "_fallback": True
            }
    
    def _calculate_confidence_optimized(self, analysis_data: Dict[str, Any]) -> float:
        """–ë—ã—Å—Ç—Ä–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        if "_fallback" in analysis_data:
            return 0.3
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã
        expected_keys = ["genre", "mood", "quality", "themes", "skill_level"]
        present_keys = sum(1 for key in expected_keys if key in analysis_data)
        
        return (present_keys / len(expected_keys)) * 0.75  # –°–∫–∏–¥–∫–∞ –∑–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
    
    def _switch_to_economy_mode(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–π —Ä–µ–∂–∏–º"""
        self.max_tokens = min(self.max_tokens, 300)
        self.context_window = min(self.context_window, 1024)
        self.timeout = min(self.timeout, 15)
        self.economy_mode = True
        logger.info("üîã –ü–µ—Ä–µ–∫–ª—é—á–∏–ª–∏—Å—å –≤ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–π —Ä–µ–∂–∏–º")
    
    def get_analyzer_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ"""
        system_specs = self._get_system_specs()
        
        return {
            "name": "OptimizedOllamaAnalyzer",
            "version": "2.0.0-optimized",
            "description": "Resource-efficient local AI analysis for older PCs",
            "type": self.analyzer_type,
            "model_info": {
                "current_model": self.model_name,
                "context_window": self.context_window,
                "max_tokens": self.max_tokens,
                "economy_mode": self.economy_mode
            },
            "system_specs": system_specs,
            "optimization_features": [
                "Lightweight models (1.5B-3B parameters)",
                "Adaptive resource management", 
                "CPU usage limiting",
                "Memory-efficient processing",
                "Economy mode for weak PCs"
            ],
            "recommended_models": self.LIGHTWEIGHT_MODELS,
            "available": self.available
        }


class ResourceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.monitoring = True
        self.cpu_history = []
        self.memory_history = []
    
    def is_system_overloaded(self, max_cpu_percent: float = 70) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory = psutil.virtual_memory()
            
            # CPU –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω
            if cpu_percent > max_cpu_percent:
                return True
            
            # –ü–∞–º—è—Ç—å –ø–æ—á—Ç–∏ –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å
            if memory.percent > 85:
                return True
            
            return False
            
        except:
            return False
    
    def get_current_load(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –Ω–∞–≥—Ä—É–∑–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            return {
                "cpu_percent": psutil.cpu_percent(),
                "memory_percent": psutil.virtual_memory().percent,
                "available_memory_gb": psutil.virtual_memory().available / (1024**3)
            }
        except:
            return {"cpu_percent": 0, "memory_percent": 0, "available_memory_gb": 0}
