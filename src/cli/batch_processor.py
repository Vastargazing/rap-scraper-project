#!/usr/bin/env python3
"""
üì¶ CLI-—É—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ —Å —Ç–µ–∫—Å—Ç–∞–º–∏
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
python src/cli/batch_processor.py --file file.txt --analyzer hybrid   # –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- Python 3.8+
- src/core/app.py, src/interfaces/analyzer_interface.py
- config.yaml

–†–ï–ó–£–õ–¨–¢–ê–¢:
- –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö

–ê–í–¢–û–†: AI Assistant
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""

import asyncio
import json
import logging
import sys
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.app import Application


@dataclass
class BatchProgress:
    """–ü—Ä–æ–≥—Ä–µ—Å—Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""

    total: int
    completed: int
    failed: int
    start_time: float
    current_time: float

    @property
    def elapsed_time(self) -> float:
        return self.current_time - self.start_time

    @property
    def progress_percent(self) -> float:
        return (self.completed / self.total * 100) if self.total > 0 else 0

    @property
    def eta_seconds(self) -> float:
        if self.completed == 0:
            return 0
        avg_time_per_item = self.elapsed_time / self.completed
        remaining_items = self.total - self.completed
        return avg_time_per_item * remaining_items

    def to_dict(self) -> dict[str, Any]:
        return {
            **asdict(self),
            "progress_percent": self.progress_percent,
            "eta_seconds": self.eta_seconds,
            "elapsed_time": self.elapsed_time,
        }


class BatchProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤"""

    def __init__(self, max_workers: int = 4, progress_interval: float = 5.0):
        self.app = Application()
        self.max_workers = max_workers
        self.progress_interval = progress_interval
        self.logger = logging.getLogger(__name__)

    async def process_batch(
        self,
        texts: list[str],
        analyzer_type: str,
        output_file: str | None = None,
        checkpoint_file: str | None = None,
        resume_from_checkpoint: bool = False,
    ) -> list[dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π checkpoint'–æ–≤

        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analyzer_type: –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            output_file: –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            checkpoint_file: –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            resume_from_checkpoint: –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ checkpoint'–∞
        """

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º checkpoint –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        start_index = 0
        existing_results = []

        if (
            resume_from_checkpoint
            and checkpoint_file
            and Path(checkpoint_file).exists()
        ):
            self.logger.info(f"Resuming from checkpoint: {checkpoint_file}")
            with open(checkpoint_file, encoding="utf-8") as f:
                checkpoint_data = json.load(f)
                existing_results = checkpoint_data.get("results", [])
                start_index = len(existing_results)
                self.logger.info(f"Resuming from index {start_index}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        progress = BatchProgress(
            total=len(texts),
            completed=start_index,
            failed=0,
            start_time=time.time(),
            current_time=time.time(),
        )

        # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞–ø—Ä—è–º—É—é –ø–æ —Å—Ç—Ä–æ–∫–µ
        analyzer = self.app.get_analyzer(analyzer_type)
        if not analyzer:
            available = self.app.list_analyzers()
            raise ValueError(
                f"Unknown analyzer type: {analyzer_type}. Available: {available}"
            )

        self.logger.info(
            f"üöÄ Starting batch processing: {len(texts)} texts with {analyzer_type}"
        )
        self.logger.info(f"Max workers: {self.max_workers}")

        results = existing_results.copy()

        # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        last_progress_time = time.time()

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –ø–∞–∫–µ—Ç–∞—Ö
        batch_size = self.max_workers * 2  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞

        for batch_start in range(start_index, len(texts), batch_size):
            batch_end = min(batch_start + batch_size, len(texts))
            batch_texts = texts[batch_start:batch_end]

            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞
            batch_results = await self._process_batch_async(
                batch_texts, analyzer, batch_start
            )

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å
            results.extend(batch_results)

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ—É–¥–∞—á–Ω—ã–µ
            batch_failed = sum(1 for r in batch_results if "error" in r)
            progress.failed += batch_failed
            progress.completed = len(results)
            progress.current_time = time.time()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint
            if checkpoint_file:
                await self._save_checkpoint(checkpoint_file, results, progress)

            # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            current_time = time.time()
            if current_time - last_progress_time >= self.progress_interval:
                self._print_progress(progress)
                last_progress_time = current_time

        # –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
        progress.current_time = time.time()
        self._print_progress(progress, final=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if output_file:
            await self._save_results(output_file, results, progress)

        return results

    async def _process_batch_async(
        self, texts: list[str], analyzer, start_index: int
    ) -> list[dict[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ —Ç–µ–∫—Å—Ç–æ–≤"""

        tasks = []
        for i, text in enumerate(texts):
            task = self._analyze_single_text(text, analyzer, start_index + i)
            tasks.append(task)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(
                    {
                        "index": start_index + i,
                        "error": str(result),
                        "text_preview": texts[i][:100] + "..."
                        if len(texts[i]) > 100
                        else texts[i],
                    }
                )
            else:
                processed_results.append(result)

        return processed_results

    async def _analyze_single_text(
        self, text: str, analyzer, index: int
    ) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        try:
            start_time = time.time()

            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –∫ —Ä–∞–∑–Ω—ã–º API –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
            if hasattr(analyzer, "analyze"):
                result = await analyzer.analyze(text)
            elif hasattr(analyzer, "analyze_song"):
                # –°—Ç–∞—Ä—ã–π API - –ø–µ—Ä–µ–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                analysis_result = analyzer.analyze_song(
                    "Unknown", f"Text_{index}", text
                )
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                result = {
                    "sentiment": getattr(analysis_result, "metadata", {})
                    .get("sentiment_analysis", {})
                    .get("overall_sentiment", "neutral"),
                    "confidence": getattr(analysis_result, "confidence", 0.8),
                    "analysis_type": "batch_analysis",
                    "metadata": getattr(analysis_result, "metadata", {}),
                    "raw_output": getattr(analysis_result, "raw_output", {}),
                }
            else:
                raise RuntimeError("Analyzer has no analyze method")

            analysis_time = time.time() - start_time

            return {
                "index": index,
                "analysis_time": round(analysis_time, 3),
                "text_length": len(text),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "result": result,
            }
        except Exception as e:
            return {
                "index": index,
                "error": str(e),
                "text_preview": text[:100] + "..." if len(text) > 100 else text,
            }

    async def _save_checkpoint(
        self,
        checkpoint_file: str,
        results: list[dict[str, Any]],
        progress: BatchProgress,
    ) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint'–∞"""
        try:
            checkpoint_data = {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "progress": progress.to_dict(),
                "results": results,
            }

            checkpoint_path = Path(checkpoint_file)
            checkpoint_path.parent.mkdir(parents=True, exist_ok=True)

            with open(checkpoint_path, "w", encoding="utf-8") as f:
                json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)

        except Exception as e:
            self.logger.warning(f"Failed to save checkpoint: {e}")

    async def _save_results(
        self, output_file: str, results: list[dict[str, Any]], progress: BatchProgress
    ) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        output_data = {
            "metadata": {
                "total_processed": len(results),
                "successful": len([r for r in results if "error" not in r]),
                "failed": progress.failed,
                "total_time": progress.elapsed_time,
                "avg_time_per_item": progress.elapsed_time / len(results)
                if results
                else 0,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            },
            "results": results,
        }

        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)

        self.logger.info(f"‚úÖ Results saved to: {output_file}")

    def _print_progress(self, progress: BatchProgress, final: bool = False) -> None:
        """–í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""

        if final:
            print("\nüéâ Batch processing completed!")
        else:
            print(
                f"üìä Progress: {progress.completed}/{progress.total} "
                f"({progress.progress_percent:.1f}%)"
            )

        print(f"   ‚úÖ Successful: {progress.completed - progress.failed}")
        print(f"   ‚ùå Failed: {progress.failed}")
        print(f"   ‚è±Ô∏è  Elapsed: {progress.elapsed_time:.1f}s")

        if not final and progress.completed > 0:
            avg_time = progress.elapsed_time / progress.completed
            print(f"   üìà Avg time/item: {avg_time:.2f}s")
            print(f"   üïí ETA: {progress.eta_seconds:.0f}s")

        print()


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_texts = [
        "This is a positive and uplifting song about hope and dreams",
        "Dark and angry lyrics expressing frustration and pain",
        "Neutral descriptive text about everyday activities",
        "Love song with romantic and emotional content",
        "Aggressive rap with explicit language and strong opinions",
    ] * 3  # 15 —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞

    processor = BatchProcessor(max_workers=2)

    try:
        results = await processor.process_batch(
            texts=test_texts,
            analyzer_type="algorithmic",
            output_file="results/batch_results.json",
            checkpoint_file="results/batch_checkpoint.json",
            resume_from_checkpoint=False,
        )

        print(f"üìã Processing completed: {len(results)} results")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        successful = len([r for r in results if "error" not in r])
        failed = len(results) - successful

        print(f"   ‚úÖ Successful: {successful}")
        print(f"   ‚ùå Failed: {failed}")

        if successful > 0:
            avg_time = (
                sum(r.get("analysis_time", 0) for r in results if "analysis_time" in r)
                / successful
            )
            print(f"   ‚è±Ô∏è  Average analysis time: {avg_time:.3f}s")

    except Exception as e:
        print(f"‚ùå Batch processing failed: {e}")


if __name__ == "__main__":
    asyncio.run(main())
