"""
üßÆ –ë–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤ –ø–µ—Å–µ–Ω

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI –º–æ–¥–µ–ª–µ–π
- –û—Ü–µ–Ω–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è, —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, —Ä–∏—Ñ–º, –ø–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç–∏
- –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ BaseAnalyzer

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ main.py, batch_processor, analyzer_cli

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- Python 3.8+
- src/interfaces/analyzer_interface.py

–†–ï–ó–£–õ–¨–¢–ê–¢:
- –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–µ–∫—Å—Ç—É: –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ, —Å–ª–æ–∂–Ω–æ—Å—Ç—å, —Ä–∏—Ñ–º—ã, –ø–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å
- –ë—ã—Å—Ç—Ä—ã–π baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏

–ê–í–¢–û–†: AI Assistant
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""
import re
import time
from datetime import datetime
from typing import Dict, Any, List
from collections import Counter

from interfaces.analyzer_interface import BaseAnalyzer, AnalysisResult, register_analyzer


@register_analyzer("algorithmic_basic")
class AlgorithmicAnalyzer(BaseAnalyzer):
    """
    –ë–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ –ø–µ—Å–µ–Ω.
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI:
    - –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    - –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (readability)
    - –†–∏—Ñ–º–æ–≤–∞—è —Å—Ö–µ–º–∞
    - –ü–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å —Å–ª–æ–≤
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
        super().__init__(config)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.min_word_length = self.config.get('min_word_length', 3)
        self.sentiment_threshold = self.config.get('sentiment_threshold', 0.1)
        
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        self._load_sentiment_dictionaries()
    
    def _load_sentiment_dictionaries(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è"""
        # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞
        self.positive_words = {
            'love', 'happy', 'joy', 'good', 'great', 'amazing', 'beautiful',
            'win', 'success', 'best', 'awesome', 'perfect', 'wonderful',
            'smile', 'laugh', 'peace', 'hope', 'dream', 'shine', 'bright'
        }
        
        # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞
        self.negative_words = {
            'hate', 'sad', 'angry', 'bad', 'terrible', 'awful', 'ugly',
            'lose', 'fail', 'worst', 'horrible', 'pain', 'hurt', 'cry',
            'dark', 'death', 'kill', 'fight', 'war', 'broke', 'poor'
        }
        
        # –°–ª–æ–≤–∞ –∞–≥—Ä–µ—Å—Å–∏–∏/–Ω–∞—Å–∏–ª–∏—è
        self.aggressive_words = {
            'kill', 'murder', 'gun', 'shoot', 'fight', 'blood', 'violence',
            'punch', 'hit', 'attack', 'destroy', 'revenge', 'enemy'
        }
        
        # –°–ª–æ–≤–∞ –æ –¥–µ–Ω—å–≥–∞—Ö/–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–º
        self.money_words = {
            'money', 'cash', 'rich', 'wealth', 'gold', 'diamond', 'expensive',
            'luxury', 'mansion', 'car', 'chain', 'brand', 'designer'
        }
    
    def analyze_song(self, artist: str, title: str, lyrics: str) -> AnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑ –ø–µ—Å–Ω–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤.
        
        Args:
            artist: –ò–º—è –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è
            title: –ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Å–Ω–∏  
            lyrics: –¢–µ–∫—Å—Ç –ø–µ—Å–Ω–∏
            
        Returns:
            AnalysisResult —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        start_time = time.time()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        processed_lyrics = self.preprocess_lyrics(lyrics)
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞
        analysis_results = self._perform_analysis(processed_lyrics)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence = self._calculate_confidence(analysis_results)
        
        processing_time = time.time() - start_time
        
        return AnalysisResult(
            artist=artist,
            title=title,
            analysis_type="algorithmic_basic",
            confidence=confidence,
            metadata={
                "analyzer_version": "1.0.0",
                "processing_date": datetime.now().isoformat(),
                "lyrics_length": len(processed_lyrics),
                "word_count": len(processed_lyrics.split())
            },
            raw_output=analysis_results,
            processing_time=processing_time,
            timestamp=datetime.now().isoformat()
        )
    
    def _perform_analysis(self, lyrics: str) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–∏–¥–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        words = self._extract_words(lyrics)
        
        return {
            "sentiment_analysis": self._analyze_sentiment(words),
            "complexity_analysis": self._analyze_complexity(lyrics, words),
            "themes_analysis": self._analyze_themes(words),
            "structure_analysis": self._analyze_structure(lyrics),
            "vocabulary_analysis": self._analyze_vocabulary(words)
        }
    
    def _extract_words(self, lyrics: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        words = re.findall(r'\b[a-zA-Z]{%d,}\b' % self.min_word_length, lyrics.lower())
        return words
    
    def _analyze_sentiment(self, words: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        word_set = set(words)
        
        positive_count = len(word_set.intersection(self.positive_words))
        negative_count = len(word_set.intersection(self.negative_words))
        aggressive_count = len(word_set.intersection(self.aggressive_words))
        
        total_sentiment_words = positive_count + negative_count + aggressive_count
        
        if total_sentiment_words == 0:
            sentiment_score = 0
            sentiment_label = "neutral"
        else:
            sentiment_score = (positive_count - negative_count - aggressive_count) / total_sentiment_words
            
            if sentiment_score > self.sentiment_threshold:
                sentiment_label = "positive"
            elif sentiment_score < -self.sentiment_threshold:
                sentiment_label = "negative"
            else:
                sentiment_label = "neutral"
        
        return {
            "sentiment_score": round(sentiment_score, 3),
            "sentiment_label": sentiment_label,
            "positive_words_count": positive_count,
            "negative_words_count": negative_count,
            "aggressive_words_count": aggressive_count,
            "total_sentiment_words": total_sentiment_words
        }
    
    def _analyze_complexity(self, lyrics: str, words: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        sentences = re.split(r'[.!?]+', lyrics)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return {"error": "No sentences found"}
        
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞—Ä—è
        unique_words = len(set(words))
        total_words = len(words)
        vocabulary_richness = unique_words / total_words if total_words > 0 else 0
        
        # –°–ª–æ–∂–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Å–ª–æ–≤
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ (Flesch-–ø–æ–¥–æ–±–Ω–∞—è)
        readability_score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * (avg_word_length / 4.7))
        readability_score = max(0, min(100, readability_score))  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ 0-100
        
        return {
            "avg_sentence_length": round(avg_sentence_length, 2),
            "vocabulary_richness": round(vocabulary_richness, 3),
            "avg_word_length": round(avg_word_length, 2),
            "readability_score": round(readability_score, 1),
            "unique_words": unique_words,
            "total_words": total_words,
            "sentences_count": len(sentences)
        }
    
    def _analyze_themes(self, words: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–º–∞—Ç–∏–∫–∏ –ø–µ—Å–Ω–∏"""
        word_set = set(words)
        
        themes = {
            "money_materialism": len(word_set.intersection(self.money_words)),
            "violence_aggression": len(word_set.intersection(self.aggressive_words)),
            "love_relationships": len(word_set.intersection(self.positive_words)),
            "negative_emotions": len(word_set.intersection(self.negative_words))
        }
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–π —Ç–µ–º—ã
        dominant_theme = max(themes.items(), key=lambda x: x[1])
        
        return {
            "themes_scores": themes,
            "dominant_theme": dominant_theme[0] if dominant_theme[1] > 0 else "neutral",
            "dominant_theme_score": dominant_theme[1]
        }
    
    def _analyze_structure(self, lyrics: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞"""
        lines = [line.strip() for line in lyrics.split('\n') if line.strip()]
        
        # –ü–æ–∏—Å–∫ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å—Ç—Ä–æ–∫ (–ø—Ä–∏–ø–µ–≤—ã)
        line_counts = Counter(lines)
        repeated_lines = {line: count for line, count in line_counts.items() if count > 1}
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        rhyme_analysis = self._simple_rhyme_analysis(lines)
        
        return {
            "total_lines": len(lines),
            "unique_lines": len(set(lines)),
            "repeated_lines_count": len(repeated_lines),
            "repetition_ratio": len(repeated_lines) / len(set(lines)) if lines else 0,
            "rhyme_density": rhyme_analysis["rhyme_density"],
            "avg_line_length": sum(len(line.split()) for line in lines) / len(lines) if lines else 0
        }
    
    def _simple_rhyme_analysis(self, lines: List[str]) -> Dict[str, Any]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º"""
        if len(lines) < 2:
            return {"rhyme_density": 0, "rhyming_pairs": 0}
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ–≤ –∏–∑ —Å—Ç—Ä–æ–∫
        last_words = []
        for line in lines:
            words = line.split()
            if words:
                last_word = re.sub(r'[^a-zA-Z]', '', words[-1].lower())
                if len(last_word) >= 2:
                    last_words.append(last_word)
        
        # –ü–æ–∏—Å–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ä–∏—Ñ–º (—É–ø—Ä–æ—â–µ–Ω–Ω–æ - –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è–º)
        rhyming_pairs = 0
        for i in range(len(last_words)):
            for j in range(i + 1, len(last_words)):
                if self._words_rhyme(last_words[i], last_words[j]):
                    rhyming_pairs += 1
        
        rhyme_density = rhyming_pairs / len(last_words) if last_words else 0
        
        return {
            "rhyme_density": round(rhyme_density, 3),
            "rhyming_pairs": rhyming_pairs,
            "total_line_endings": len(last_words)
        }
    
    def _words_rhyme(self, word1: str, word2: str) -> bool:
        """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∏—Ñ–º—ã –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏—é"""
        if len(word1) < 2 or len(word2) < 2:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 2-3 —Å–∏–º–≤–æ–ª–æ–≤
        return word1[-2:] == word2[-2:] or word1[-3:] == word2[-3:]
    
    def _analyze_vocabulary(self, words: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–∞–ø–∞—Å–∞"""
        if not words:
            return {"error": "No words to analyze"}
        
        word_freq = Counter(words)
        
        # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
        most_common = word_freq.most_common(10)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç
        freq_distribution = {}
        for word, freq in word_freq.items():
            freq_key = f"freq_{freq}"
            freq_distribution[freq_key] = freq_distribution.get(freq_key, 0) + 1
        
        return {
            "most_common_words": most_common,
            "frequency_distribution": freq_distribution,
            "hapax_legomena": sum(1 for freq in word_freq.values() if freq == 1),  # –°–ª–æ–≤–∞, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è 1 —Ä–∞–∑
            "vocabulary_size": len(word_freq)
        }
    
    def _calculate_confidence(self, analysis_results: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö"""
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–Ω–æ—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        confidence_factors = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        for analysis_type, results in analysis_results.items():
            if isinstance(results, dict) and "error" not in results:
                confidence_factors.append(1.0)
            else:
                confidence_factors.append(0.0)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        if "vocabulary_analysis" in analysis_results:
            vocab_results = analysis_results["vocabulary_analysis"]
            if "vocabulary_size" in vocab_results:
                # –ë–æ–ª—å—à–µ —Å–ª–æ–≤–∞—Ä—å = –≤—ã—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                vocab_size = vocab_results["vocabulary_size"]
                vocab_confidence = min(1.0, vocab_size / 50)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                confidence_factors.append(vocab_confidence)
        
        return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.0
    
    def get_analyzer_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ"""
        return {
            "name": "AlgorithmicAnalyzer",
            "version": "1.0.0",
            "description": "Basic algorithmic text analysis without AI models",
            "author": "Rap Scraper Project",
            "type": self.analyzer_type,
            "supported_features": self.supported_features,
            "config_options": {
                "min_word_length": "Minimum word length for analysis (default: 3)",
                "sentiment_threshold": "Threshold for sentiment classification (default: 0.1)"
            }
        }
    
    @property
    def analyzer_type(self) -> str:
        """–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        return "algorithmic"
    
    @property
    def supported_features(self) -> List[str]:
        """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        return [
            "sentiment_analysis",
            "complexity_analysis", 
            "themes_analysis",
            "structure_analysis",
            "vocabulary_analysis",
            "rhyme_analysis"
        ]
