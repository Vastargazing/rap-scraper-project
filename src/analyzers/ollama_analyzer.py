# TODO(code-review): Remove emoji from module docstring (Google style guide)
# TODO(code-review): Add proper Google-style docstring with sections:
#   - Module description
#   - Typical usage example
#   - Attributes (if any module-level)
"""
ü¶ô Ollama AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤ –ø–µ—Å–µ–Ω (–ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏)

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- –õ–æ–∫–∞–ª—å–Ω—ã–π AI-–∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –ø–µ—Å–µ–Ω —á–µ—Ä–µ–∑ Ollama
- –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –æ–±–ª–∞—á–Ω—ã–µ API
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–∏–≤–∞—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ main.py, batch_processor, analyzer_cli

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- Python 3.8+
- src/interfaces/analyzer_interface.py
- Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä)

–†–ï–ó–£–õ–¨–¢–ê–¢:
- –ú–µ—Ç—Ä–∏–∫–∏: –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ, –∫–∞—á–µ—Å—Ç–≤–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –∂–∞–Ω—Ä
- –ë—ã—Å—Ç—Ä—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ç–µ—Å—Ç–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏—è

–ê–í–¢–û–†: AI Assistant
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""

# TODO(code-review): Sort imports alphabetically within groups (stdlib, third-party, local)
# TODO(code-review): Add 'from typing import Optional' for better type safety
import json
import logging
import time
from datetime import datetime
from typing import Any

import requests

from interfaces.analyzer_interface import (
    AnalysisResult,
    BaseAnalyzer,
    register_analyzer,
)

# TODO(code-review): Extract magic numbers to module-level constants with descriptive names
# Example: DEFAULT_MODEL_NAME, DEFAULT_BASE_URL, DEFAULT_TIMEOUT, etc.

# TODO(code-review): Add constants for retry logic, timeouts, and other configuration
# Example:
# _DEFAULT_MODEL = "llama3.2:3b"
# _DEFAULT_BASE_URL = "http://localhost:11434"
# _DEFAULT_TEMPERATURE = 0.1
# _DEFAULT_TIMEOUT = 60
# _MAX_LYRICS_LENGTH = 1500
# _MODEL_PULL_TIMEOUT = 300
# _AVAILABILITY_CHECK_TIMEOUT = 5
# _CONFIDENCE_PENALTY_FOR_LOCAL_MODELS = 0.8
# _LOW_CONFIDENCE_FALLBACK = 0.4

logger = logging.getLogger(__name__)


# TODO(code-review): Add type hint for logger (logging.Logger)
# logger: logging.Logger = logging.getLogger(__name__)

@register_analyzer("ollama")
class OllamaAnalyzer(BaseAnalyzer):
    # TODO(code-review): Improve class docstring to follow Google style guide format:
    #   - One-line summary
    #   - Detailed description (optional)
    #   - Attributes section listing all class attributes
    #   - Example usage section
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞ –±–∞–∑–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama.

    –õ–æ–∫–∞–ª—å–Ω—ã–π AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏—è:
    - –ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
    - –õ–æ–∫–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å)
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    - –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
    """

    # TODO(code-review): Add type hints for all instance attributes at class level
    # model_name: str
    # base_url: str
    # temperature: float
    # timeout: int
    # available: bool

    def __init__(self, config: dict[str, Any] | None = None):
        # TODO(code-review): Expand docstring with Google style:
        #   Args:
        #       config: Configuration dictionary with optional keys...
        #   Raises:
        #       ConnectionError: If Ollama server is not accessible
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        super().__init__(config)

        # TODO(code-review): Use module-level constants instead of magic values
        # TODO(code-review): Add type casting for config values to ensure type safety
        # TODO(code-review): Validate config values (e.g., temperature should be 0-2)
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Ollama
        self.model_name = self.config.get("model_name", "llama3.2:3b")
        self.base_url = self.config.get("base_url", "http://localhost:11434")
        self.temperature = self.config.get("temperature", 0.1)
        self.timeout = self.config.get("timeout", 60)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        self.available = self._check_availability()

        # TODO(code-review): Remove emoji from log messages (not production-ready)
        # TODO(code-review): Use structured logging with extra fields instead of f-strings
        # Example: logger.info("Ollama analyzer initialized", extra={"model": self.model_name})
        if self.available:
            logger.info(f"‚úÖ Ollama –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.model_name}")
        else:
            logger.warning("‚ö†Ô∏è Ollama –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

    def _check_availability(self) -> bool:
        # TODO(code-review): Add proper Google-style docstring:
        #   Returns:
        #       bool: True if Ollama server is available and model is ready
        #   Raises:
        #       (Document any exceptions or note that they're caught internally)
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            # TODO(code-review): Extract "/api/tags" to a constant (e.g., _API_TAGS_ENDPOINT)
            # TODO(code-review): Extract timeout (5) to constant (_AVAILABILITY_CHECK_TIMEOUT)
            # TODO(code-review): Extract proxies dict to a constant or helper method
            # TODO(code-review): Add comment explaining why proxies are disabled
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–µ—Ä–∞
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=5,
                proxies={"http": "", "https": ""},
            )

            # TODO(code-review): Use response.raise_for_status() for better error handling
            # TODO(code-review): Add explicit check for response.ok or status code range
            if response.status_code == 200:
                models = response.json().get("models", [])
                # TODO(code-review): Add type hint and null check for models list
                # TODO(code-review): Handle case where "name" key might be missing
                available_models = [model["name"] for model in models]
                # TODO(code-review): Remove emoji from logs
                logger.info(f"ü¶ô Ollama –¥–æ—Å—Ç—É–ø–µ–Ω. –ú–æ–¥–µ–ª–∏: {available_models}")

                # TODO(code-review): Use exact match instead of 'in' for model checking
                # Current logic: "llama" in "llama3.2" would match incorrectly
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω–æ–π –º–æ–¥–µ–ª–∏
                if any(self.model_name in model for model in available_models):
                    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.model_name} –Ω–∞–π–¥–µ–Ω–∞")
                    return True
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
                return self._pull_model()

            # TODO(code-review): Log the actual status code for debugging
            return False

        # TODO(code-review): Catch more specific exceptions (ConnectionError, Timeout, etc.)
        # TODO(code-review): Consider retrying with exponential backoff
        except requests.exceptions.RequestException as e:
            # TODO(code-review): Remove emoji from logs
            logger.warning(f"‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            logger.info("üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: ollama serve")
            return False

    def _pull_model(self) -> bool:
        # TODO(code-review): Add proper Google-style docstring with Returns section
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
        try:
            # TODO(code-review): Remove emoji from logs
            logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {self.model_name}...")

            # TODO(code-review): Extract "/api/pull" to constant
            # TODO(code-review): Extract timeout (300) to constant (_MODEL_PULL_TIMEOUT)
            # TODO(code-review): Extract proxies to constant/helper
            # TODO(code-review): Add progress indication for long-running operation
            # TODO(code-review): Consider using streaming response to show download progress
            response = requests.post(
                f"{self.base_url}/api/pull",
                json={"name": self.model_name},
                timeout=300,  # 5 –º–∏–Ω—É—Ç –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É
                proxies={"http": "", "https": ""},
            )

            # TODO(code-review): Use response.raise_for_status() or check response.ok
            if response.status_code == 200:
                # TODO(code-review): Remove emoji
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return True
            # TODO(code-review): Don't log full response.text (might be large), log status code
            # TODO(code-review): Remove emoji
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {response.text}")
            return False

        # TODO(code-review): Never catch bare Exception - use specific exceptions
        # TODO(code-review): Should be: except requests.exceptions.RequestException
        except Exception as e:
            # TODO(code-review): Remove emoji, add more context to error message
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    def analyze_song(self, artist: str, title: str, lyrics: str) -> AnalysisResult:
        # TODO(code-review): Add Raises section to docstring:
        #   Raises:
        #       ValueError: If input parameters are invalid
        #       RuntimeError: If Ollama analyzer is unavailable or request fails
        """
        –ê–Ω–∞–ª–∏–∑ –ø–µ—Å–Ω–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama –º–æ–¥–µ–ª–∏.

        Args:
            artist: –ò–º—è –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è
            title: –ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Å–Ω–∏
            lyrics: –¢–µ–∫—Å—Ç –ø–µ—Å–Ω–∏

        Returns:
            AnalysisResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        start_time = time.time()

        # TODO(code-review): Add input sanitization before validation
        # TODO(code-review): Validate individual parameters (check for empty strings, None, etc.)
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not self.validate_input(artist, title, lyrics):
            # TODO(code-review): Provide more specific error message about what's invalid
            raise ValueError("Invalid input parameters")

        # TODO(code-review): This check should be done in __init__ or raise specific exception
        if not self.available:
            # TODO(code-review): Consider custom exception class (OllamaUnavailableError)
            raise RuntimeError(
                "Ollama analyzer is not available. Make sure Ollama is running."
            )

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        processed_lyrics = self.preprocess_lyrics(lyrics)

        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            prompt = self._create_analysis_prompt(artist, title, processed_lyrics)

            # TODO(code-review): Extract "/api/generate" to constant
            # TODO(code-review): Extract all magic numbers to constants:
            #   - top_p (0.9) -> _DEFAULT_TOP_P
            #   - num_ctx (4096) -> _DEFAULT_CONTEXT_WINDOW
            #   - num_predict (1500) -> _MAX_TOKENS_RESPONSE
            # TODO(code-review): Extract proxies dict to constant/helper
            # TODO(code-review): Consider extracting request payload to separate method for testing
            # TODO(code-review): Add retry logic with exponential backoff for transient failures
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": self.temperature,
                        "top_p": 0.9,
                        "num_ctx": 4096,  # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
                        "num_predict": 1500,  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç–≤–µ—Ç–∞
                    },
                },
                timeout=self.timeout,
                proxies={"http": "", "https": ""},
            )

            # TODO(code-review): Use response.raise_for_status() instead of manual check
            # TODO(code-review): Don't include full response.text in error (might be large/sensitive)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Ollama request failed: {response.status_code} - {response.text}"
                )

            # TODO(code-review): Add error handling for invalid JSON response
            # TODO(code-review): Validate response structure before accessing fields
            result = response.json()
            analysis_text = result.get("response", "")

            # TODO(code-review): Check for whitespace-only responses, not just empty
            if not analysis_text:
                raise RuntimeError("Empty response from Ollama model")

            # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            analysis_data = self._parse_response(analysis_text)

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = self._calculate_confidence(analysis_data)

            processing_time = time.time() - start_time

            # TODO(code-review): Extract "ollama" string to constant or use class attribute
            # TODO(code-review): Don't call datetime.now() twice - reuse timestamp
            # TODO(code-review): Consider adding request_id for tracking/debugging
            # TODO(code-review): Validate confidence is in range [0, 1]
            # TODO(code-review): Don't include base_url in metadata (might be sensitive)
            return AnalysisResult(
                artist=artist,
                title=title,
                analysis_type="ollama",
                confidence=confidence,
                metadata={
                    "model_name": self.model_name,
                    "base_url": self.base_url,
                    "processing_date": datetime.now().isoformat(),
                    "lyrics_length": len(processed_lyrics),
                    "temperature": self.temperature,
                    "timeout": self.timeout,
                },
                raw_output=analysis_data,
                processing_time=processing_time,
                timestamp=datetime.now().isoformat(),
            )

        # TODO(code-review): Separate different exception types and handle specifically
        # TODO(code-review): Add retry logic for ConnectionError, Timeout exceptions
        except requests.exceptions.RequestException as e:
            # TODO(code-review): Remove emoji from error logs
            # TODO(code-review): Add structured logging with extra fields
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            raise RuntimeError(f"Ollama connection failed: {e}") from e

        # TODO(code-review): NEVER catch bare Exception - specify exact exception types
        # TODO(code-review): This catches ValueError, RuntimeError, etc. - handle each separately
        except Exception as e:
            # TODO(code-review): Remove emoji, don't log PII (artist/title might be sensitive)
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ Ollama –¥–ª—è {artist} - {title}: {e}")
            raise RuntimeError(f"Ollama analysis failed: {e}") from e

    def _create_analysis_prompt(self, artist: str, title: str, lyrics: str) -> str:
        # TODO(code-review): Add Google-style docstring with Args and Returns sections
        # TODO(code-review): Add sanitization for artist/title to prevent prompt injection
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è Ollama –º–æ–¥–µ–ª–∏"""
        # TODO(code-review): Extract 1500 to constant (_MAX_LYRICS_LENGTH)
        # TODO(code-review): Use textwrap.shorten() or smart truncation at word boundaries
        # TODO(code-review): Add logging when lyrics are truncated
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        max_lyrics_length = 1500
        if len(lyrics) > max_lyrics_length:
            lyrics = lyrics[:max_lyrics_length] + "..."

        # TODO(code-review): Extract prompt template to constant or separate file
        # TODO(code-review): Use a template engine (e.g., jinja2) for complex prompts
        # TODO(code-review): Validate that f-strings don't break JSON structure
        # TODO(code-review): Consider using triple-quoted strings with proper indentation
        # TODO(code-review): This method is too long (>50 lines) - consider splitting
        return f"""Analyze this rap song and return ONLY a valid JSON response with the analysis.

Artist: {artist}
Title: {title}
Lyrics: {lyrics}

Return ONLY valid JSON with this structure:
{{
    "basic_analysis": {{
        "genre": "rap/trap/drill/old-school/gangsta/emo-rap",
        "mood": "aggressive/melancholic/energetic/confident/neutral",
        "energy": "low/medium/high",
        "explicit": true/false
    }},
    "content_themes": {{
        "main_topics": ["money", "relationships", "street_life", "success", "struggle"],
        "narrative_style": "storytelling/boastful/confessional/abstract",
        "emotional_tone": "positive/negative/neutral/mixed"
    }},
    "technical_aspects": {{
        "rhyme_complexity": "simple/moderate/complex",
        "flow_style": "steady/varied/aggressive/laid-back",
        "wordplay_level": "basic/good/excellent",
        "structure_type": "traditional/experimental/freestyle"
    }},
    "quality_assessment": {{
        "lyrical_skill": 0.0-1.0,
        "creativity": 0.0-1.0,
        "authenticity": 0.0-1.0,
        "overall_quality": 0.0-1.0
    }},
    "experimental_features": {{
        "cultural_era": "1990s/2000s/2010s/2020s",
        "regional_style": "east_coast/west_coast/south/midwest/international",
        "influences": ["list", "of", "influences"],
        "innovation_level": 0.0-1.0
    }}
}}

Respond with ONLY the JSON object, no additional text!"""
        # TODO(code-review): Add validation that returned string is within model's context limit
        # TODO(code-review): Consider using Pydantic models for the expected JSON schema

    def _parse_response(self, response_text: str) -> dict[str, Any]:
        # TODO(code-review): Add Google-style docstring with Args, Returns, Raises sections
        # TODO(code-review): Add type hint: -> Dict[str, Any] or use TypedDict for structure
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama –º–æ–¥–µ–ª–∏"""
        try:
            # TODO(code-review): Use regex for more robust JSON extraction
            # TODO(code-review): Handle nested braces correctly (current logic might fail)
            # –ü–æ–∏—Å–∫ JSON –±–ª–æ–∫–∞ –≤ –æ—Ç–≤–µ—Ç–µ
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1

            if json_start == -1 or json_end == 0:
                raise ValueError("No JSON found in response")

            json_str = response_text[json_start:json_end]

            # TODO(code-review): This string replacement is fragile and might break valid JSON
            # TODO(code-review): Use json.loads directly and catch errors instead of pre-processing
            # TODO(code-review): Document why these replacements are necessary
            # –û—á–∏—Å—Ç–∫–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            json_str = json_str.replace("\\n", "\\\\n")
            json_str = json_str.replace("\n", " ")

            # –ü–∞—Ä—Å–∏–Ω–≥ JSON
            analysis_data = json.loads(json_str)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            self._validate_analysis_structure(analysis_data)

            return analysis_data

        # TODO(code-review): Good - specific exception handling
        except json.JSONDecodeError as e:
            # TODO(code-review): Remove emoji from logs
            # TODO(code-review): Extract 300 to constant
            # TODO(code-review): Use logger.exception() to include traceback
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç Ollama: {e}")
            logger.error(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response_text[:300]}...")

            # TODO(code-review): Add comment explaining why we fall back to basic extraction
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å —Ö–æ—Ç—è –±—ã –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            return self._extract_basic_info(response_text)

        # TODO(code-review): NEVER catch bare Exception - this is too broad
        # TODO(code-review): Specify exact exception types (ValueError, KeyError, etc.)
        except Exception as e:
            # TODO(code-review): Remove emoji
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞ Ollama: {e}")
            raise ValueError(f"Ollama response parsing failed: {e}") from e

    def _extract_basic_info(self, response_text: str) -> dict[str, Any]:
        # TODO(code-review): Add Google-style docstring with Args, Returns sections
        # TODO(code-review): Add note that this is a fallback method with lower accuracy
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON"""
        # TODO(code-review): Remove emoji from logs
        logger.warning("‚ö†Ô∏è –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞")

        # TODO(code-review): This is a fragile keyword-based approach - document limitations
        # TODO(code-review): Consider using regex patterns instead of simple 'in' checks
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        text_lower = response_text.lower()

        # TODO(code-review): Extract genre keywords to constants or configuration
        # TODO(code-review): Use elif properly to avoid redundant checks
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∂–∞–Ω—Ä–∞
        genre = "rap"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if "trap" in text_lower:
            genre = "trap"
        elif "drill" in text_lower:
            genre = "drill"
        elif "old school" in text_lower or "old-school" in text_lower:
            genre = "old-school"

        # TODO(code-review): Extract mood keywords to constants
        # TODO(code-review): This could match unrelated words - use word boundaries
        # TODO(code-review): Consider using a scoring system instead of first-match
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        mood = "neutral"
        if any(word in text_lower for word in ["aggressive", "angry", "hard"]):
            mood = "aggressive"
        elif any(word in text_lower for word in ["sad", "melancholic", "depressed"]):
            mood = "melancholic"
        elif any(word in text_lower for word in ["energetic", "upbeat", "hype"]):
            mood = "energetic"
        elif any(word in text_lower for word in ["confident", "boastful"]):
            mood = "confident"

        # TODO(code-review): Extract this default structure to a constant or factory method
        # TODO(code-review): Use dataclass or Pydantic model for type safety
        return {
            "basic_analysis": {
                "genre": genre,
                "mood": mood,
                # TODO(code-review): Extract "medium" to constant
                "energy": "medium",
                # TODO(code-review): This boolean logic might match "not explicit" as True
                "explicit": "explicit" in text_lower or "profanity" in text_lower,
            },
            "content_themes": {
                # TODO(code-review): Extract default values to constants
                "main_topics": ["general"],
                "narrative_style": "abstract",
                "emotional_tone": "neutral",
            },
            "technical_aspects": {
                # TODO(code-review): Extract default values to constants
                "rhyme_complexity": "moderate",
                "flow_style": "steady",
                "wordplay_level": "basic",
                "structure_type": "traditional",
            },
            "quality_assessment": {
                # TODO(code-review): Extract 0.5 fallback score to named constant
                # TODO(code-review): Consider using lower confidence scores for fallback
                "lyrical_skill": 0.5,
                "creativity": 0.5,
                "authenticity": 0.5,
                "overall_quality": 0.5,
            },
            "experimental_features": {
                # TODO(code-review): Extract default values to constants
                "cultural_era": "2020s",
                "regional_style": "international",
                "influences": ["modern_rap"],
                "innovation_level": 0.5,
            },
            # TODO(code-review): Use more descriptive key name (e.g., "fallback_parsing")
            "_parsing_note": "Extracted from non-JSON response",
        }

    def _validate_analysis_structure(self, data: dict[str, Any]) -> None:
        # TODO(code-review): Add Google-style docstring with Args and Raises sections
        # TODO(code-review): Consider raising ValidationError instead of just logging warnings
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        # TODO(code-review): Extract required sections list to class-level constant
        # TODO(code-review): Use a schema validation library (e.g., jsonschema, Pydantic)
        required_sections = [
            "basic_analysis",
            "content_themes",
            "technical_aspects",
            "quality_assessment",
            "experimental_features",
        ]

        # TODO(code-review): Collect all missing sections and log once
        # TODO(code-review): Consider raising exception if critical sections are missing
        for section in required_sections:
            if section not in data:
                # TODO(code-review): Remove emoji from logs
                logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–µ–∫—Ü–∏—è: {section}")

        # TODO(code-review): Extract expected metrics to constant
        # TODO(code-review): Validate nested structure (e.g., genre values, mood values)
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        quality_assessment = data.get("quality_assessment", {})
        for metric in [
            "lyrical_skill",
            "creativity",
            "authenticity",
            "overall_quality",
        ]:
            if metric in quality_assessment:
                value = quality_assessment[metric]
                # TODO(code-review): Good validation, but should fix invalid values not just log
                # TODO(code-review): Consider clamping values to [0, 1] range instead of warning
                if not isinstance(value, (int, float)) or not 0 <= value <= 1:
                    # TODO(code-review): Remove emoji
                    logger.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ {metric}: {value}")

    def _calculate_confidence(self, analysis_data: dict[str, Any]) -> float:
        # TODO(code-review): Add Google-style docstring with Args and Returns sections
        # TODO(code-review): Document the confidence calculation algorithm
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∞–Ω–∞–ª–∏–∑–∞"""
        confidence_factors = []

        # TODO(code-review): Extract section names to constant (duplicated from validation)
        # TODO(code-review): Magic number 5 should be len() of the sections list
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        expected_sections = 5
        completed_sections = 0

        for section_name in [
            "basic_analysis",
            "content_themes",
            "technical_aspects",
            "quality_assessment",
            "experimental_features",
        ]:
            # TODO(code-review): Check if section is non-empty dict, not just truthy
            if analysis_data.get(section_name):
                completed_sections += 1

        # TODO(code-review): Add check for division by zero (though unlikely here)
        completeness_score = completed_sections / expected_sections
        confidence_factors.append(completeness_score)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        quality_assessment = analysis_data.get("quality_assessment", {})
        if quality_assessment:
            valid_metrics = []
            for metric_value in quality_assessment.values():
                if isinstance(metric_value, (int, float)) and 0 <= metric_value <= 1:
                    valid_metrics.append(metric_value)

            if valid_metrics:
                # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
                avg_quality = sum(valid_metrics) / len(valid_metrics)
                confidence_factors.append(avg_quality)

        # TODO(code-review): Extract 0.3 to named constant (_FALLBACK_PARSING_PENALTY)
        # TODO(code-review): Document why 0.3 was chosen
        # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        if "_parsing_note" in analysis_data:
            confidence_factors.append(0.3)  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å

        # TODO(code-review): Extract 0.8 to constant (_LOCAL_MODEL_CONFIDENCE_PENALTY)
        # TODO(code-review): Extract 0.4 to constant (_DEFAULT_LOW_CONFIDENCE)
        # TODO(code-review): Document the reasoning behind these magic numbers
        # TODO(code-review): Ensure return value is always in range [0.0, 1.0]
        # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if confidence_factors:
            base_confidence = sum(confidence_factors) / len(confidence_factors)
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–æ–Ω–∏ –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã)
            return base_confidence * 0.8
        return 0.4  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    def get_analyzer_info(self) -> dict[str, Any]:
        # TODO(code-review): Add Google-style docstring with Returns section
        # TODO(code-review): Consider using dataclass or TypedDict for return type
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ"""
        # TODO(code-review): Extract version to constant at module level
        # TODO(code-review): Extract all string literals to constants
        # TODO(code-review): Don't expose base_url (might be sensitive internal info)
        # TODO(code-review): Use semantic versioning strictly (current: 2.0.0)
        return {
            "name": "OllamaAnalyzer",
            "version": "2.0.0",
            "description": "Local AI analysis using Ollama models for experimentation and learning",
            "author": "Rap Scraper Project",
            "type": self.analyzer_type,
            "supported_features": self.supported_features,
            "model_info": {
                "model_name": self.model_name,
                "base_url": self.base_url,
                "provider": "Ollama Local",
                "temperature": self.temperature,
                "cost": "Free (local)",
            },
            "requirements": ["Ollama server running", "Model downloaded"],
            "available": self.available,
            "config_options": {
                # TODO(code-review): Extract default values to module constants
                # TODO(code-review): Use the actual constants instead of hardcoded strings
                "model_name": "Ollama model to use (default: llama3.2:3b)",
                "base_url": "Ollama server URL (default: http://localhost:11434)",
                "temperature": "Generation temperature (default: 0.1)",
                "timeout": "Request timeout in seconds (default: 60)",
            },
            "setup_instructions": [
                # TODO(code-review): Load instructions from external file or config
                # TODO(code-review): Add links to documentation
                "1. Install Ollama from https://ollama.ai",
                "2. Run: ollama serve",
                "3. Pull model: ollama pull llama3.2:3b",
                "4. Start analysis",
            ],
        }

    @property
    def analyzer_type(self) -> str:
        # TODO(code-review): Add Google-style docstring with Returns section
        """–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        # TODO(code-review): Extract "ai" to class-level constant
        # TODO(code-review): Consider using Enum for analyzer types
        return "ai"

    @property
    def supported_features(self) -> list[str]:
        # TODO(code-review): Add Google-style docstring with Returns section
        # TODO(code-review): Use tuple instead of list for immutable return value
        # TODO(code-review): Consider using Enum or constants for feature names
        """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        # TODO(code-review): Extract this list to class-level constant
        # TODO(code-review): Add documentation for what each feature means
        return [
            "basic_classification",
            "mood_analysis",
            "content_analysis",
            "technical_analysis",
            "quality_assessment",
            "experimental_features",
            "local_processing",
            "privacy_friendly",
            "cost_free",
        ]

# TODO(code-review): GENERAL FILE-LEVEL IMPROVEMENTS:
# 1. Add unit tests for all methods (especially _parse_response, _calculate_confidence)
# 2. Add integration tests with mock Ollama server
# 3. Consider dependency injection for requests library (easier mocking)
# 4. Add metrics/monitoring (e.g., track success rate, latency, model usage)
# 5. Consider circuit breaker pattern for Ollama connection failures
# 6. Add request/response logging for debugging (with privacy considerations)
# 7. Implement caching for repeated analyses
# 8. Add rate limiting to prevent overwhelming local Ollama instance
# 9. Consider async/await for non-blocking I/O operations
# 10. Add comprehensive error codes for different failure modes
# 11. Create custom exception hierarchy (OllamaError, ModelNotFoundError, etc.)
# 12. Add type checking with mypy in CI/CD pipeline
# 13. Run linters: pylint, flake8, black for formatting
# 14. Add performance profiling for slow methods
# 15. Document thread-safety considerations
# 16. Add examples in docstrings
# 17. Consider adding __repr__ and __str__ methods for debugging
# 18. Add logging of confidence scores distribution for monitoring
# 19. Consider feature flags for experimental functionality
# 20. Add deprecation warnings for any backwards-incompatible changes
