#!/usr/bin/env python3
"""
üìä CLI-—É—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏, —Ç–æ—á–Ω–æ—Å—Ç–∏, —Ä–µ—Å—É—Ä—Å–æ–≤ —Ä–∞–∑–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
python src/cli/performance_monitor.py --analyzer qwen      # –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Qwen
python src/cli/performance_monitor.py --all                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- Python 3.8+
- src/core/app.py, src/interfaces/analyzer_interface.py
- psutil, statistics

–†–ï–ó–£–õ–¨–¢–ê–¢:
- –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏

–ê–í–¢–û–†: AI Assistant
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""
import asyncio
import json
import time
import sys
import psutil
import statistics
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import logging

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.app import Application
from interfaces.analyzer_interface import AnalyzerType


@dataclass
class PerformanceMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    analyzer_name: str
    test_count: int
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    median_time: float
    
    # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    avg_cpu_percent: float
    avg_memory_mb: float
    peak_memory_mb: float
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    success_rate: float
    error_count: int
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
    items_per_second: float
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class PerformanceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
    
    def __init__(self, monitoring_interval: float = 0.1):
        self.app = Application()
        self.monitoring_interval = monitoring_interval
        self.logger = logging.getLogger(__name__)
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        self.cpu_measurements = []
        self.memory_measurements = []
        self.is_monitoring = False
    
    async def benchmark_analyzer(
        self,
        analyzer_type: str,
        test_texts: List[str],
        warmup_runs: int = 3
    ) -> PerformanceMetrics:
        """
        –ë–µ–Ω—á–º–∞—Ä–∫ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            analyzer_type: –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            test_texts: –¢–µ–∫—Å—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            warmup_runs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥—Ä–µ–≤–æ—á–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤
        """
        
        # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞–ø—Ä—è–º—É—é –ø–æ —Å—Ç—Ä–æ–∫–µ
        analyzer = self.app.get_analyzer(analyzer_type)
        if not analyzer:
            available = self.app.list_analyzers()
            raise ValueError(f"Unknown analyzer type: {analyzer_type}. Available: {available}")
        
        self.logger.info(f"üî¨ Benchmarking {analyzer_type} analyzer with {len(test_texts)} texts")
        
        # –ü—Ä–æ–≥—Ä–µ–≤–æ—á–Ω—ã–µ –∑–∞–ø—É—Å–∫–∏
        if warmup_runs > 0:
            self.logger.info(f"üî• Warmup runs: {warmup_runs}")
            warmup_text = test_texts[0] if test_texts else "Warmup text"
            for i in range(warmup_runs):
                try:
                    await analyzer.analyze(warmup_text)
                except Exception:
                    pass
        
        # –û—Å–Ω–æ–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.logger.info("üìä Starting performance measurement...")
        
        # –°–±—Ä–æ—Å –º–µ—Ç—Ä–∏–∫
        self.cpu_measurements.clear()
        self.memory_measurements.clear()
        
        # –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
        monitoring_task = asyncio.create_task(self._monitor_system_resources())
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ—Å—Ç—ã
        execution_times = []
        error_count = 0
        
        start_time = time.time()
        
        for i, text in enumerate(test_texts):
            self.logger.debug(f"Processing text {i+1}/{len(test_texts)}")
            
            text_start_time = time.time()
            try:
                # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –∫ —Ä–∞–∑–Ω—ã–º API –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
                if hasattr(analyzer, 'analyze'):
                    await analyzer.analyze(text)
                elif hasattr(analyzer, 'analyze_song'):
                    # –°—Ç–∞—Ä—ã–π API - –ø–µ—Ä–µ–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    analyzer.analyze_song("Unknown", f"Text_{i}", text)
                else:
                    raise RuntimeError(f"Analyzer has no analyze method")
                
                text_time = time.time() - text_start_time
                execution_times.append(text_time)
            except Exception as e:
                error_count += 1
                self.logger.warning(f"Error processing text {i+1}: {e}")
        
        total_time = time.time() - start_time
        
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        self.is_monitoring = False
        try:
            await monitoring_task
        except asyncio.CancelledError:
            pass  # –û–∂–∏–¥–∞–µ–º–∞—è –æ—Ç–º–µ–Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        success_count = len(execution_times)
        
        if execution_times:
            avg_time = statistics.mean(execution_times)
            min_time = min(execution_times)
            max_time = max(execution_times)
            median_time = statistics.median(execution_times)
        else:
            avg_time = min_time = max_time = median_time = 0
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        avg_cpu = statistics.mean(self.cpu_measurements) if self.cpu_measurements else 0
        avg_memory = statistics.mean(self.memory_measurements) if self.memory_measurements else 0
        peak_memory = max(self.memory_measurements) if self.memory_measurements else 0
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = PerformanceMetrics(
            analyzer_name=analyzer_type,
            test_count=len(test_texts),
            total_time=total_time,
            avg_time=avg_time,
            min_time=min_time,
            max_time=max_time,
            median_time=median_time,
            avg_cpu_percent=avg_cpu,
            avg_memory_mb=avg_memory,
            peak_memory_mb=peak_memory,
            success_rate=(success_count / len(test_texts) * 100) if test_texts else 0,
            error_count=error_count,
            items_per_second=(success_count / total_time) if total_time > 0 else 0
        )
        
        self.logger.info("‚úÖ Benchmarking completed")
        return metrics
    
    async def compare_analyzers(
        self,
        analyzer_types: List[str],
        test_texts: List[str],
        output_file: Optional[str] = None
    ) -> Dict[str, PerformanceMetrics]:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        
        self.logger.info(f"üèÅ Comparing {len(analyzer_types)} analyzers")
        
        results = {}
        
        for analyzer_type in analyzer_types:
            self.logger.info(f"Testing {analyzer_type}...")
            try:
                metrics = await self.benchmark_analyzer(analyzer_type, test_texts)
                results[analyzer_type] = metrics
                
                # –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç
                self.logger.info(f"  ‚è±Ô∏è  Avg time: {metrics.avg_time:.3f}s")
                self.logger.info(f"  üìà Success rate: {metrics.success_rate:.1f}%")
                self.logger.info(f"  üöÄ Throughput: {metrics.items_per_second:.1f} items/s")
                
            except Exception as e:
                self.logger.error(f"Failed to benchmark {analyzer_type}: {e}")
                continue
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_report = self._create_comparison_report(results)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if output_file:
            report_data = {
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                'test_config': {
                    'analyzers': analyzer_types,
                    'test_count': len(test_texts),
                    'total_chars': sum(len(text) for text in test_texts)
                },
                'individual_results': {k: v.to_dict() for k, v in results.items()},
                'comparison': comparison_report
            }
            
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"üìÑ Comparison report saved to: {output_file}")
        
        return results
    
    async def _monitor_system_resources(self) -> None:
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        self.is_monitoring = True
        process = psutil.Process()
        max_monitoring_time = 300  # –ú–∞–∫—Å–∏–º—É–º 5 –º–∏–Ω—É—Ç
        start_time = time.time()
        
        while self.is_monitoring and (time.time() - start_time < max_monitoring_time):
            try:
                # CPU (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
                cpu_percent = process.cpu_percent()
                self.cpu_measurements.append(cpu_percent)
                
                # –ü–∞–º—è—Ç—å (–≤ MB)
                memory_info = process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                self.memory_measurements.append(memory_mb)
                
                await asyncio.sleep(self.monitoring_interval)
                
            except asyncio.CancelledError:
                # –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–º–µ–Ω—É
                break
            except Exception:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
                pass
        
        self.is_monitoring = False
    
    def _create_comparison_report(
        self,
        results: Dict[str, PerformanceMetrics]
    ) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        
        if not results:
            return {}
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        by_speed = sorted(results.items(), key=lambda x: x[1].avg_time)
        by_throughput = sorted(results.items(), key=lambda x: x[1].items_per_second, reverse=True)
        by_success_rate = sorted(results.items(), key=lambda x: x[1].success_rate, reverse=True)
        by_memory = sorted(results.items(), key=lambda x: x[1].avg_memory_mb)
        
        comparison = {
            'fastest': {
                'analyzer': by_speed[0][0],
                'avg_time': by_speed[0][1].avg_time,
                'improvement_factor': by_speed[-1][1].avg_time / by_speed[0][1].avg_time if by_speed[0][1].avg_time > 0 else 1
            },
            'highest_throughput': {
                'analyzer': by_throughput[0][0],
                'items_per_second': by_throughput[0][1].items_per_second
            },
            'most_reliable': {
                'analyzer': by_success_rate[0][0],
                'success_rate': by_success_rate[0][1].success_rate
            },
            'most_memory_efficient': {
                'analyzer': by_memory[0][0],
                'avg_memory_mb': by_memory[0][1].avg_memory_mb
            },
            'rankings': {
                'by_speed': [(name, metrics.avg_time) for name, metrics in by_speed],
                'by_throughput': [(name, metrics.items_per_second) for name, metrics in by_throughput],
                'by_success_rate': [(name, metrics.success_rate) for name, metrics in by_success_rate],
                'by_memory': [(name, metrics.avg_memory_mb) for name, metrics in by_memory]
            }
        }
        
        return comparison


def generate_test_texts(count: int = 50) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
    
    base_texts = [
        # –ö–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
        "Happy song",
        "Sad lyrics",
        "Love ballad",
        "Angry rap",
        "Chill vibes",
        
        # –°—Ä–µ–¥–Ω–∏–µ —Ç–µ–∫—Å—Ç—ã
        "This is a beautiful song about love and happiness in life",
        "Dark and mysterious lyrics expressing deep emotional pain and sorrow",
        "Uplifting and motivational text encouraging people to follow their dreams",
        "Aggressive and intense rap lyrics with strong political messages",
        "Calm and peaceful meditation on nature and human existence",
        
        # –î–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        """This is a comprehensive analysis of modern hip-hop culture and its impact 
        on society, exploring themes of social justice, personal growth, and artistic expression 
        through complex metaphors and intricate wordplay that challenges conventional thinking 
        about music and its role in contemporary discourse.""",
        
        """A deeply emotional ballad that tells the story of loss, recovery, and finding 
        hope in the darkest moments of life, weaving together personal experiences with 
        universal themes of human resilience and the power of love to overcome adversity 
        and transform our understanding of what it means to be truly alive."""
    ]
    
    # –†–∞—Å—à–∏—Ä—è–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
    texts = []
    for i in range(count):
        base_text = base_texts[i % len(base_texts)]
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        variation = f" (variation {i//len(base_texts) + 1})" if i >= len(base_texts) else ""
        texts.append(base_text + variation)
    
    return texts


async def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    monitor = PerformanceMonitor()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_texts = generate_test_texts(20)
    
    print("üî¨ Performance Monitoring Demo")
    print(f"Test dataset: {len(test_texts)} texts")
    print()
    
    try:
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
        available_analyzers = ["algorithmic"]  # –ù–∞—á–Ω–µ–º —Å –æ–¥–Ω–æ–≥–æ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ
        
        results = await monitor.compare_analyzers(
            analyzer_types=available_analyzers,
            test_texts=test_texts,
            output_file="performance_report.json"
        )
        
        # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç
        print("\nüìä Performance Summary:")
        print("=" * 50)
        
        for analyzer_name, metrics in results.items():
            print(f"\n{analyzer_name.upper()} Analyzer:")
            print(f"  ‚è±Ô∏è  Average time: {metrics.avg_time:.3f}s")
            print(f"  üìè Time range: {metrics.min_time:.3f}s - {metrics.max_time:.3f}s")
            print(f"  üöÄ Throughput: {metrics.items_per_second:.1f} items/s")
            print(f"  üíæ Memory usage: {metrics.avg_memory_mb:.1f} MB (peak: {metrics.peak_memory_mb:.1f} MB)")
            print(f"  üìà Success rate: {metrics.success_rate:.1f}%")
        
    except Exception as e:
        print(f"‚ùå Performance monitoring failed: {e}")


if __name__ == "__main__":
    asyncio.run(main())
