"""
üßÆ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤ –ø–µ—Å–µ–Ω

–ö–õ–Æ–ß–ï–í–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø:
‚ú® –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º (–≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–∫–æ–Ω—á–∞–Ω–∏–π)
üéØ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ flow –∏ —Ä–∏—Ç–º–∞
üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Å–ª–æ–≤–∞—Ä—è–º–∏
üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ (Flesch, SMOG, ARI)
üéµ –ê–Ω–∞–ª–∏–∑ –º—É–∑—ã–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
üîÑ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
‚ö° –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
üìà –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- üéØ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI –º–æ–¥–µ–ª–µ–π
- ‚ö° –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö (57K+ —Ç—Ä–µ–∫–æ–≤)
- üìä Baseline –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏
- üóÑÔ∏è Production-ready –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Å PostgreSQL –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
- üìà –î–µ—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
üñ•Ô∏è CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:
  python src/analyzers/algorithmic_analyzer.py --stats
  python src/analyzers/algorithmic_analyzer.py --analyze-all --limit 100
  python src/analyzers/algorithmic_analyzer.py --analyze-track 123

üìù –ü—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:
  analyzer = AdvancedAlgorithmicAnalyzer()
  result = analyzer.analyze_song("Artist", "Title", "Lyrics...")

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- üêç Python 3.8+
- üóÑÔ∏è PostgreSQL —Å asyncpg
- üìÑ PyYAML –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
- üîß src/interfaces/analyzer_interface.py

–†–ï–ó–£–õ–¨–¢–ê–¢:
- üéµ –ê–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º: —Å—Ö–µ–º–∞, –ø–ª–æ—Ç–Ω–æ—Å—Ç—å, —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
- üåä Flow –º–µ—Ç—Ä–∏–∫–∏: –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≥–æ–≤, —Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
- üìö –ß–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å: Flesch, SMOG, ARI, Coleman-Liau –∏–Ω–¥–µ–∫—Å—ã
- üí≠ –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å, –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å, —Å–ª–æ–∂–Ω–æ—Å—Ç—å
- üé® –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: –¥–µ–Ω—å–≥–∏, —É–ª–∏—Ü–∞, —É—Å–ø–µ—Ö, –æ—Ç–Ω–æ—à–µ–Ω–∏—è
- ‚úçÔ∏è –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–µ–º—ã: –º–µ—Ç–∞—Ñ–æ—Ä—ã, –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏—è, –ø–æ–≤—Ç–æ—Ä—ã
- üìä –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ, –∞—Ä—Ç–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å

–í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
- üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö PostgreSQL
- üîç –ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤ –ø–æ ID
- üöÄ –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –∏ –±–∞—Ç—á–∏–Ω–≥–æ–º
- üíæ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- üé≠ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º —Å –ø—Ä–∏–º–µ—Ä–æ–º

–ê–í–¢–û–†: Rap Scraper Project Team
–í–ï–†–°–ò–Ø: 2.0.0 Advanced
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""

# TODO(google-review): [STYLE] Organize imports: stdlib, third-party, local
# TODO(google-review): [STYLE] Add module-level docstring after imports
import asyncio
import hashlib
import logging
import math
import re
import time
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

# –ò–º–ø–æ—Ä—Ç—ã —Å fallback –¥–ª—è standalone –∑–∞–ø—É—Å–∫–∞
try:
    from config.config_loader import get_config
    from interfaces.analyzer_interface import (
        AnalysisResult,
        BaseAnalyzer,
        register_analyzer,
    )

    PROJECT_IMPORT_SUCCESS = True
except ImportError:
    PROJECT_IMPORT_SUCCESS = False
    import sys
    from pathlib import Path

    # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –∏ –¥–æ–±–∞–≤–∏—Ç—å src –≤ –ø—É—Ç—å
    current_dir = Path(__file__).resolve().parent
    possible_roots = [current_dir.parent.parent, current_dir.parent, current_dir]

    for root in possible_roots:
        src_path = root / "src"
        if src_path.exists() and (src_path / "interfaces").exists():
            if str(src_path) not in sys.path:
                sys.path.append(str(src_path))
            try:
                from interfaces.analyzer_interface import (
                    AnalysisResult,
                    BaseAnalyzer,
                    register_analyzer,
                )

                PROJECT_IMPORT_SUCCESS = True
                break
            except ImportError:
                continue

    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏
    if not PROJECT_IMPORT_SUCCESS:
        # –ë–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è standalone —Ä–∞–±–æ—Ç—ã
        class BaseAnalyzer:
            def __init__(self, config: dict[str, Any] | None = None):
                self.config = config or {}

            def validate_input(self, artist: str, title: str, lyrics: str) -> bool:
                return bool(artist and title and lyrics and len(lyrics.strip()) > 10)

            def preprocess_lyrics(self, lyrics: str) -> str:
                return re.sub(r"\s+", " ", lyrics.strip())

        @dataclass
        class AnalysisResult:
            artist: str
            title: str
            analysis_type: str
            confidence: float
            metadata: dict[str, Any]
            raw_output: dict[str, Any]
            processing_time: float
            timestamp: str

        def register_analyzer(name: str):  # noqa: ARG001
            def decorator(cls):
                return cls

            return decorator


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class PhoneticPattern:
    """–§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Ñ–º"""

    vowel_sounds: dict[str, list[str]] = field(
        default_factory=lambda: {
            "ay": ["ai", "ay", "ey", "a_e"],
            "ee": ["ee", "ea", "ie", "y"],
            "oh": ["o", "oa", "ow", "o_e"],
            "oo": ["oo", "u", "ew", "ue"],
            "ah": ["a", "au", "aw"],
            "ih": ["i", "y", "ie"],
            "eh": ["e", "ea", "ai"],
            "uh": ["u", "o", "ou"],
        }
    )

    consonant_clusters: set[str] = field(
        default_factory=lambda: {
            "ch",
            "sh",
            "th",
            "wh",
            "ck",
            "ng",
            "ph",
            "gh",
            "st",
            "sp",
            "sc",
            "sk",
            "sm",
            "sn",
            "sw",
            "sl",
            "bl",
            "br",
            "cl",
            "cr",
            "dr",
            "fl",
            "fr",
            "gl",
            "gr",
            "pl",
            "pr",
            "tr",
            "tw",
        }
    )


class AdvancedLexicon:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    # TODO(google-review): [DOCSTRING] Add Args, Returns sections to docstring
    # TODO(google-review): [ARCHITECTURE] Consider loading from config file

    def __init__(self):
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≥—Ä–∞–¥–∞—Ü–∏–µ–π –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
        self.emotions = {
            "joy": {
                "weak": {"happy", "good", "nice", "okay", "fine", "content"},
                "medium": {"great", "wonderful", "amazing", "excellent", "fantastic"},
                "strong": {
                    "ecstatic",
                    "euphoric",
                    "blissful",
                    "overjoyed",
                    "triumphant",
                },
            },
            "anger": {
                "weak": {"annoyed", "bothered", "upset", "frustrated", "irritated"},
                "medium": {"angry", "mad", "furious", "pissed", "heated"},
                "strong": {"rage", "wrath", "livid", "enraged", "incensed"},
            },
            "sadness": {
                "weak": {"sad", "down", "blue", "low", "melancholy"},
                "medium": {"depressed", "miserable", "heartbroken", "devastated"},
                "strong": {"suicidal", "hopeless", "despairing", "anguished"},
            },
            "fear": {
                "weak": {"worried", "nervous", "anxious", "concerned", "uneasy"},
                "medium": {"scared", "afraid", "frightened", "terrified"},
                "strong": {"petrified", "horrified", "panic", "dread"},
            },
        }

        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ä—ç–ø–∞
        self.rap_themes = {
            "money_wealth": {
                "money",
                "cash",
                "bread",
                "dough",
                "paper",
                "green",
                "bills",
                "rich",
                "wealth",
                "fortune",
                "bank",
                "account",
                "invest",
                "luxury",
                "expensive",
                "diamond",
                "gold",
                "platinum",
                "mansion",
                "penthouse",
                "yacht",
                "lambo",
                "ferrari",
                "bentley",
            },
            "street_life": {
                "street",
                "hood",
                "block",
                "corner",
                "ghetto",
                "projects",
                "hustle",
                "grind",
                "struggle",
                "survive",
                "real",
                "raw",
                "concrete",
                "pavement",
                "alley",
                "neighborhood",
            },
            "success_ambition": {
                "success",
                "win",
                "victory",
                "champion",
                "boss",
                "king",
                "queen",
                "crown",
                "throne",
                "empire",
                "legend",
                "icon",
                "achieve",
                "accomplish",
                "conquer",
                "dominate",
                "rise",
            },
            "relationships_love": {
                "love",
                "heart",
                "baby",
                "girl",
                "woman",
                "man",
                "relationship",
                "kiss",
                "hug",
                "romance",
                "passion",
                "soul",
                "forever",
                "together",
                "commitment",
                "loyalty",
                "trust",
            },
            "violence_conflict": {
                "gun",
                "shoot",
                "kill",
                "murder",
                "death",
                "blood",
                "war",
                "fight",
                "battle",
                "enemy",
                "revenge",
                "violence",
                "attack",
                "weapon",
                "bullet",
                "trigger",
                "blast",
                "destroy",
            },
            "drugs_party": {
                "weed",
                "smoke",
                "high",
                "drunk",
                "party",
                "club",
                "dance",
                "drink",
                "bottle",
                "shot",
                "pill",
                "lean",
                "molly",
                "celebration",
                "turn_up",
                "lit",
                "wild",
                "crazy",
            },
        }

        # –°–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º—ã—à–ª–µ–Ω–∏—è
        self.complexity_indicators = {
            "philosophical": {
                "existence",
                "reality",
                "consciousness",
                "purpose",
                "meaning",
                "truth",
                "wisdom",
                "knowledge",
                "understand",
                "perspective",
                "philosophy",
                "metaphysical",
                "spiritual",
                "transcend",
            },
            "abstract": {
                "concept",
                "theory",
                "principle",
                "ideology",
                "paradigm",
                "dimension",
                "universe",
                "infinity",
                "eternal",
                "essence",
                "phenomenon",
                "manifestation",
                "transformation",
                "evolution",
            },
            "analytical": {
                "analyze",
                "examine",
                "investigate",
                "evaluate",
                "assess",
                "consider",
                "contemplate",
                "reflect",
                "introspect",
                "ponder",
                "calculate",
                "measure",
                "compare",
                "contrast",
                "deduce",
            },
        }

        # –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–µ–º—ã –≤ —Ç–µ–∫—Å—Ç–∞—Ö
        self.literary_devices = {
            "metaphor_indicators": {
                "like",
                "as",
                "than",
                "seems",
                "appears",
                "resembles",
                "similar",
                "reminds",
                "symbolize",
                "represent",
                "embody",
            },
            "time_references": {
                "yesterday",
                "today",
                "tomorrow",
                "past",
                "present",
                "future",
                "memory",
                "remember",
                "forget",
                "history",
                "destiny",
                "fate",
            },
            "contrast_words": {
                "but",
                "however",
                "although",
                "despite",
                "nevertheless",
                "nonetheless",
                "whereas",
                "while",
                "opposite",
                "contrast",
            },
        }


class FlowAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä flow –∏ —Ä–∏—Ç–º–∞"""

    def __init__(self):
        self.phonetic_patterns = PhoneticPattern()

    def analyze_flow_patterns(self, lines: list[str]) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ flow"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns, Raises sections
        # TODO(google-review): [ARCHITECTURE] Function too long (40+ lines)
        if not lines:
            return self._empty_flow_result()

        syllable_patterns = []
        stress_patterns = []
        line_lengths = []

        for line in lines:
            syllables = self._count_syllables_advanced(line)
            syllable_patterns.append(syllables)
            line_lengths.append(len(line.split()))

            # –ê–Ω–∞–ª–∏–∑ —É–¥–∞—Ä–µ–Ω–∏–π (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            stress_pattern = self._analyze_stress_pattern(line)
            stress_patterns.append(stress_pattern)

        return {
            "syllable_consistency": self._calculate_consistency(syllable_patterns),
            "average_syllables_per_line": sum(syllable_patterns)
            / len(syllable_patterns),
            "syllable_variance": self._calculate_variance(syllable_patterns),
            "line_length_consistency": self._calculate_consistency(line_lengths),
            "stress_pattern_regularity": self._analyze_stress_regularity(
                stress_patterns
            ),
            "flow_breaks": self._count_flow_interruptions(lines),
            "rhythmic_density": self._calculate_rhythmic_density(lines),
        }

    def _count_syllables_advanced(self, text: str) -> int:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≥–æ–≤"""
        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = re.findall(r"\b[a-zA-Z]+\b", text.lower())
        if not words:
            return 0

        total_syllables = 0
        for word in words:
            syllables = self._syllables_in_word(word)
            total_syllables += syllables

        return total_syllables

    def _syllables_in_word(self, word: str) -> int:
        """–¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≥–æ–≤ –≤ —Å–ª–æ–≤–µ"""
        if len(word) <= 2:
            return 1

        word = word.lower().strip()

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏
        special_cases = {
            "the": 1,
            "a": 1,
            "an": 1,
            "and": 1,
            "or": 1,
            "but": 1,
            "through": 1,
            "though": 1,
            "every": 2,
            "very": 2,
            "people": 2,
            "little": 2,
            "middle": 2,
            "simple": 2,
        }

        if word in special_cases:
            return special_cases[word]

        # –ü–æ–¥—Å—á–µ—Ç –≥–ª–∞—Å–Ω—ã—Ö –≥—Ä—É–ø–ø
        vowels = "aeiouy"
        syllable_count = 0
        prev_was_vowel = False

        for i, char in enumerate(word):
            is_vowel = char in vowels

            if is_vowel:
                # –ù–æ–≤–∞—è –≥–ª–∞—Å–Ω–∞—è –≥—Ä—É–ø–ø–∞
                if not prev_was_vowel:
                    syllable_count += 1
                # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è –¥–∏—Ñ—Ç–æ–Ω–≥–æ–≤
                elif i > 0 and word[i - 1 : i + 1] in [
                    "ai",
                    "au",
                    "ea",
                    "ee",
                    "ei",
                    "ie",
                    "oa",
                    "oo",
                    "ou",
                    "ue",
                ]:
                    pass  # –ù–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫

            prev_was_vowel = is_vowel

        # –ò—Å–∫–ª—é—á–µ–Ω–∏—è
        # TODO(google-review): [STYLE] Line exceeds 80 chars, break into multiple
        if word.endswith("e") and syllable_count > 1 and not word.endswith(("le", "se", "me", "ne", "ve", "ze", "de", "ge")):
            syllable_count -= 1

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        if word.endswith(("ed", "es", "er", "ly")):
            pass  # –£–∂–µ —É—á—Ç–µ–Ω–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∞–ª–≥–æ—Ä–∏—Ç–º–µ

        return max(1, syllable_count)

    def _analyze_stress_pattern(self, line: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —É–¥–∞—Ä–µ–Ω–∏–π (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [ARCHITECTURE] Magic numbers: 3 should be constant
        words = line.split()
        if not words:
            return ""

        stress_pattern = []
        for word in words:
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è —É–¥–∞—Ä–µ–Ω–∏–π
            # TODO(google-review): [ARCHITECTURE] Magic number 3
            if len(word) <= 3:
                stress_pattern.append("1")  # –£–¥–∞—Ä–Ω—ã–π
            elif word.lower() in {"the", "and", "but", "for", "with", "from", "into"}:
                stress_pattern.append("0")  # –ë–µ–∑—É–¥–∞—Ä–Ω—ã–π
            else:
                # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤ —Å—Ç–∞–≤–∏–º —É–¥–∞—Ä–µ–Ω–∏–µ –Ω–∞ –ø–µ—Ä–≤—ã–π —Å–ª–æ–≥
                stress_pattern.append("1")

        return "".join(stress_pattern)

    def _calculate_consistency(self, values: list[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–π"""
        if len(values) < 2:
            return 1.0

        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º: —á–µ–º –º–µ–Ω—å—à–µ –≤–∞—Ä–∏–∞—Ü–∏—è, —Ç–µ–º –≤—ã—à–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
        consistency = 1 / (1 + variance)
        return min(consistency, 1.0)

    def _calculate_variance(self, values: list[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏"""
        if len(values) < 2:
            return 0.0

        mean = sum(values) / len(values)
        return sum((x - mean) ** 2 for x in values) / len(values)

    def _analyze_stress_regularity(self, stress_patterns: list[str]) -> float:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —É–¥–∞—Ä–µ–Ω–∏–π"""
        if not stress_patterns:
            return 0.0

        # –ò—â–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        pattern_counts = Counter(stress_patterns)
        most_common = pattern_counts.most_common(1)

        if most_common:
            return most_common[0][1] / len(stress_patterns)

        return 0.0

    def _count_flow_interruptions(self, lines: list[str]) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–π flow"""
        interruptions = 0
        punctuation = {".", "!", "?", ";", ":", ",", "--", "..."}

        for line in lines:
            for punct in punctuation:
                interruptions += line.count(punct)

        return interruptions

    def _calculate_rhythmic_density(self, lines: list[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏"""
        if not lines:
            return 0.0

        total_words = sum(len(line.split()) for line in lines)
        total_syllables = sum(self._count_syllables_advanced(line) for line in lines)

        if total_words == 0:
            return 0.0

        # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å = —Å–ª–æ–≥–∏ –Ω–∞ —Å–ª–æ–≤–æ
        density = total_syllables / total_words
        return min(density / 2.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0, 1]

    def _empty_flow_result(self) -> dict[str, Any]:
        """–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ flow"""
        return {
            "syllable_consistency": 0.0,
            "average_syllables_per_line": 0.0,
            "syllable_variance": 0.0,
            "line_length_consistency": 0.0,
            "stress_pattern_regularity": 0.0,
            "flow_breaks": 0,
            "rhythmic_density": 0.0,
        }


class RhymeAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–∏—Ñ–º —Å —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"""

    def __init__(self):
        self.phonetic_patterns = PhoneticPattern()
        self.rhyme_cache = {}

    def analyze_rhyme_structure(self, lines: list[str]) -> dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [ARCHITECTURE] Function too long (40+ lines)
        # TODO(google-review): [ARCHITECTURE] Magic number 2
        if len(lines) < 2:
            return self._empty_rhyme_result()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è —Å—Ç—Ä–æ–∫
        line_endings = self._extract_line_endings(lines)

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–∏—Ñ–º
        perfect_rhymes = self._find_perfect_rhymes(line_endings)
        near_rhymes = self._find_near_rhymes(line_endings)
        internal_rhymes = self._find_internal_rhymes(lines)

        # –°—Ö–µ–º–∞ —Ä–∏—Ñ–º–æ–≤–∫–∏
        rhyme_scheme = self._detect_complex_rhyme_scheme(line_endings)

        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        phonetic_similarity = self._calculate_phonetic_similarity(line_endings)
        rhyme_density = self._calculate_rhyme_density(
            line_endings, perfect_rhymes, near_rhymes
        )

        return {
            "perfect_rhymes": len(perfect_rhymes),
            "near_rhymes": len(near_rhymes),
            "internal_rhymes": len(internal_rhymes),
            "rhyme_scheme": rhyme_scheme,
            "rhyme_scheme_complexity": self._evaluate_scheme_complexity(rhyme_scheme),
            "phonetic_similarity_score": phonetic_similarity,
            "rhyme_density": rhyme_density,
            "alliteration_score": self._calculate_alliteration(lines),
            "assonance_score": self._calculate_assonance(lines),
            "consonance_score": self._calculate_consonance(lines),
        }

    def _extract_line_endings(self, lines: list[str]) -> list[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏–π —Å—Ç—Ä–æ–∫ —Å –æ—á–∏—Å—Ç–∫–æ–π"""
        endings = []
        for line in lines:
            # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ
            words = re.findall(r"\b[a-zA-Z]+\b", line)
            if words:
                ending = words[-1].lower()
                endings.append(ending)
            else:
                endings.append("")
        return endings

    def _find_perfect_rhymes(self, endings: list[str]) -> list[tuple[int, int]]:
        """–ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Ä–∏—Ñ–º"""
        # TODO(google-review): [PERFORMANCE] O(n¬≤) complexity, consider optimization
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        return [
            (i, j)
            for i in range(len(endings))
            for j in range(i + 1, len(endings))
            if self._is_perfect_rhyme(endings[i], endings[j])
        ]

    def _find_near_rhymes(self, endings: list[str]) -> list[tuple[int, int]]:
        """–ü–æ–∏—Å–∫ –Ω–µ—Ç–æ—á–Ω—ã—Ö —Ä–∏—Ñ–º"""
        # TODO(google-review): [PERFORMANCE] O(n¬≤) complexity, consider optimization
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        return [
            (i, j)
            for i in range(len(endings))
            for j in range(i + 1, len(endings))
            if not self._is_perfect_rhyme(endings[i], endings[j])
            and self._is_near_rhyme(endings[i], endings[j])
        ]

    def _find_internal_rhymes(self, lines: list[str]) -> list[tuple[int, str, str]]:
        """–ü–æ–∏—Å–∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ä–∏—Ñ–º"""
        # TODO(google-review): [PERFORMANCE] Nested loops O(n*m¬≤), optimize
        # TODO(google-review): [ARCHITECTURE] Magic number 3
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        internal_rhymes = []
        for line_idx, line in enumerate(lines):
            words = re.findall(r"\b[a-zA-Z]{3,}\b", line.lower())
            internal_rhymes.extend([
                (line_idx, words[i], words[j])
                for i in range(len(words))
                for j in range(i + 1, len(words))
                if self._is_perfect_rhyme(words[i], words[j])
                or self._is_near_rhyme(words[i], words[j])
            ])
        return internal_rhymes

    def _is_perfect_rhyme(self, word1: str, word2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ–π —Ä–∏—Ñ–º—ã —Å —É—á–µ—Ç–æ–º —Ñ–æ–Ω–µ—Ç–∏–∫–∏"""
        if not word1 or not word2 or word1 == word2:
            return False

        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        cache_key = tuple(sorted([word1, word2]))
        if cache_key in self.rhyme_cache:
            return self.rhyme_cache[cache_key]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è–º —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
        result = False
        for suffix_len in range(2, min(len(word1), len(word2)) + 1):
            if word1[-suffix_len:] == word2[-suffix_len:]:
                result = True
                break

        # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        if not result:
            result = self._phonetic_rhyme_check(word1, word2)

        self.rhyme_cache[cache_key] = result
        return result

    def _is_near_rhyme(self, word1: str, word2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ—Ç–æ—á–Ω–æ–π —Ä–∏—Ñ–º—ã"""
        if not word1 or not word2 or len(word1) < 2 or len(word2) < 2:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–∑–≤—É—á–∏–µ –≥–ª–∞—Å–Ω—ã—Ö (assonance)
        vowels1 = [c for c in word1[-3:] if c in "aeiou"]
        vowels2 = [c for c in word2[-3:] if c in "aeiou"]

        if vowels1 and vowels2 and vowels1[-1] == vowels2[-1]:
            return True

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–∑–≤—É—á–∏–µ —Å–æ–≥–ª–∞—Å–Ω—ã—Ö (consonance)
        consonants1 = [c for c in word1[-3:] if c not in "aeiou"]
        consonants2 = [c for c in word2[-3:] if c not in "aeiou"]

        return len(set(consonants1) & set(consonants2)) >= 1

    def _phonetic_rhyme_check(self, word1: str, word2: str) -> bool:
        """–§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∏—Ñ–º—ã"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã phonetic matching –∞–ª–≥–æ—Ä–∏—Ç–º

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–∏–µ –∑–≤—É–∫–∏
        phonetic_groups = {
            "k_sounds": ["c", "k", "ck", "q"],
            "s_sounds": ["s", "c", "z"],
            "f_sounds": ["f", "ph", "gh"],
            "long_a": ["a", "ai", "ay", "ei"],
            "long_e": ["e", "ee", "ea", "ie"],
            "long_i": ["i", "ie", "y", "igh"],
            "long_o": ["o", "oa", "ow", "ough"],
            "long_u": ["u", "ue", "ew", "ou"],
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è –Ω–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        end1 = word1[-2:]
        end2 = word2[-2:]

        for group in phonetic_groups.values():
            if any(end1.endswith(sound) for sound in group) and any(
                end2.endswith(sound) for sound in group
            ):
                return True

        return False

    def _detect_complex_rhyme_scheme(self, endings: list[str]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ–π —Å—Ö–µ–º—ã —Ä–∏—Ñ–º–æ–≤–∫–∏"""
        # TODO(google-review): [ARCHITECTURE] Magic numbers: 4, 16 as constants
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        if len(endings) < 4:
            return "insufficient"

        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 16 —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–µ–º—ã
        sample = endings[:16]

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–∏—Ñ–º—É—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞
        rhyme_groups = {}
        scheme = []
        next_letter = "A"

        for ending in sample:
            assigned_letter = None

            # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≥—Ä—É–ø–ø—É
            for group_word, letter in rhyme_groups.items():
                if self._is_perfect_rhyme(ending, group_word) or self._is_near_rhyme(
                    ending, group_word
                ):
                    assigned_letter = letter
                    break

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
            if assigned_letter is None:
                assigned_letter = next_letter
                rhyme_groups[ending] = next_letter
                next_letter = chr(ord(next_letter) + 1)

            scheme.append(assigned_letter)

        return "".join(scheme)

    def _evaluate_scheme_complexity(self, scheme: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å—Ö–µ–º—ã —Ä–∏—Ñ–º–æ–≤–∫–∏"""
        if not scheme or scheme == "insufficient":
            return 0.0

        # –§–∞–∫—Ç–æ—Ä—ã —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        unique_rhymes = len(set(scheme))
        total_lines = len(scheme)

        # –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        patterns = {"ABAB": 0.6, "AABB": 0.4, "ABCB": 0.7, "ABBA": 0.8, "AAAA": 0.2}

        complexity_score = unique_rhymes / total_lines

        # –ë–æ–Ω—É—Å –∑–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern, bonus in patterns.items():
            if pattern in scheme:
                complexity_score += bonus * 0.1

        return min(complexity_score, 1.0)

    def _calculate_phonetic_similarity(self, endings: list[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        if len(endings) < 2:
            return 0.0

        similarity_scores = []

        for i in range(len(endings)):
            for j in range(i + 1, len(endings)):
                score = self._phonetic_similarity_score(endings[i], endings[j])
                similarity_scores.append(score)

        return (
            sum(similarity_scores) / len(similarity_scores)
            if similarity_scores
            else 0.0
        )

    def _phonetic_similarity_score(self, word1: str, word2: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–≤—É—Ö —Å–ª–æ–≤"""
        if not word1 or not word2:
            return 0.0

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
        max_len = min(len(word1), len(word2), 4)
        similarity = 0.0

        for i in range(1, max_len + 1):
            if word1[-i:] == word2[-i:]:
                similarity += i * 0.25

        return min(similarity, 1.0)

    def _calculate_rhyme_density(
        self, endings: list[str], perfect_rhymes: list, near_rhymes: list
    ) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ä–∏—Ñ–º"""
        # TODO(google-review): [ARCHITECTURE] Magic numbers: 2, 0.7 as constants
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        if len(endings) < 2:
            return 0.0

        total_rhymes = (
            len(perfect_rhymes) + len(near_rhymes) * 0.7
        )  # Near rhymes —Å—á–∏—Ç–∞—é—Ç—Å—è —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º
        max_possible_rhymes = (
            len(endings) // 2
        )  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ä–∏—Ñ–º

        return (
            min(total_rhymes / max_possible_rhymes, 1.0)
            if max_possible_rhymes > 0
            else 0.0
        )

    def _calculate_alliteration(self, lines: list[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏"""
        # TODO(google-review): [ARCHITECTURE] Magic numbers: 2, 0.5 as constants
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        if not lines:
            return 0.0

        alliteration_count = 0
        total_word_pairs = 0

        for line in lines:
            words = [word.lower() for word in re.findall(r"\b[a-zA-Z]{2,}\b", line)]
            if len(words) < 2:
                continue

            for i in range(len(words) - 1):
                total_word_pairs += 1
                if words[i][0] == words[i + 1][0]:
                    alliteration_count += 1

                    # –ë–æ–Ω—É—Å –∑–∞ –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏—é —á–µ—Ä–µ–∑ —Å–ª–æ–≤–æ
                    if i < len(words) - 2 and words[i][0] == words[i + 2][0]:
                        alliteration_count += 0.5

        return alliteration_count / max(total_word_pairs, 1)

    def _calculate_assonance(self, lines: list[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∞—Å—Å–æ–Ω–∞–Ω—Å–∞ (–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –≥–ª–∞—Å–Ω—ã—Ö)"""
        if not lines:
            return 0.0

        vowels = "aeiou"
        assonance_count = 0
        total_comparisons = 0

        for line in lines:
            words = [word.lower() for word in re.findall(r"\b[a-zA-Z]{3,}\b", line)]

            for i in range(len(words)):
                for j in range(
                    i + 1, min(i + 3, len(words))
                ):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–∏–∂–∞–π—à–∏–µ —Å–ª–æ–≤–∞
                    vowels_i = [c for c in words[i] if c in vowels]
                    vowels_j = [c for c in words[j] if c in vowels]

                    if vowels_i and vowels_j:
                        total_comparisons += 1
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≥–ª–∞—Å–Ω—ã—Ö
                        common_vowels = set(vowels_i) & set(vowels_j)
                        if common_vowels:
                            assonance_count += len(common_vowels) / max(
                                len(vowels_i), len(vowels_j)
                            )

        return assonance_count / max(total_comparisons, 1)

    def _calculate_consonance(self, lines: list[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ–Ω—Å–æ–Ω–∞–Ω—Å–∞ (–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–Ω—ã—Ö)"""
        if not lines:
            return 0.0

        vowels = "aeiou"
        consonance_count = 0
        total_comparisons = 0

        for line in lines:
            words = [word.lower() for word in re.findall(r"\b[a-zA-Z]{3,}\b", line)]

            for i in range(len(words)):
                for j in range(i + 1, min(i + 3, len(words))):
                    consonants_i = [
                        c for c in words[i] if c not in vowels and c.isalpha()
                    ]
                    consonants_j = [
                        c for c in words[j] if c not in vowels and c.isalpha()
                    ]

                    if consonants_i and consonants_j:
                        total_comparisons += 1
                        common_consonants = set(consonants_i) & set(consonants_j)
                        if common_consonants:
                            consonance_count += len(common_consonants) / max(
                                len(consonants_i), len(consonants_j)
                            )

        return consonance_count / max(total_comparisons, 1)

    def _empty_rhyme_result(self) -> dict[str, Any]:
        """–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Ñ–º"""
        return {
            "perfect_rhymes": 0,
            "near_rhymes": 0,
            "internal_rhymes": 0,
            "rhyme_scheme": "insufficient",
            "rhyme_scheme_complexity": 0.0,
            "phonetic_similarity_score": 0.0,
            "rhyme_density": 0.0,
            "alliteration_score": 0.0,
            "assonance_score": 0.0,
            "consonance_score": 0.0,
        }


class ReadabilityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""

    def analyze_readability(self, text: str) -> dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [ARCHITECTURE] Function too long (40+ lines)
        if not text.strip():
            return self._empty_readability_result()

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        sentences = self._count_sentences(text)
        words = self._count_words(text)
        syllables = self._count_total_syllables(text)

        if sentences == 0 or words == 0:
            return self._empty_readability_result()

        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏
        flesch_score = self._calculate_flesch_reading_ease(sentences, words, syllables)
        flesch_kincaid_grade = self._calculate_flesch_kincaid_grade(
            sentences, words, syllables
        )
        smog_index = self._calculate_smog_index(text, sentences)
        ari_score = self._calculate_ari(sentences, words, text)
        coleman_liau = self._calculate_coleman_liau(text, sentences, words)

        return {
            "flesch_reading_ease": flesch_score,
            "flesch_kincaid_grade": flesch_kincaid_grade,
            "smog_index": smog_index,
            "automated_readability_index": ari_score,
            "coleman_liau_index": coleman_liau,
            "average_sentence_length": words / sentences,
            "average_syllables_per_word": syllables / words,
            "readability_consensus": self._calculate_consensus(
                flesch_score, flesch_kincaid_grade, smog_index
            ),
        }

    def _count_sentences(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ
        sentences = re.split(r"[.!?]+", text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return len(sentences)

    def _count_words(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤"""
        words = re.findall(r"\b[a-zA-Z]+\b", text)
        return len(words)

    def _count_total_syllables(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≥–æ–≤"""
        # TODO(google-review): [PERFORMANCE] Creating FlowAnalyzer in loop expensive
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        words = re.findall(r"\b[a-zA-Z]+\b", text.lower())
        total_syllables = 0

        flow_analyzer = FlowAnalyzer()
        for word in words:
            total_syllables += flow_analyzer._syllables_in_word(word)

        return total_syllables

    def _calculate_flesch_reading_ease(
        self, sentences: int, words: int, syllables: int
    ) -> float:
        """–ò–Ω–¥–µ–∫—Å —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ –§–ª–µ—à–∞"""
        # TODO(google-review): [ARCHITECTURE] Magic numbers as constants
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        if sentences == 0 or words == 0:
            return 0.0

        asl = words / sentences  # Average Sentence Length
        asw = syllables / words  # Average Syllables per Word

        score = 206.835 - (1.015 * asl) - (84.6 * asw)
        return max(0, min(100, score))

    def _calculate_flesch_kincaid_grade(
        self, sentences: int, words: int, syllables: int
    ) -> float:
        """–ò–Ω–¥–µ–∫—Å —É—Ä–æ–≤–Ω—è –∫–ª–∞—Å—Å–∞ –§–ª–µ—à–∞-–ö–∏–Ω–∫–µ–π–¥–∞"""
        if sentences == 0 or words == 0:
            return 0.0

        asl = words / sentences
        asw = syllables / words

        grade = (0.39 * asl) + (11.8 * asw) - 15.59
        return max(0, grade)

    def _calculate_smog_index(self, text: str, sentences: int) -> float:
        """–ò–Ω–¥–µ–∫—Å SMOG"""
        # TODO(google-review): [PERFORMANCE] Creating FlowAnalyzer in method
        # TODO(google-review): [ARCHITECTURE] Magic numbers: 3, 30
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        if sentences < 3:
            return 0.0

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–∞ —Å 3+ —Å–ª–æ–≥–∞–º–∏
        words = re.findall(r"\b[a-zA-Z]+\b", text.lower())
        complex_words = 0
        flow_analyzer = FlowAnalyzer()

        for word in words:
            if flow_analyzer._syllables_in_word(word) >= 3:
                complex_words += 1

        if complex_words == 0:
            return 0.0

        # SMOG = 3 + ‚àö(complex_words * 30 / sentences)
        return 3 + math.sqrt(complex_words * 30 / sentences)

    def _calculate_ari(self, sentences: int, words: int, text: str) -> float:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ (ARI)"""
        if sentences == 0 or words == 0:
            return 0.0

        characters = len(re.sub(r"[^a-zA-Z]", "", text))

        if characters == 0:
            return 0.0

        ari = (4.71 * (characters / words)) + (0.5 * (words / sentences)) - 21.43
        return max(0, ari)

    def _calculate_coleman_liau(self, text: str, sentences: int, words: int) -> float:
        """–ò–Ω–¥–µ–∫—Å –ö–æ—É–ª–º–∞–Ω–∞-–õ–∏–∞—É"""
        if words == 0 or sentences == 0:
            return 0.0

        characters = len(re.sub(r"[^a-zA-Z]", "", text))

        letters_per_100 = (characters / words) * 100  # Average letters per 100 words
        sentences_per_100 = (sentences / words) * 100  # Average sentences per 100 words

        cli = (0.0588 * letters_per_100) - (0.296 * sentences_per_100) - 15.8
        return max(0, cli)

    def _calculate_consensus(self, flesch: float, fk_grade: float, smog: float) -> str:
        """–ö–æ–Ω—Å–µ–Ω—Å—É—Å –ø–æ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏"""
        # TODO(google-review): [ARCHITECTURE] Magic numbers, use dict/config
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Flesch –≤ –ø—Ä–∏–º–µ—Ä–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–ª–∞—Å—Å–∞
        if flesch >= 90:
            flesch_grade = 5
        elif flesch >= 80:
            flesch_grade = 6
        elif flesch >= 70:
            flesch_grade = 7
        elif flesch >= 60:
            flesch_grade = 8
        elif flesch >= 50:
            flesch_grade = 9
        elif flesch >= 30:
            flesch_grade = 12
        else:
            flesch_grade = 16

        # –°—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ —É—Ä–æ–≤–Ω–µ–π
        avg_grade = (flesch_grade + fk_grade + smog) / 3

        if avg_grade <= 6:
            return "elementary"
        if avg_grade <= 8:
            return "middle_school"
        if avg_grade <= 12:
            return "high_school"
        if avg_grade <= 16:
            return "college"
        return "graduate"

    def _empty_readability_result(self) -> dict[str, Any]:
        """–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return {
            "flesch_reading_ease": 0.0,
            "flesch_kincaid_grade": 0.0,
            "smog_index": 0.0,
            "automated_readability_index": 0.0,
            "coleman_liau_index": 0.0,
            "average_sentence_length": 0.0,
            "average_syllables_per_word": 0.0,
            "readability_consensus": "insufficient",
        }


@register_analyzer("advanced_algorithmic")
class AdvancedAlgorithmicAnalyzer(BaseAnalyzer):
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º

    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º
    - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ flow –∏ —Ä–∏—Ç–º–∞
    - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏
    - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å –≥—Ä–∞–¥–∞—Ü–∏–µ–π —ç–º–æ—Ü–∏–π
    - –ê–Ω–∞–ª–∏–∑ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö –ø—Ä–∏–µ–º–æ–≤
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    # TODO(google-review): [ARCHITECTURE] God class - too many responsibilities
    # TODO(google-review): [DOCSTRING] Add Args, Returns to class docstring

    def __init__(self, config: dict[str, Any] | None = None):
        # TODO(google-review): [PERFORMANCE] Cache has no max size limit
        # TODO(google-review): [DOCSTRING] Add Args section to docstring
        super().__init__(config)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.lexicon = AdvancedLexicon()
        self.flow_analyzer = FlowAnalyzer()
        self.rhyme_analyzer = RhymeAnalyzer()
        self.readability_analyzer = ReadabilityAnalyzer()

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.cache_enabled = self.config.get("cache_enabled", True)
        self.analysis_cache = {} if self.cache_enabled else None
        self.detailed_logging = self.config.get("detailed_logging", False)

        if self.detailed_logging:
            logger.setLevel(logging.DEBUG)

    def analyze_song(self, artist: str, title: str, lyrics: str) -> AnalysisResult:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Å–Ω–∏ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏
        """
        # TODO(google-review): [DOCSTRING] Add Args, Returns, Raises sections
        # TODO(google-review): [ARCHITECTURE] Function too long (90+ lines)
        # TODO(google-review): [ERROR_HANDLING] Generic ValueError, be specific
        start_time = time.time()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = None
        if self.cache_enabled:
            cache_key = self._generate_cache_key(artist, title, lyrics)
            if cache_key in self.analysis_cache:
                cached_result = self.analysis_cache[cache_key]
                logger.debug(f"Returning cached result for {artist} - {title}")
                return cached_result

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        processed_lyrics = self.preprocess_lyrics(lyrics)
        lines = self._split_into_lines(processed_lyrics)
        words = self._extract_meaningful_words(processed_lyrics)

        if self.detailed_logging:
            logger.debug(
                f"Processing {artist} - {title}: {len(lines)} lines, {len(words)} words"
            )

        # –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑
        analysis_results = {
            "advanced_sentiment": self._analyze_advanced_sentiment(words),
            "rhyme_analysis": self.rhyme_analyzer.analyze_rhyme_structure(lines),
            "flow_analysis": self.flow_analyzer.analyze_flow_patterns(lines),
            "readability_metrics": self.readability_analyzer.analyze_readability(
                processed_lyrics
            ),
            "thematic_analysis": self._analyze_themes_advanced(words),
            "literary_devices": self._analyze_literary_devices(processed_lyrics, words),
            "vocabulary_sophistication": self._analyze_vocabulary_sophistication(words),
            "structural_analysis": self._analyze_structure_advanced(
                lines, processed_lyrics
            ),
            "creativity_metrics": self._analyze_creativity_advanced(
                processed_lyrics, words, lines
            ),
        }

        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        composite_scores = self._calculate_advanced_composite_scores(analysis_results)
        analysis_results.update(composite_scores)

        # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = self._calculate_advanced_confidence(analysis_results, lines, words)

        processing_time = time.time() - start_time

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            "analyzer_version": "2.0.0",
            "processing_date": datetime.now(tz=timezone.utc).isoformat(),
            "lyrics_length": len(processed_lyrics),
            "word_count": len(words),
            "line_count": len(lines),
            "processing_components": list(analysis_results.keys()),
            "cache_used": False,
            "detailed_logging": self.detailed_logging,
        }

        # Build plain dictionary result (legacy-friendly)
        result_dict = {
            "analysis_type": "advanced_algorithmic",
            "analysis_data": analysis_results,
            "confidence": confidence,
            "processing_time": processing_time,
            "metadata": metadata,
            "raw_output": analysis_results,
            "timestamp": datetime.now(tz=timezone.utc).isoformat(),
            "artist": artist,
            "title": title,
        }

        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if self.cache_enabled and cache_key:
            self.analysis_cache[cache_key] = result_dict

        if self.detailed_logging:
            logger.debug(
                f"Analysis completed for {artist} - {title} in {processing_time:.3f}s"
            )

        return result_dict

    def _generate_cache_key(self, artist: str, title: str, lyrics: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –¥–ª—è –∫—ç—à–∞"""
        # TODO(google-review): [SECURITY] MD5 deprecated, use SHA256
        # TODO(google-review): [ARCHITECTURE] Magic number 500 as constant
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        content = (
            f"{artist}|{title}|{lyrics[:500]}"  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        )
        return hashlib.md5(content.encode()).hexdigest()

    def _split_into_lines(self, lyrics: str) -> list[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–æ–∫–∏ —Å –æ—á–∏—Å—Ç–∫–æ–π"""
        lines = [line.strip() for line in lyrics.split("\n") if line.strip()]

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        return [
            line
            for line in lines
            if not re.match(
                r"^\[.*\]$|^\(.*\)$|^(Verse|Chorus|Bridge|Outro|Intro)[\s\d]*:",
                line,
                re.IGNORECASE,
            )
        ]

    def _extract_meaningful_words(self, lyrics: str) -> list[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤"""
        # TODO(google-review): [PERFORMANCE] Stop words recreated each call
        # TODO(google-review): [ARCHITECTURE] Move stop_words to class constant
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        words = re.findall(r"\b[a-zA-Z]{2,}\b", lyrics.lower())

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
        stop_words = {
            "the",
            "a",
            "an",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
            "by",
            "from",
            "up",
            "about",
            "into",
            "through",
            "during",
            "before",
            "after",
            "above",
            "below",
            "between",
            "among",
            "this",
            "that",
            "these",
            "those",
            "i",
            "me",
            "my",
            "myself",
            "we",
            "our",
            "ours",
            "ourselves",
            "you",
            "your",
            "yours",
            "he",
            "him",
            "his",
            "himself",
            "she",
            "her",
            "hers",
            "herself",
            "it",
            "its",
            "itself",
            "they",
            "them",
            "their",
            "theirs",
            "themselves",
            "what",
            "which",
            "who",
            "whom",
            "am",
            "is",
            "are",
            "was",
            "were",
            "be",
            "been",
            "being",
            "have",
            "has",
            "had",
            "having",
            "do",
            "does",
            "did",
            "doing",
            "will",
            "would",
            "could",
            "should",
            "yeah",
            "uh",
            "oh",
            "got",
            "get",
            "gotta",
            "wanna",
            "gonna",
            "ain",
            "yall",
            "em",
            "ya",
            "like",
            "just",
            "now",
            "know",
            "see",
            "come",
            "go",
            "say",
            "said",
            "tell",
            "make",
            "way",
            "time",
            "want",
            "need",
            "take",
            "give",
            "put",
            "keep",
            "let",
            "think",
        }

        # TODO(google-review): [ARCHITECTURE] Magic number 3 as constant
        return [word for word in words if word not in stop_words and len(word) >= 3]

    def _analyze_advanced_sentiment(
        self, words: list[str]
    ) -> dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Å –≥—Ä–∞–¥–∞—Ü–∏–µ–π"""
        # TODO(google-review): [ARCHITECTURE] Function too long (60+ lines)
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        if not words:
            return self._empty_sentiment_result()

        emotion_scores = {}
        total_emotional_words = 0

        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —ç–º–æ—Ü–∏–π —Å —É—á–µ—Ç–æ–º –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
        for emotion, intensity_levels in self.lexicon.emotions.items():
            emotion_score = 0.0
            emotion_word_count = 0

            for intensity, word_set in intensity_levels.items():
                matches = len(set(words) & word_set)
                if matches > 0:
                    # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
                    # TODO(google-review): [ARCHITECTURE] Move weights to class const
                    intensity_weight = {"weak": 1.0, "medium": 2.0, "strong": 3.0}[
                        intensity
                    ]
                    emotion_score += matches * intensity_weight
                    emotion_word_count += matches
                    total_emotional_words += matches

            emotion_scores[emotion] = {
                "score": emotion_score,
                "word_count": emotion_word_count,
                "normalized_score": emotion_score / len(words) if words else 0,
            }

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–π —ç–º–æ—Ü–∏–∏
        if emotion_scores:
            dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1]["score"])
            dominant_emotion_name = dominant_emotion[0]
            dominant_emotion_strength = dominant_emotion[1]["score"]
        else:
            dominant_emotion_name = "neutral"
            dominant_emotion_strength = 0

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–π —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        positive_emotions = emotion_scores.get("joy", {}).get("score", 0)
        negative_emotions = (
            emotion_scores.get("anger", {}).get("score", 0)
            + emotion_scores.get("sadness", {}).get("score", 0)
            + emotion_scores.get("fear", {}).get("score", 0)
        )

        if total_emotional_words > 0:
            valence = (positive_emotions - negative_emotions) / total_emotional_words
        else:
            valence = 0.0

        return {
            "emotion_scores": emotion_scores,
            "dominant_emotion": dominant_emotion_name,
            "dominant_emotion_strength": dominant_emotion_strength,
            "emotional_valence": valence,
            "emotional_intensity": total_emotional_words / len(words) if words else 0,
            "total_emotional_words": total_emotional_words,
            "emotional_complexity": len(
                [e for e in emotion_scores.values() if e["score"] > 0]
            ),
        }

    def _analyze_themes_advanced(self, words: list[str]) -> dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [ARCHITECTURE] Function too long (50+ lines)
        if not words:
            return {"theme_scores": {}, "dominant_theme": "neutral"}

        word_set = set(words)
        theme_scores = {}

        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for theme, theme_words in self.lexicon.rap_themes.items():
            matches = len(word_set & theme_words)
            theme_scores[theme] = {
                "absolute_count": matches,
                "relative_score": matches / len(words),
                "theme_coverage": matches / len(theme_words) if theme_words else 0,
            }

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏—Ö —Ç–µ–º
        sorted_themes = sorted(
            theme_scores.items(), key=lambda x: x[1]["absolute_count"], reverse=True
        )

        dominant_theme = (
            sorted_themes[0][0]
            if sorted_themes and sorted_themes[0][1]["absolute_count"] > 0
            else "neutral"
        )

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        active_themes = [
            theme
            for theme, scores in theme_scores.items()
            if scores["absolute_count"] > 0
        ]
        thematic_diversity = len(active_themes) / len(self.lexicon.rap_themes)

        return {
            "theme_scores": theme_scores,
            "dominant_theme": dominant_theme,
            "secondary_themes": [
                theme[0]
                for theme in sorted_themes[1:4]
                if theme[1]["absolute_count"] > 0
            ],
            "thematic_diversity": thematic_diversity,
            "total_thematic_words": sum(
                score["absolute_count"] for score in theme_scores.values()
            ),
        }

    def _analyze_literary_devices(
        self, lyrics: str, words: list[str]
    ) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö –ø—Ä–∏–µ–º–æ–≤"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [ARCHITECTURE] Function too long (50+ lines)
        if not lyrics or not words:
            return self._empty_literary_result()

        # –ü–æ–∏—Å–∫ –º–µ—Ç–∞—Ñ–æ—Ä –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–π
        metaphor_count = 0
        simile_count = 0
        lyrics_lower = lyrics.lower()

        for indicator in self.lexicon.literary_devices["metaphor_indicators"]:
            if indicator in ["like", "as"]:
                simile_count += lyrics_lower.count(f" {indicator} ")
            else:
                metaphor_count += lyrics_lower.count(indicator)

        # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—Ç—Å—ã–ª–æ–∫
        time_references = sum(
            lyrics_lower.count(time_word)
            for time_word in self.lexicon.literary_devices["time_references"]
        )

        # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–æ–≤ –∏ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π
        contrast_usage = sum(
            lyrics_lower.count(contrast_word)
            for contrast_word in self.lexicon.literary_devices["contrast_words"]
        )

        # –ê–Ω–∞–ª–∏–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ —Ä–µ—Ñ—Ä–µ–Ω–æ–≤
        line_repetitions = self._analyze_repetitions(lyrics)

        # –ü–µ—Ä—Å–æ–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        personification_indicators = [
            "speaks",
            "whispers",
            "calls",
            "cries",
            "laughs",
            "dances",
        ]
        personification_count = sum(
            lyrics_lower.count(indicator) for indicator in personification_indicators
        )

        return {
            "metaphor_count": metaphor_count,
            "simile_count": simile_count,
            "time_references": time_references,
            "contrast_usage": contrast_usage,
            "repetition_analysis": line_repetitions,
            "personification_count": personification_count,
            "total_literary_devices": metaphor_count
            + simile_count
            + time_references
            + contrast_usage
            + personification_count,
        }

    def _analyze_repetitions(self, lyrics: str) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [ARCHITECTURE] Magic numbers: 2, 5
        lines = [line.strip().lower() for line in lyrics.split("\n") if line.strip()]

        if not lines:
            return {"repeated_lines": 0, "repetition_ratio": 0.0}

        line_counts = Counter(lines)
        repeated_lines = {
            line: count for line, count in line_counts.items() if count > 1
        }

        # –ê–Ω–∞–ª–∏–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑ (2-4 —Å–ª–æ–≤–∞)
        phrase_repetitions = defaultdict(int)
        for line in lines:
            words = line.split()
            for i in range(len(words) - 1):
                for j in range(2, min(5, len(words) - i + 1)):
                    phrase = " ".join(words[i : i + j])
                    if len(phrase) > 5:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã
                        phrase_repetitions[phrase] += 1

        repeated_phrases = {
            phrase: count for phrase, count in phrase_repetitions.items() if count > 1
        }

        return {
            "repeated_lines": len(repeated_lines),
            "repeated_phrases": len(repeated_phrases),
            "repetition_ratio": len(repeated_lines) / len(set(lines)) if lines else 0,
            "most_repeated_line": max(line_counts.items(), key=lambda x: x[1])
            if line_counts
            else None,
            "total_line_repetitions": sum(
                count - 1 for count in line_counts.values() if count > 1
            ),
        }

    def _analyze_vocabulary_sophistication(self, words: list[str]) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤–∞—Ä—è"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [ARCHITECTURE] Function too long (70+ lines)
        # TODO(google-review): [PERFORMANCE] common_words recreated each call
        if not words:
            return self._empty_vocabulary_result()

        word_set = set(words)

        # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º—ã—à–ª–µ–Ω–∏—è
        complexity_scores = {}
        total_complex_words = 0

        for category, category_words in self.lexicon.complexity_indicators.items():
            matches = len(word_set & category_words)
            complexity_scores[category] = matches
            total_complex_words += matches

        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤
        # TODO(google-review): [ARCHITECTURE] Magic number 7 as constant
        word_lengths = [len(word) for word in words]
        avg_word_length = sum(word_lengths) / len(word_lengths)
        long_words = len([w for w in words if len(w) >= 7])

        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞—Ä—è
        vocabulary_richness = len(word_set) / len(words)

        # –†–µ–¥–∫–∏–µ/–Ω–µ–æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        common_words = {
            "love",
            "money",
            "life",
            "time",
            "good",
            "bad",
            "right",
            "wrong",
            "black",
            "white",
            "red",
            "blue",
            "big",
            "small",
            "old",
            "new",
            "high",
            "low",
            "fast",
            "slow",
            "hot",
            "cold",
            "young",
            "real",
            "hard",
            "easy",
            "free",
            "game",
            "play",
            "work",
            "home",
            "house",
        }

        uncommon_words = len(word_set - common_words)
        uncommon_ratio = uncommon_words / len(words)

        return {
            "complexity_scores": complexity_scores,
            "total_complex_words": total_complex_words,
            "average_word_length": avg_word_length,
            "long_words_count": long_words,
            "vocabulary_richness": vocabulary_richness,
            "uncommon_words_ratio": uncommon_ratio,
            "lexical_diversity": len(word_set) / max(len(words), 1),
            "sophisticated_vocabulary_score": (
                total_complex_words + long_words + uncommon_words
            )
            / len(words),
        }

    def _analyze_structure_advanced(
        self, lines: list[str], full_text: str
    ) -> dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        if not lines:
            return self._empty_structure_result()

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        total_lines = len(lines)
        line_lengths = [len(line.split()) for line in lines]
        avg_line_length = sum(line_lengths) / len(line_lengths)
        line_length_variance = sum(
            (x - avg_line_length) ** 2 for x in line_lengths
        ) / len(line_lengths)

        # –ê–Ω–∞–ª–∏–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –ø–∞—É–∑—ã
        punctuation_analysis = self._analyze_punctuation(full_text)

        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å
        structure_patterns = self._find_structure_patterns(lines)

        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–ø–æ –ø—É—Å—Ç—ã–º —Å—Ç—Ä–æ–∫–∞–º)
        stanzas = self._identify_stanzas(full_text)

        return {
            "total_lines": total_lines,
            "average_line_length": avg_line_length,
            "line_length_variance": line_length_variance,
            "punctuation_analysis": punctuation_analysis,
            "structure_patterns": structure_patterns,
            "stanza_analysis": stanzas,
            "structural_consistency": 1.0
            - (line_length_variance / max(avg_line_length, 1)),
        }

    def _analyze_punctuation(self, text: str) -> dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏"""
        punctuation_counts = {
            "periods": text.count("."),
            "commas": text.count(","),
            "exclamations": text.count("!"),
            "questions": text.count("?"),
            "semicolons": text.count(";"),
            "colons": text.count(":"),
            "dashes": text.count("-") + text.count("--"),
            "ellipses": text.count("..."),
            "parentheses": text.count("(") + text.count(")"),
            "quotations": text.count('"') + text.count("'"),
        }

        total_punctuation = sum(punctuation_counts.values())
        punctuation_counts["total"] = total_punctuation

        return punctuation_counts

    def _find_structure_patterns(self, lines: list[str]) -> dict[str, Any]:
        """–ü–æ–∏—Å–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if len(lines) < 4:
            return {"pattern_found": False}

        # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª–∏–Ω—ã —Å—Ç—Ä–æ–∫
        line_lengths = [len(line.split()) for line in lines]

        # –ü–æ–∏—Å–∫ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, ABAB –ø–æ –¥–ª–∏–Ω–µ)
        pattern_length = 4
        patterns_found = []

        for i in range(len(line_lengths) - pattern_length + 1):
            pattern = line_lengths[i : i + pattern_length]
            # –ò—â–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —ç—Ç–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            for j in range(i + pattern_length, len(line_lengths) - pattern_length + 1):
                if line_lengths[j : j + pattern_length] == pattern:
                    patterns_found.append(pattern)
                    break

        return {
            "pattern_found": len(patterns_found) > 0,
            "patterns": patterns_found,
            "pattern_consistency": len(patterns_found)
            / max(len(line_lengths) // pattern_length, 1),
        }

    def _identify_stanzas(self, text: str) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫
        stanzas = [
            stanza.strip() for stanza in re.split(r"\n\s*\n", text) if stanza.strip()
        ]

        if not stanzas:
            stanzas = [text]  # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω—ã—Ö —Å—Ç—Ä–æ—Ñ, –≤–µ—Å—å —Ç–µ–∫—Å—Ç - –æ–¥–Ω–∞ —Å—Ç—Ä–æ—Ñ–∞

        stanza_lengths = [len(stanza.split("\n")) for stanza in stanzas]

        return {
            "stanza_count": len(stanzas),
            "average_stanza_length": sum(stanza_lengths) / len(stanza_lengths)
            if stanza_lengths
            else 0,
            "stanza_length_consistency": self._calculate_consistency_score(
                stanza_lengths
            ),
            "stanza_lengths": stanza_lengths,
        }

    def _analyze_creativity_advanced(
        self, lyrics: str, words: list[str], lines: list[str]
    ) -> dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [ARCHITECTURE] Function too long (40+ lines)
        if not words or not lines:
            return self._empty_creativity_result()

        # –ù–µ–æ–ª–æ–≥–∏–∑–º—ã –∏ –Ω–µ–æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã
        neologisms = self._detect_neologisms(words)

        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        unique_phrases = self._find_unique_phrases(lines)

        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–¥–≤–∏–≥–∏ –∏ –∏–≥—Ä–∞ —Å–ª–æ–≤
        wordplay_analysis = self._analyze_advanced_wordplay(lyrics, words)

        # –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å —Ä–∏—Ñ–º
        innovative_rhymes = self._analyze_rhyme_innovation(lines)

        # –û–±—â–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
        creativity_factors = [
            len(neologisms) / max(len(words), 1),
            len(unique_phrases) / max(len(lines), 1),
            wordplay_analysis["total_score"],
            innovative_rhymes["innovation_score"],
        ]

        overall_creativity = sum(creativity_factors) / len(creativity_factors)

        return {
            "neologisms": neologisms,
            "unique_phrases": unique_phrases,
            "wordplay_analysis": wordplay_analysis,
            "innovative_rhymes": innovative_rhymes,
            "creativity_factors": creativity_factors,
            "overall_creativity_score": overall_creativity,
        }

    def _detect_neologisms(self, words: list[str]) -> list[str]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–µ–æ–ª–æ–≥–∏–∑–º–æ–≤ –∏ –Ω–µ–æ–±—ã—á–Ω—ã—Ö —Å–ª–æ–≤"""
        # TODO(google-review): [ARCHITECTURE] Magic numbers: 6, 4, 10
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω—ã—Ö –Ω–µ–æ–ª–æ–≥–∏–∑–º–æ–≤
        potential_neologisms = []

        for word in words:
            # –°–ª–æ–≤–∞ —Å –Ω–µ–æ–±—ã—á–Ω—ã–º–∏ —Å—É—Ñ—Ñ–∏–∫—Å–∞–º–∏ –∏–ª–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏
            if len(word) > 6 and (
                word.endswith(("ness", "tion", "ism"))
                or word.startswith(("un", "pre", "over"))
            ):
                potential_neologisms.append(word)

            # –°–ª–æ–≤–∞ —Å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è —á–∞—Å—Ç—è–º–∏
            if len(word) > 4:
                mid = len(word) // 2
                if word[:mid] == word[mid:] or word[:mid] in word[mid:]:
                    potential_neologisms.append(word)

        return potential_neologisms[
            :10
        ]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

    def _find_unique_phrases(self, lines: list[str]) -> list[str]:
        """–ü–æ–∏—Å–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑–æ–≤—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"""
        unique_phrases = []

        # –ò—â–µ–º —Ñ—Ä–∞–∑—ã —Å –Ω–µ–æ–±—ã—á–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
        for line in lines:
            words = line.split()
            if len(words) >= 3:
                # –ü–æ–∏—Å–∫ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
                if len(words) >= 4 and words[0].lower() in [
                    "when",
                    "where",
                    "how",
                    "why",
                ]:
                    unique_phrases.append(line)

                # –ü–æ–∏—Å–∫ –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏–π –≤ —Ñ—Ä–∞–∑–∞—Ö
                if (
                    len(
                        [
                            w
                            for w in words[:3]
                            if w and words[0] and w[0].lower() == words[0][0].lower()
                        ]
                    )
                    >= 2
                ):
                    unique_phrases.append(line)

        return unique_phrases[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

    def _analyze_advanced_wordplay(
        self, lyrics: str, words: list[str]
    ) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–µ–º–æ–≤ –∏–≥—Ä—ã —Å–ª–æ–≤"""
        # TODO(google-review): [ARCHITECTURE] Magic numbers as constants
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [PERFORMANCE] Nested loops inefficient
        wordplay_score = 0
        techniques_found = []

        # –î–≤–æ–π–Ω—ã–µ —Å–º—ã—Å–ª—ã (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        double_meanings = [
            word
            for word in set(words)
            if len(word) > 4
            and any(
                other in word
                for other in words
                if other != word and len(other) > 2
            )
        ]

        if double_meanings:
            wordplay_score += len(double_meanings) * 0.1
            techniques_found.append("double_meanings")

        # –ó–≤—É–∫–æ–ø–æ–¥—Ä–∞–∂–∞–Ω–∏—è
        onomatopoeia = [
            "bang",
            "boom",
            "crash",
            "pop",
            "snap",
            "crack",
            "splash",
            "whoosh",
        ]
        onomatopoeia_count = sum(lyrics.lower().count(sound) for sound in onomatopoeia)

        if onomatopoeia_count > 0:
            wordplay_score += onomatopoeia_count * 0.05
            techniques_found.append("onomatopoeia")

        # –ö–∞–ª–∞–º–±—É—Ä—ã (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        puns_detected = 0
        for i, word in enumerate(words[:-1]):
            next_word = words[i + 1]
            if (
                len(word) > 3
                and len(next_word) > 3
                and word[:-1] == next_word[:-1]
                and word != next_word
            ):
                puns_detected += 1

        if puns_detected > 0:
            wordplay_score += puns_detected * 0.15
            techniques_found.append("potential_puns")

        return {
            "total_score": min(wordplay_score, 1.0),
            "techniques_found": techniques_found,
            "double_meanings": double_meanings,
            "onomatopoeia_count": onomatopoeia_count,
            "potential_puns": puns_detected,
        }

    def _analyze_rhyme_innovation(self, lines: list[str]) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç–∏ —Ä–∏—Ñ–º"""
        # TODO(google-review): [ARCHITECTURE] Magic numbers: 4, 12, 3, 6
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        if len(lines) < 4:
            return {"innovation_score": 0.0}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è
        endings = []
        for line in lines[:12]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 12 —Å—Ç—Ä–æ–∫
            words = line.split()
            if words:
                ending = re.sub(r"[^\w]", "", words[-1].lower())
                if len(ending) >= 2:
                    endings.append(ending)

        innovation_factors = []

        # –ú—É–ª—å—Ç–∏—Å–ª–æ–∂–Ω—ã–µ —Ä–∏—Ñ–º—ã
        multisyllabic_rhymes = 0
        flow_analyzer = FlowAnalyzer()
        for ending in endings:
            if flow_analyzer._syllables_in_word(ending) >= 3:
                multisyllabic_rhymes += 1

        if endings:
            multisyllabic_ratio = multisyllabic_rhymes / len(endings)
            innovation_factors.append(multisyllabic_ratio)

        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∏—Ñ–º—ã
        internal_rhyme_count = 0
        for line in lines:
            words = line.split()
            for i in range(len(words) - 1):
                for j in range(i + 1, len(words)):
                    if self._simple_rhyme_check(words[i], words[j]):
                        internal_rhyme_count += 1

        internal_rhyme_ratio = internal_rhyme_count / max(len(lines), 1)
        innovation_factors.append(min(internal_rhyme_ratio, 1.0))

        # –ù–µ–æ–±—ã—á–Ω—ã–µ —Ä–∏—Ñ–º—ã (–¥–ª–∏–Ω–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è)
        long_rhymes = len([e for e in endings if len(e) >= 6])
        long_rhyme_ratio = long_rhymes / max(len(endings), 1)
        innovation_factors.append(long_rhyme_ratio)

        overall_innovation = (
            sum(innovation_factors) / len(innovation_factors)
            if innovation_factors
            else 0
        )

        return {
            "innovation_score": overall_innovation,
            "multisyllabic_rhymes": multisyllabic_rhymes,
            "internal_rhymes": internal_rhyme_count,
            "long_rhymes": long_rhymes,
            "innovation_factors": innovation_factors,
        }

    def _simple_rhyme_check(self, word1: str, word2: str) -> bool:
        """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∏—Ñ–º—ã"""
        if len(word1) < 2 or len(word2) < 2 or word1 == word2:
            return False
        return word1[-2:].lower() == word2[-2:].lower()

    def _calculate_advanced_composite_scores(
        self, analysis_results: dict[str, Any]
    ) -> dict[str, Any]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫"""
        # TODO(google-review): [ARCHITECTURE] Magic weights should be constants
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        rhyme_density = analysis_results.get("rhyme_analysis", {}).get(
            "rhyme_density", 0
        )
        flow_consistency = analysis_results.get("flow_analysis", {}).get(
            "syllable_consistency", 0
        )
        vocabulary_richness = analysis_results.get("vocabulary_sophistication", {}).get(
            "vocabulary_richness", 0
        )
        creativity_score = analysis_results.get("creativity_metrics", {}).get(
            "overall_creativity_score", 0
        )
        readability = analysis_results.get("readability_metrics", {}).get(
            "flesch_reading_ease", 0
        )

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º readability (Flesch scale: 0-100, higher = easier)
        normalized_readability = readability / 100

        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        technical_mastery = (
            rhyme_density * 0.4 + flow_consistency * 0.4 + vocabulary_richness * 0.2
        )

        artistic_sophistication = (
            creativity_score * 0.5
            + vocabulary_richness * 0.3
            + (1 - normalized_readability) * 0.2
        )  # –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π —Ç–µ–∫—Å—Ç = –≤—ã—à–µ –∞—Ä—Ç–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å

        overall_quality = (
            technical_mastery * 0.4
            + artistic_sophistication * 0.4
            + creativity_score * 0.2
        )

        # –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å
        innovation_score = creativity_score * 0.6 + rhyme_density * 0.4

        return {
            "composite_scores": {
                "technical_mastery": technical_mastery,
                "artistic_sophistication": artistic_sophistication,
                "overall_quality": overall_quality,
                "innovation_score": innovation_score,
                "complexity_balance": (
                    vocabulary_richness + (1 - normalized_readability)
                )
                / 2,
            }
        }

    def _calculate_advanced_confidence(
        self, analysis_results: dict[str, Any], lines: list[str], words: list[str]
    ) -> float:
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        # TODO(google-review): [ARCHITECTURE] Magic numbers: 100, 10, 8, 2
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        confidence_factors = []

        # –§–∞–∫—Ç–æ—Ä –æ–±—ä–µ–º–∞ —Ç–µ–∫—Å—Ç–∞
        text_volume_factor = min(len(words) / 100, 1.0) * min(len(lines) / 10, 1.0)
        confidence_factors.append(text_volume_factor)

        # –§–∞–∫—Ç–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞
        expected_analyses = [
            "advanced_sentiment",
            "rhyme_analysis",
            "flow_analysis",
            "readability_metrics",
        ]
        completed_analyses = sum(
            1 for analysis in expected_analyses if analysis in analysis_results
        )
        completeness_factor = completed_analyses / len(expected_analyses)
        confidence_factors.append(completeness_factor)

        # –§–∞–∫—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        if words and lines:
            avg_line_length = sum(len(line.split()) for line in lines) / len(lines)
            quality_factor = min(
                avg_line_length / 8, 1.0
            )  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏ ~ 8 —Å–ª–æ–≤
            confidence_factors.append(quality_factor)

        # –§–∞–∫—Ç–æ—Ä —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å–ª–æ–≤–∞—Ä—è
        if words:
            vocab_diversity = len(set(words)) / len(words)
            diversity_factor = min(vocab_diversity * 2, 1.0)
            confidence_factors.append(diversity_factor)

        return (
            sum(confidence_factors) / len(confidence_factors)
            if confidence_factors
            else 0.5
        )

    def _calculate_consistency_score(self, values: list[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–π"""
        if len(values) < 2:
            return 1.0

        mean = sum(values) / len(values)
        if mean == 0:
            return 1.0

        variance = sum((x - mean) ** 2 for x in values) / len(values)
        coefficient_of_variation = (variance**0.5) / mean

        # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: —á–µ–º –º–µ–Ω—å—à–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏, —Ç–µ–º –≤—ã—à–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
        consistency = 1 / (1 + coefficient_of_variation)
        return min(consistency, 1.0)

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –ø—É—Å—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    def _empty_sentiment_result(self) -> dict[str, Any]:
        # TODO(google-review): [TYPING] Add return type hint
        return {
            "emotion_scores": {},
            "dominant_emotion": "neutral",
            "dominant_emotion_strength": 0,
            "emotional_valence": 0.0,
            "emotional_intensity": 0.0,
            "total_emotional_words": 0,
            "emotional_complexity": 0,
        }

    def _empty_literary_result(self) -> dict[str, Any]:
        # TODO(google-review): [TYPING] Add return type hint
        return {
            "metaphor_count": 0,
            "simile_count": 0,
            "time_references": 0,
            "contrast_usage": 0,
            "repetition_analysis": {"repeated_lines": 0, "repetition_ratio": 0.0},
            "personification_count": 0,
            "total_literary_devices": 0,
        }

    def _empty_vocabulary_result(self) -> dict[str, Any]:
        # TODO(google-review): [TYPING] Add return type hint
        return {
            "complexity_scores": {},
            "total_complex_words": 0,
            "average_word_length": 0.0,
            "long_words_count": 0,
            "vocabulary_richness": 0.0,
            "uncommon_words_ratio": 0.0,
            "lexical_diversity": 0.0,
            "sophisticated_vocabulary_score": 0.0,
        }

    def _empty_structure_result(self) -> dict[str, Any]:
        # TODO(google-review): [TYPING] Add return type hint
        return {
            "total_lines": 0,
            "average_line_length": 0.0,
            "line_length_variance": 0.0,
            "punctuation_analysis": {},
            "structure_patterns": {"pattern_found": False},
            "stanza_analysis": {"stanza_count": 0},
            "structural_consistency": 0.0,
        }

    def _empty_creativity_result(self) -> dict[str, Any]:
        # TODO(google-review): [TYPING] Add return type hint
        return {
            "neologisms": [],
            "unique_phrases": [],
            "wordplay_analysis": {"total_score": 0.0},
            "innovative_rhymes": {"innovation_score": 0.0},
            "creativity_factors": [0.0, 0.0, 0.0, 0.0],
            "overall_creativity_score": 0.0,
        }

    def get_analyzer_info(self) -> dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ"""
        return {
            "name": "AdvancedAlgorithmicAnalyzer",
            "version": "2.0.0",
            "description": "Advanced algorithmic lyrics analysis with phonetic rhyme analysis, flow metrics, readability indices, and semantic sophistication scoring",
            "author": "Rap Scraper Project - Advanced Edition",
            "type": self.analyzer_type,
            "supported_features": self.supported_features,
            "components": [
                "AdvancedLexicon",
                "FlowAnalyzer",
                "RhymeAnalyzer",
                "ReadabilityAnalyzer",
                "LiteraryDevicesAnalyzer",
            ],
            "config_options": {
                "cache_enabled": "Enable result caching for performance (default: True)",
                "detailed_logging": "Enable detailed debug logging (default: False)",
                "min_word_length": "Minimum word length for analysis (default: 3)",
                "max_cache_size": "Maximum cache entries (default: 1000)",
            },
            "performance": {
                "typical_processing_time": "50-200ms per song",
                "memory_usage": "~5-10MB for cache",
                "scalability": "Excellent for batch processing",
            },
        }

    @property
    def analyzer_type(self) -> str:
        """–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        return "advanced_algorithmic"

    @property
    def supported_features(self) -> list[str]:
        """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        return [
            "phonetic_rhyme_analysis",
            "advanced_flow_metrics",
            "readability_indices",
            "emotional_gradient_analysis",
            "thematic_categorization",
            "literary_devices_detection",
            "vocabulary_sophistication",
            "structural_pattern_analysis",
            "creativity_assessment",
            "composite_scoring",
            "performance_caching",
        ]

    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        if self.analysis_cache:
            self.analysis_cache.clear()
            logger.info("Analysis cache cleared")


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
async def demo_advanced_analysis() -> None:
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    # TODO(google-review): [DOCSTRING] Add Returns section to docstring

    sample_lyrics = """
    Metaphors cascade like waterfalls in my mind
    Each syllable calculated, rhythmically designed  
    Philosophy meets poetry in this verbal shrine
    Where consciousness and consonance perfectly align

    The lexicon I wield cuts deeper than a blade
    Multisyllabic mastery in every word I've made
    Assonance and alliteration, my lyrical trade
    While lesser wordsmiths stumble, my foundation's never swayed

    In the labyrinth of language, I navigate with ease
    Semantic sophistication brings critics to their knees  
    Innovation flows through every line like autumn leaves
    This artistic architecture is what true genius achieves
    """

    print("üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–†–û–î–í–ò–ù–£–¢–û–ì–û –ê–õ–ì–û–†–ò–¢–ú–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê")
    print("=" * 70)

    analyzer = AdvancedAlgorithmicAnalyzer(
        {"cache_enabled": True, "detailed_logging": True}
    )

    try:
        result = analyzer.analyze_song(
            "Demo Artist", "Advanced Analysis Demo", sample_lyrics
        )

        print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
        print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.3f}")
        print(f"‚ö° –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result['processing_time']:.3f}s")

        # –†–∏—Ñ–º—ã –∏ –∑–≤—É—á–∞–Ω–∏–µ
        rhyme_analysis = result["raw_output"].get("rhyme_analysis", {})
        print("\nüéµ –†–ò–§–ú–´ –ò –ó–í–£–ß–ê–ù–ò–ï:")
        print(f"  –°—Ö–µ–º–∞ —Ä–∏—Ñ–º–æ–≤–∫–∏: {rhyme_analysis.get('rhyme_scheme', 'N/A')}")
        print(f"  –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∏—Ñ–º: {rhyme_analysis.get('rhyme_density', 0):.3f}")
        print(f"  –ê–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏—è: {rhyme_analysis.get('alliteration_score', 0):.3f}")
        print(f"  –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∏—Ñ–º—ã: {rhyme_analysis.get('internal_rhymes', 0)}")

        # Flow –∞–Ω–∞–ª–∏–∑
        flow_analysis = result["raw_output"].get("flow_analysis", {})
        print("\nüåä FLOW –ò –†–ò–¢–ú:")
        print(
            f"  –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≥–æ–≤: {flow_analysis.get('syllable_consistency', 0):.3f}"
        )
        print(
            f"  –°—Ä. —Å–ª–æ–≥–æ–≤ –Ω–∞ —Å—Ç—Ä–æ–∫—É: {flow_analysis.get('average_syllables_per_line', 0):.1f}"
        )
        print(
            f"  –†–∏—Ç–º–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {flow_analysis.get('rhythmic_density', 0):.3f}"
        )

        # –ß–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å
        readability = result["raw_output"].get("readability_metrics", {})
        print("\nüìö –ß–ò–¢–ê–ë–ï–õ–¨–ù–û–°–¢–¨:")
        print(f"  Flesch Reading Ease: {readability.get('flesch_reading_ease', 0):.1f}")
        print(f"  SMOG Index: {readability.get('smog_index', 0):.1f}")
        print(f"  –ö–æ–Ω—Å–µ–Ω—Å—É—Å: {readability.get('readability_consensus', 'N/A')}")

        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        composite = result["raw_output"].get("composite_scores", {})
        print("\nüèÜ –ö–û–ú–ü–û–ó–ò–¢–ù–´–ï –û–¶–ï–ù–ö–ò:")
        print(f"  –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ: {composite.get('technical_mastery', 0):.3f}")
        print(
            f"  –ê—Ä—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —É—Ç–æ–Ω—á—ë–Ω–Ω–æ—Å—Ç—å: {composite.get('artistic_sophistication', 0):.3f}"
        )
        print(f"  –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {composite.get('overall_quality', 0):.3f}")
        print(f"  –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å: {composite.get('innovation_score', 0):.3f}")

        print("=" * 70)
        print("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

    except Exception as e:
        # TODO(google-review): [ERROR_HANDLING] Use logger.exception instead
        # TODO(google-review): [ERROR_HANDLING] Too broad exception clause
        print(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}")
        import traceback

        traceback.print_exc()


# –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PostgreSQL
class PostgreSQLAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PostgreSQL –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
    # TODO(google-review): [ARCHITECTURE] Missing connection pooling
    # TODO(google-review): [ARCHITECTURE] Tight coupling with database

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PostgreSQL –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.analyzer = AdvancedAlgorithmicAnalyzer(
            {"cache_enabled": True, "detailed_logging": False}
        )

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è PostgreSQL (–±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ config.yaml)
        self.db_config = self._load_db_config()

    def _load_db_config(self) -> dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ë–î"""
        try:
            import yaml

            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É config_loader
            try:
                from config.config_loader import get_config
                config_obj = get_config()
                db_config = config_obj.database
                return {
                    "host": db_config.host,
                    "port": db_config.port,
                    "name": db_config.database,
                    "user": db_config.username,
                    "password": db_config.password,
                }
            except (ImportError, AttributeError):
                # Fallback –Ω–∞ YAML —Ñ–∞–π–ª
                config_path = Path(__file__).parent.parent.parent / "config.yaml"

                if config_path.exists():
                    with open(config_path, encoding="utf-8") as f:
                        config = yaml.safe_load(f)
                        return config.get("database", {})
                else:
                    print(
                        "‚ö†Ô∏è –§–∞–π–ª config.yaml –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"
                    )
                    # TODO(google-review): [SECURITY] Hardcoded credentials risk
                    # TODO(google-review): [ARCHITECTURE] Use environment variables
                    return {
                        "host": "localhost",
                        "port": 5432,
                        "name": "rap_lyrics_db",
                        "user": "postgres",
                        "password": "password",
                    }
        except Exception as e:
            # TODO(google-review): [ERROR_HANDLING] Too broad exception clause
            # TODO(google-review): [ERROR_HANDLING] Use logger instead of print
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {}

    async def get_database_stats(self) -> dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        # TODO(google-review): [DOCSTRING] Add Returns section to docstring
        # TODO(google-review): [ARCHITECTURE] Function too long (100+ lines)
        try:
            import asyncpg

            # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            conn = await asyncpg.connect(
                host=self.db_config.get("host", "localhost"),
                port=self.db_config.get("port", 5432),
                database=self.db_config.get("name", "rap_lyrics_db"),
                user=self.db_config.get("user", "postgres"),
                password=self.db_config.get("password", "password"),
            )

            try:
                # –ó–∞–ø—Ä–æ—Å –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                total_stats_query = """
                SELECT 
                    COUNT(*) as total_songs,
                    COUNT(DISTINCT artist) as unique_artists,
                    COUNT(CASE WHEN lyrics IS NOT NULL THEN 1 END) as songs_with_lyrics,
                    COUNT(CASE WHEN lyrics IS NOT NULL AND LENGTH(lyrics) > 100 THEN 1 END) as analyzable_songs,
                    COUNT(CASE WHEN lyrics IS NULL OR LENGTH(lyrics) <= 100 THEN 1 END) as non_analyzable_songs
                FROM tracks;
                """

                # –ó–∞–ø—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Å–µ–Ω —Å —Ç–µ–∫—Å—Ç–∞–º–∏)
                lyrics_stats_query = """
                SELECT 
                    AVG(LENGTH(lyrics)) as avg_lyrics_length,
                    MIN(LENGTH(lyrics)) as min_lyrics_length,
                    MAX(LENGTH(lyrics)) as max_lyrics_length,
                    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY LENGTH(lyrics)) as median_lyrics_length
                FROM tracks 
                WHERE lyrics IS NOT NULL;
                """

                total_result = await conn.fetchrow(total_stats_query)
                lyrics_result = await conn.fetchrow(lyrics_stats_query)

                stats = {
                    "total_songs": total_result["total_songs"],
                    "unique_artists": total_result["unique_artists"],
                    "songs_with_lyrics": total_result["songs_with_lyrics"],
                    "analyzable_songs": total_result["analyzable_songs"],
                    "non_analyzable_songs": total_result["non_analyzable_songs"],
                    "avg_lyrics_length": float(lyrics_result["avg_lyrics_length"])
                    if lyrics_result["avg_lyrics_length"]
                    else 0,
                    "min_lyrics_length": lyrics_result["min_lyrics_length"],
                    "max_lyrics_length": lyrics_result["max_lyrics_length"],
                    "median_lyrics_length": float(lyrics_result["median_lyrics_length"])
                    if lyrics_result["median_lyrics_length"]
                    else 0,
                }

                print("üìä –ü–û–õ–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•:")
                print("=" * 50)
                print(f"  üìÄ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –ë–î: {stats['total_songs']:,}")
                print(f"  üé§ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π: {stats['unique_artists']:,}")
                print(f"  üìù –ü–µ—Å–µ–Ω —Å —Ç–µ–∫—Å—Ç–∞–º–∏: {stats['songs_with_lyrics']:,}")
                print(
                    f"  ‚úÖ –ü–µ—Å–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (>100 —Å–∏–º–≤–æ–ª–æ–≤): {stats['analyzable_songs']:,}"
                )
                print(
                    f"  ‚ùå –ù–µ–ø—Ä–∏–≥–æ–¥–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {stats['non_analyzable_songs']:,}"
                )
                print("")
                print("üìè –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –î–õ–ò–ù–ï –¢–ï–ö–°–¢–û–í:")
                print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {stats['avg_lyrics_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
                print(
                    f"  –ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {stats['median_lyrics_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤"
                )
                print(
                    f"  –î–∏–∞–ø–∞–∑–æ–Ω: {stats['min_lyrics_length']:,} - {stats['max_lyrics_length']:,} —Å–∏–º–≤–æ–ª–æ–≤"
                )

                # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
                if stats["total_songs"] > 0:
                    lyrics_percent = (
                        stats["songs_with_lyrics"] / stats["total_songs"]
                    ) * 100
                    analyzable_percent = (
                        stats["analyzable_songs"] / stats["total_songs"]
                    ) * 100
                    print("")
                    print("üìä –ü–†–û–¶–ï–ù–¢–ù–´–ï –°–û–û–¢–ù–û–®–ï–ù–ò–Ø:")
                    print(f"  –ü–µ—Å–µ–Ω —Å —Ç–µ–∫—Å—Ç–∞–º–∏: {lyrics_percent:.1f}%")
                    print(f"  –ü—Ä–∏–≥–æ–¥–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {analyzable_percent:.1f}%")

                return stats

            finally:
                await conn.close()

        except Exception as e:
            # TODO(google-review): [ERROR_HANDLING] Too broad exception clause
            # TODO(google-review): [ERROR_HANDLING] Use logger.exception
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ë–î: {e}")
            return {}

    async def analyze_all_songs(
        self, limit: int | None = None, batch_size: int = 100
    ) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –ø–µ—Å–µ–Ω –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [ARCHITECTURE] Function too long (90+ lines)
        # TODO(google-review): [ARCHITECTURE] Magic number 100
        try:
            import asyncpg

            conn = await asyncpg.connect(
                host=self.db_config.get("host", "localhost"),
                port=self.db_config.get("port", 5432),
                database=self.db_config.get("name", "rap_lyrics_db"),
                user=self.db_config.get("user", "postgres"),
                password=self.db_config.get("password", "password"),
            )

            try:
                # –ó–∞–ø—Ä–æ—Å –ø–µ—Å–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                # TODO(google-review): [SECURITY] SQL injection risk with f-string
                # TODO(google-review): [ARCHITECTURE] Use parameterized queries
                limit_clause = f"LIMIT {limit}" if limit else ""
                query = f"""
                SELECT id, artist, title, lyrics 
                FROM tracks 
                WHERE lyrics IS NOT NULL AND LENGTH(lyrics) > 100
                ORDER BY id
                {limit_clause};
                """

                tracks = await conn.fetch(query)
                total_songs = len(tracks)

                print(
                    f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ {total_songs:,} –ø–µ—Å–µ–Ω (–±–∞—Ç—á–∏ –ø–æ {batch_size})"
                )

                processed = 0
                results = []

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏
                for i in range(0, total_songs, batch_size):
                    batch = tracks[i : i + batch_size]
                    batch_results = []

                    for song in batch:
                        try:
                            # –ê–Ω–∞–ª–∏–∑ –ø–µ—Å–Ω–∏
                            result = self.analyzer.analyze_song(
                                artist=song["artist"],
                                title=song["title"],
                                lyrics=song["lyrics"],
                            )

                            batch_results.append(
                                {
                                    "song_id": song["id"],
                                    "artist": song["artist"],
                                    "title": song["title"],
                                    "analysis": result["raw_output"],
                                    "confidence": result["confidence"],
                                    "processing_time": result["processing_time"],
                                }
                            )

                            processed += 1

                        except Exception as e:
                            # TODO(google-review): [ERROR_HANDLING] Too broad exception
                            # TODO(google-review): [ERROR_HANDLING] Use logger
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Å–Ω–∏ {song['id']}: {e}")

                    results.extend(batch_results)

                    # –ü—Ä–æ–≥—Ä–µ—Å—Å
                    progress = (processed / total_songs) * 100
                    print(
                        f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed:,}/{total_songs:,} ({progress:.1f}%)"
                    )

                print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed:,} –ø–µ—Å–µ–Ω")

                return {
                    "total_processed": processed,
                    "results": results,
                    "summary_stats": self._calculate_summary_stats(results),
                }

            finally:
                await conn.close()

        except Exception as e:
            # TODO(google-review): [ERROR_HANDLING] Too broad exception clause
            # TODO(google-review): [ERROR_HANDLING] Use logger.exception
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Å–µ–Ω: {e}")
            return {}

    async def analyze_single_track(self, track_id: int) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–µ—Å–Ω–∏ –ø–æ ID"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        # TODO(google-review): [ARCHITECTURE] Function too long (50+ lines)
        try:
            import asyncpg

            conn = await asyncpg.connect(
                host=self.db_config.get("host", "localhost"),
                port=self.db_config.get("port", 5432),
                database=self.db_config.get("name", "rap_lyrics_db"),
                user=self.db_config.get("user", "postgres"),
                password=self.db_config.get("password", "password"),
            )

            try:
                # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Å–Ω—é
                query = "SELECT id, artist, title, lyrics FROM tracks WHERE id = $1"
                song = await conn.fetchrow(query, track_id)

                if not song:
                    print(f"‚ùå –ü–µ—Å–Ω—è —Å ID {track_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                    return {}

                if not song["lyrics"] or len(song["lyrics"]) < 100:
                    print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (ID: {track_id})")
                    return {}

                print(f"üéµ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {song['artist']} - {song['title']}")

                # –ê–Ω–∞–ª–∏–∑
                result = self.analyzer.analyze_song(
                    artist=song["artist"], title=song["title"], lyrics=song["lyrics"]
                )

                # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                self._print_analysis_results(result)

                return {
                    "song_id": song["id"],
                    "artist": song["artist"],
                    "title": song["title"],
                    "analysis": result["raw_output"],
                    "confidence": result["confidence"],
                    "processing_time": result["processing_time"],
                }

            finally:
                await conn.close()

        except Exception as e:
            # TODO(google-review): [ERROR_HANDLING] Too broad exception clause
            # TODO(google-review): [ERROR_HANDLING] Use logger.exception
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–∫–∞ {track_id}: {e}")
            return {}

    def _calculate_summary_stats(self, results: list[dict]) -> dict[str, Any]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        # TODO(google-review): [DOCSTRING] Add Args, Returns sections
        if not results:
            return {}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        confidences = [r["confidence"] for r in results]
        processing_times = [r["processing_time"] for r in results]

        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        technical_scores = []
        artistic_scores = []

        for result in results:
            composite = result.get("analysis", {}).get("composite_scores", {})
            if composite:
                technical_scores.append(composite.get("technical_mastery", 0))
                artistic_scores.append(composite.get("artistic_sophistication", 0))

        return {
            "avg_confidence": sum(confidences) / len(confidences),
            "avg_processing_time": sum(processing_times) / len(processing_times),
            "avg_technical_mastery": sum(technical_scores) / len(technical_scores)
            if technical_scores
            else 0,
            "avg_artistic_sophistication": sum(artistic_scores) / len(artistic_scores)
            if artistic_scores
            else 0,
            "total_results": len(results),
        }

    def _print_analysis_results(self, result: dict[str, Any]) -> None:
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        # TODO(google-review): [TYPING] Add return type hint
        # TODO(google-review): [DOCSTRING] Add Args section to docstring
        # TODO(google-review): [ARCHITECTURE] Function too long (40+ lines)
        print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
        print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.3f}")
        print(f"‚ö° –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result['processing_time']:.3f}s")

        # –†–∏—Ñ–º—ã –∏ –∑–≤—É—á–∞–Ω–∏–µ
        rhyme_analysis = result["raw_output"].get("rhyme_analysis", {})
        if rhyme_analysis:
            print("\nüéµ –†–ò–§–ú–´ –ò –ó–í–£–ß–ê–ù–ò–ï:")
            print(f"  –°—Ö–µ–º–∞ —Ä–∏—Ñ–º–æ–≤–∫–∏: {rhyme_analysis.get('rhyme_scheme', 'N/A')}")
            print(f"  –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∏—Ñ–º: {rhyme_analysis.get('rhyme_density', 0):.3f}")
            print(f"  –ê–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏—è: {rhyme_analysis.get('alliteration_score', 0):.3f}")
            print(f"  –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∏—Ñ–º—ã: {rhyme_analysis.get('internal_rhymes', 0)}")

        # Flow –∞–Ω–∞–ª–∏–∑
        flow_analysis = result["raw_output"].get("flow_analysis", {})
        if flow_analysis:
            print("\nüåä FLOW –ò –†–ò–¢–ú:")
            print(
                f"  –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≥–æ–≤: {flow_analysis.get('syllable_consistency', 0):.3f}"
            )
            print(
                f"  –°—Ä. —Å–ª–æ–≥–æ–≤ –Ω–∞ —Å—Ç—Ä–æ–∫—É: {flow_analysis.get('average_syllables_per_line', 0):.1f}"
            )
            print(
                f"  –†–∏—Ç–º–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {flow_analysis.get('rhythmic_density', 0):.3f}"
            )

        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        composite = result["raw_output"].get("composite_scores", {})
        if composite:
            print("\nüèÜ –ö–û–ú–ü–û–ó–ò–¢–ù–´–ï –û–¶–ï–ù–ö–ò:")
            print(
                f"  –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ: {composite.get('technical_mastery', 0):.3f}"
            )
            print(
                f"  –ê—Ä—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —É—Ç–æ–Ω—á—ë–Ω–Ω–æ—Å—Ç—å: {composite.get('artistic_sophistication', 0):.3f}"
            )
            print(f"  –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {composite.get('overall_quality', 0):.3f}")
            print(f"  –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å: {composite.get('innovation_score', 0):.3f}")


async def main() -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PostgreSQL"""
    # TODO(google-review): [DOCSTRING] Add Returns section to docstring
    # TODO(google-review): [ARCHITECTURE] Function too long (120+ lines)
    import argparse

    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(
        description="–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è PostgreSQL"
    )

    parser.add_argument(
        "--stats", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"
    )
    parser.add_argument(
        "--analyze-all", action="store_true", help="–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Ç—Ä–µ–∫–∏ –≤ –±–∞–∑–µ"
    )
    parser.add_argument(
        "--analyze-track",
        type=int,
        metavar="ID",
        help="–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç—Ä–µ–∫ –ø–æ ID",
    )
    parser.add_argument(
        "--limit",
        type=int,
        metavar="N",
        help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        metavar="N",
        help="–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 100)",
    )
    parser.add_argument(
        "--demo", action="store_true", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"
    )

    args = parser.parse_args()

    # –ï—Å–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –º–µ–Ω—é
    action_args = [args.stats, args.analyze_all, args.analyze_track, args.demo]
    if not any(action_args):
        print()
        print("üßÆ –ü–†–û–î–í–ò–ù–£–¢–´–ô –ê–õ–ì–û–†–ò–¢–ú–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –î–õ–Ø POSTGRESQL")
        print("=" * 65)
        print("üéØ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ AI –º–æ–¥–µ–ª–µ–π")
        print("‚ö° –†–∞–±–æ—Ç–∞ —Å 57K+ —Ç—Ä–µ–∫–æ–≤ –≤ PostgreSQL –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        print("üìä –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: —Ä–∏—Ñ–º—ã, flow, —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å, —ç–º–æ—Ü–∏–∏")
        print()
        print("üñ•Ô∏è CLI –ò–ù–¢–ï–†–§–ï–ô–°:")
        print("  --stats              üìä –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ë–î")
        print("  --analyze-all        üöÄ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Ç—Ä–µ–∫–∏")
        print("  --analyze-track ID   üéµ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç—Ä–µ–∫")
        print("  --limit N            üî¢ –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–∫–æ–≤")
        print("  --batch-size N       üì¶ –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        print("  --demo               üé≠ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞")
        print("  --help               ‚ùì –ü–æ–ª–Ω–∞—è —Å–ø—Ä–∞–≤–∫–∞")
        print()
        print("üí° –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø:")
        print("  python src/analyzers/algorithmic_analyzer.py --stats")
        print(
            "  python src/analyzers/algorithmic_analyzer.py --analyze-all --limit 100"
        )
        print("  python src/analyzers/algorithmic_analyzer.py --analyze-track 123")
        print("  python src/analyzers/algorithmic_analyzer.py --demo")
        print()
        print("üìà –ú–ï–¢–†–ò–ö–ò –ê–ù–ê–õ–ò–ó–ê:")
        print("  üéµ –†–∏—Ñ–º—ã: —Å—Ö–µ–º–∞, –ø–ª–æ—Ç–Ω–æ—Å—Ç—å, —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")
        print("  üåä Flow: –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≥–æ–≤, —Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å")
        print("  üìö –ß–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å: Flesch, SMOG, ARI –∏–Ω–¥–µ–∫—Å—ã")
        print("  üí≠ –≠–º–æ—Ü–∏–∏: –≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å, –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å, —Å–ª–æ–∂–Ω–æ—Å—Ç—å")
        print("  üé® –¢–µ–º—ã: –¥–µ–Ω—å–≥–∏, —É–ª–∏—Ü–∞, —É—Å–ø–µ—Ö, –æ—Ç–Ω–æ—à–µ–Ω–∏—è")
        print("=" * 65)
        return

    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –æ–Ω –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω—É–∂–µ–Ω
    try:
        if args.demo:
            print("üöÄ –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...")
            await demo_advanced_analysis()

        else:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ë–î
            print("‚úÖ PostgreSQL –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            analyzer = PostgreSQLAnalyzer()

            if args.stats:
                print("üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
                await analyzer.get_database_stats()

            elif args.analyze_track:
                print(f"üéµ –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–∫–∞ ID: {args.analyze_track}")
                await analyzer.analyze_single_track(args.analyze_track)

            elif args.analyze_all:
                print("üöÄ –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ç—Ä–µ–∫–æ–≤...")
                results = await analyzer.analyze_all_songs(
                    limit=args.limit, batch_size=args.batch_size
                )

                if results:
                    summary = results.get("summary_stats", {})
                    print("\nüìà –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç—Ä–µ–∫–æ–≤: {summary.get('total_results', 0):,}")
                    print(
                        f"  –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {summary.get('avg_confidence', 0):.3f}"
                    )
                    print(
                        f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {summary.get('avg_processing_time', 0):.3f}s"
                    )
                    print(
                        f"  –°—Ä–µ–¥–Ω–µ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ: {summary.get('avg_technical_mastery', 0):.3f}"
                    )
                    print(
                        f"  –°—Ä–µ–¥–Ω—è—è –∞—Ä—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —É—Ç–æ–Ω—á—ë–Ω–Ω–æ—Å—Ç—å: {summary.get('avg_artistic_sophistication', 0):.3f}"
                    )

    except Exception as e:
        # TODO(google-review): [ERROR_HANDLING] Too broad exception clause
        # TODO(google-review): [ERROR_HANDLING] Use logger.exception instead
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
