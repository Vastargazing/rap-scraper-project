"""
üß† –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –¥–ª—è –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞ Rap Scraper (PostgreSQL Edition)

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- –ï–¥–∏–Ω—ã–π API –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ (Qwen, Emotional, Algorithmic, Ollama, Simplified, Multimodal)
- –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è PostgreSQL
- –§–∞–±—Ä–∏–∫–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å PostgreSQL —á–µ—Ä–µ–∑ postgres_adapter.py
- Batch processing –∏ concurrent access support

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
from src.interfaces.analyzer_interface import BaseAnalyzer, AnalysisResult, AnalyzerFactory

# –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
analyzer = AnalyzerFactory.create('qwen')
result = analyzer.analyze_song("Kendrick Lamar", "HUMBLE.", lyrics)

# –ê–Ω–∞–ª–∏–∑ –≤—Å–µ–º–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏
all_results = await AnalyzerFactory.analyze_with_all(artist, title, lyrics)

# –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤
await AnalyzerFactory.mass_analyze('emotional', batch_size=50)

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- Python 3.8+
- asyncpg, psycopg2-binary (PostgreSQL)
- src.database.postgres_adapter (PostgreSQL manager)
- src.utils.config (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)

–†–ï–ó–£–õ–¨–¢–ê–¢:
- –ï–¥–∏–Ω—ã–π API –¥–ª—è –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
- PostgreSQL –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å batch operations
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- Concurrent processing support
- Comprehensive error handling –∏ logging

–ê–í–¢–û–†: AI Assistant  
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""

import asyncio
import time
import logging
import json
import os
import sys
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union, Tuple, AsyncGenerator
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import traceback

# PostgreSQL –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
try:
    from src.database.postgres_adapter import PostgreSQLManager
    from src.utils.config import get_db_config
    POSTGRES_AVAILABLE = True
except ImportError as e:
    logging.warning(f"‚ö†Ô∏è PostgreSQL adapter –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
    POSTGRES_AVAILABLE = False

# –ï—Å–ª–∏ —Ñ–∞–π–ª –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é (python src/interfaces/analyzer_interface.py),
# —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–æ—Ä–µ–Ω—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –≤ sys.path —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞–∫–µ—Ç `src`.
try:
    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if repo_root not in sys.path:
        sys.path.insert(0, repo_root)
except Exception:
    pass

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AnalyzerType(Enum):
    """–¢–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ"""
    EMOTIONAL = "emotional"
    QWEN = "qwen" 
    ALGORITHMIC = "algorithmic"
    OLLAMA = "ollama"
    SIMPLIFIED = "simplified"
    MULTIMODAL = "multimodal"
    GEMMA = "gemma"  # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Gemma –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏


class AnalysisStatus(Enum):
    """–°—Ç–∞—Ç—É—Å—ã –∞–Ω–∞–ª–∏–∑–∞"""
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"
    PARTIAL = "partial"


@dataclass
class AnalysisResult:
    """
    –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è PostgreSQL
    
    –°–æ–≤–º–µ—Å—Ç–∏–º —Å —Ç–∞–±–ª–∏—Ü–µ–π analysis_results:
    - track_id (FK to tracks)
    - analyzer_type (qwen, emotional, etc.)
    - analysis_data (JSONB)
    - confidence_score
    - created_at
    """
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
    artist: str
    title: str
    analyzer_type: str
    analysis_data: Dict[str, Any]
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    confidence: float = field(default=0.0)
    processing_time: float = field(default=0.0)
    
    # PostgreSQL –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
    track_id: Optional[int] = field(default=None)
    status: AnalysisStatus = field(default=AnalysisStatus.SUCCESS)
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    # –û—à–∏–±–∫–∏ –∏ –æ—Ç–ª–∞–¥–∫–∞
    error_message: Optional[str] = field(default=None)
    raw_output: Optional[Dict[str, Any]] = field(default=None)
    
    def to_postgres_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è PostgreSQL"""
        return {
            'track_id': self.track_id,
            'analyzer_type': self.analyzer_type,
            'analysis_data': self.analysis_data,
            'confidence_score': self.confidence,
            'metadata': {
                **self.metadata,
                'processing_time': self.processing_time,
                'status': self.status.value,
                'timestamp': self.timestamp,
                'error_message': self.error_message
            }
        }


class BaseAnalyzer(ABC):
    """
    –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
    
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
    - –ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
    - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å PostgreSQL
    - Batch processing support
    - Error handling –∏ logging
    - Automatic result saving
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.config = config or {}
        self.name = self.__class__.__name__
        self.available = True
        self.model_name = None
        self.api_url = None
        
        # PostgreSQL –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
        self.db_manager = None
        if POSTGRES_AVAILABLE:
            try:
                self.db_manager = PostgreSQLManager()
                logger.info(f"‚úÖ {self.name}: PostgreSQL adapter –ø–æ–¥–∫–ª—é—á–µ–Ω")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è {self.name}: PostgreSQL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.available = False
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_analyzed': 0,
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'total_time': 0.0,
            'avg_confidence': 0.0
        }

    @abstractmethod
    async def analyze_song(self, artist: str, title: str, lyrics: str, 
                          track_id: Optional[int] = None) -> AnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π –ø–µ—Å–Ω–∏
        
        Args:
            artist: –ò–º—è –∞—Ä—Ç–∏—Å—Ç–∞
            title: –ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Å–Ω–∏
            lyrics: –¢–µ–∫—Å—Ç –ø–µ—Å–Ω–∏
            track_id: ID —Ç—Ä–µ–∫–∞ –≤ PostgreSQL (–µ—Å–ª–∏ –µ—Å—Ç—å)
            
        Returns:
            AnalysisResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        pass

    @abstractmethod
    def get_analyzer_info(self) -> Dict[str, Any]:
        """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–± –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ"""
        pass

    @property
    @abstractmethod
    def analyzer_type(self) -> str:
        """–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ (–¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å AnalyzerType)"""
        pass

    @property
    @abstractmethod
    def supported_features(self) -> List[str]:
        """–°–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∏—á –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        pass

    # PostgreSQL –º–µ—Ç–æ–¥—ã
    
    async def save_to_database(self, result: AnalysisResult) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ PostgreSQL"""
        if not self.db_manager:
            logger.warning(f"‚ùå {self.name}: PostgreSQL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return False
        
        try:
            await self.db_manager.initialize()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞
            existing = await self.db_manager.get_analysis(
                result.track_id, result.analyzer_type
            )
            
            if existing:
                logger.debug(f"üìã {self.name}: –ê–Ω–∞–ª–∏–∑ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–ª—è track_id={result.track_id}")
                return True
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
            postgres_data = result.to_postgres_dict()
            success = await self.db_manager.save_analysis(postgres_data)
            
            if success:
                logger.debug(f"üíæ {self.name}: –ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è track_id={result.track_id}")
                self.stats['successful'] += 1
            else:
                logger.error(f"‚ùå {self.name}: –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è track_id={result.track_id}")
                self.stats['failed'] += 1
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå {self.name}: –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ PostgreSQL: {e}")
            self.stats['failed'] += 1
            return False

    async def get_unanalyzed_tracks(self, limit: int = 100) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        if not self.db_manager:
            logger.warning(f"‚ùå {self.name}: PostgreSQL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return []
        
        try:
            await self.db_manager.initialize()
            
            tracks = await self.db_manager.get_unanalyzed_tracks(
                analyzer_type=self.analyzer_type,
                limit=limit
            )
            
            logger.info(f"üìä {self.name}: –ù–∞–π–¥–µ–Ω–æ {len(tracks)} –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤")
            return tracks
            
        except Exception as e:
            logger.error(f"‚ùå {self.name}: –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–∫–æ–≤: {e}")
            return []

    async def mass_analyze(self, batch_size: int = 50, max_tracks: Optional[int] = None) -> Dict[str, int]:
        """
        –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            max_tracks: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–∫–æ–≤ (None = –≤—Å–µ)
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {'processed': N, 'successful': N, 'failed': N}
        """
        logger.info(f"üöÄ {self.name}: –ó–∞–ø—É—Å–∫ –º–∞—Å—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (batch_size={batch_size})")
        
        stats = {'processed': 0, 'successful': 0, 'failed': 0, 'skipped': 0}
        
        while True:
            # –ü–æ–ª—É—á–∞–µ–º –±–∞—Ç—á –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤
            tracks = await self.get_unanalyzed_tracks(limit=batch_size)
            
            if not tracks:
                logger.info(f"‚úÖ {self.name}: –í—Å–µ —Ç—Ä–µ–∫–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
                break
            
            if max_tracks and stats['processed'] >= max_tracks:
                logger.info(f"‚úÖ {self.name}: –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ç—Ä–µ–∫–æ–≤: {max_tracks}")
                break
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
            batch_start = time.time()
            
            for track in tracks:
                try:
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–∫
                    result = await self.analyze_song(
                        artist=track['artist'],
                        title=track['title'], 
                        lyrics=track['lyrics'],
                        track_id=track['id']
                    )
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    if await self.save_to_database(result):
                        stats['successful'] += 1
                    else:
                        stats['failed'] += 1
                    
                except Exception as e:
                    logger.error(f"‚ùå {self.name}: –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–∫–∞ {track['id']}: {e}")
                    stats['failed'] += 1
                
                stats['processed'] += 1
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞
                if max_tracks and stats['processed'] >= max_tracks:
                    break
            
            batch_time = time.time() - batch_start
            logger.info(f"üìä {self.name}: –ë–∞—Ç—á –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {batch_time:.1f}—Å "
                       f"(—É—Å–ø–µ—à–Ω–æ: {stats['successful']}, –æ—à–∏–±–æ–∫: {stats['failed']})")
        
        logger.info(f"üèÅ {self.name}: –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")
        return stats

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    
    def validate_input(self, artist: str, title: str, lyrics: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not all([artist, title, lyrics]):
            return False

        if len(lyrics.strip()) < 10:
            return False

        return True

    def preprocess_lyrics(self, lyrics: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Å–Ω–∏"""
        lyrics = lyrics.strip()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        import re
        lyrics = re.sub(r'\s+', ' ', lyrics)
        
        return lyrics

    def update_stats(self, result: AnalysisResult):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.stats['total_analyzed'] += 1
        self.stats['total_time'] += result.processing_time
        
        if result.status == AnalysisStatus.SUCCESS:
            self.stats['successful'] += 1
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            total_successful = self.stats['successful']
            current_avg = self.stats['avg_confidence']
            self.stats['avg_confidence'] = ((current_avg * (total_successful - 1)) + result.confidence) / total_successful
        elif result.status == AnalysisStatus.FAILED:
            self.stats['failed'] += 1
        else:
            self.stats['skipped'] += 1

    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        avg_time = self.stats['total_time'] / max(self.stats['total_analyzed'], 1)
        
        return {
            **self.stats,
            'avg_processing_time': avg_time,
            'success_rate': self.stats['successful'] / max(self.stats['total_analyzed'], 1),
            'analyzer_name': self.name,
            'analyzer_type': self.analyzer_type
        }


class AnalyzerFactory:
    """
    –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏
    
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è, —Å–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤,
    –º–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ–º–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏
    """

    _analyzers: Dict[str, type] = {}
    _instances: Dict[str, BaseAnalyzer] = {}

    @classmethod
    def register(cls, name: str, analyzer_class: type) -> None:
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        if not issubclass(analyzer_class, BaseAnalyzer):
            raise ValueError(f"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å—Å—è –æ—Ç BaseAnalyzer")

        cls._analyzers[name] = analyzer_class
        logger.info(f"üìù –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {name}")

    @classmethod
    def create(cls, name: str, config: Optional[Dict[str, Any]] = None, 
               singleton: bool = True) -> BaseAnalyzer:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        if name not in cls._analyzers:
            available = list(cls._analyzers.keys())
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {name}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {available}")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º singleton –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if singleton and name in cls._instances:
            return cls._instances[name]

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
        analyzer_class = cls._analyzers[name]
        instance = analyzer_class(config)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ singleton
        if singleton:
            cls._instances[name] = instance

        logger.info(f"üè≠ –°–æ–∑–¥–∞–Ω –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {name}")
        return instance

    @classmethod
    async def analyze_with_all(cls, artist: str, title: str, lyrics: str, 
                              track_id: Optional[int] = None,
                              save_to_db: bool = True) -> Dict[str, AnalysisResult]:
        """
        –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π –ø–µ—Å–Ω–∏ –≤—Å–µ–º–∏ –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏
        
        Args:
            artist: –ò–º—è –∞—Ä—Ç–∏—Å—Ç–∞
            title: –ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Å–Ω–∏
            lyrics: –¢–µ–∫—Å—Ç –ø–µ—Å–Ω–∏
            track_id: ID —Ç—Ä–µ–∫–∞ –≤ PostgreSQL
            save_to_db: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –±–∞–∑—É
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
        """
        logger.info(f"üéµ –ê–Ω–∞–ª–∏–∑ –≤—Å–µ–º–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏: {artist} - {title}")
        
        results = {}
        start_time = time.time()
        
        for name in cls._analyzers.keys():
            try:
                analyzer = cls.create(name)
                
                if not analyzer.available:
                    logger.warning(f"‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä {name} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                    continue
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
                result = await analyzer.analyze_song(artist, title, lyrics, track_id)
                results[name] = result
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
                if save_to_db and track_id:
                    await analyzer.save_to_database(result)
                
                logger.info(f"‚úÖ {name}: confidence={result.confidence:.2f}, "
                           f"time={result.processing_time:.2f}s")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ {name}: {e}")
                logger.debug(traceback.format_exc())
        
        total_time = time.time() - start_time
        logger.info(f"üèÅ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_time:.2f}—Å. "
                   f"–£—Å–ø–µ—à–Ω—ã—Ö: {len(results)}/{len(cls._analyzers)}")
        
        return results

    @classmethod
    async def mass_analyze_all(cls, batch_size: int = 25, max_tracks: Optional[int] = None) -> Dict[str, Dict[str, int]]:
        """
        –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ–º–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            max_tracks: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—É
        """
        logger.info(f"üöÄ –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ–º–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏ (batch_size={batch_size})")
        
        all_stats = {}
        
        for name in cls._analyzers.keys():
            try:
                analyzer = cls.create(name)
                
                if not analyzer.available:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {name}")
                    continue
                
                logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ –º–∞—Å—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {name}")
                stats = await analyzer.mass_analyze(batch_size, max_tracks)
                all_stats[name] = stats
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–∞—Å—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ {name}: {e}")
                all_stats[name] = {'error': str(e)}
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_processed = sum(s.get('processed', 0) for s in all_stats.values())
        total_successful = sum(s.get('successful', 0) for s in all_stats.values())
        
        logger.info(f"üèÅ –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_processed}, "
                   f"—É—Å–ø–µ—à–Ω–æ: {total_successful}")
        
        return all_stats

    @classmethod
    def list_available(cls) -> List[str]:
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        return list(cls._analyzers.keys())

    @classmethod
    def get_analyzer_info(cls, name: str) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ"""
        if name not in cls._analyzers:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {name}")

        analyzer = cls.create(name)
        return analyzer.get_analyzer_info()

    @classmethod
    def get_all_stats(cls) -> Dict[str, Dict[str, Any]]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        stats = {}
        
        for name, instance in cls._instances.items():
            stats[name] = instance.get_stats()
        
        return stats


# –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
def register_analyzer(name: str):
    """
    –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        @register_analyzer("qwen")
        class QwenAnalyzer(BaseAnalyzer):
            ...
    """
    def decorator(analyzer_class):
        AnalyzerFactory.register(name, analyzer_class)
        return analyzer_class
    return decorator


@register_analyzer("qwen")
class QwenAnalyzerWrapper(BaseAnalyzer):
    """
    Wrapper around the existing legacy QwenAnalyzer implementation.

    This wrapper imports the legacy analyzer at runtime to avoid circular
    imports and adapts its synchronous API to the async `BaseAnalyzer`
    contract. It also ensures the returned `AnalysisResult` matches the
    new schema used by this interface.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self._legacy_config = config or {}
        self.name = "QwenAnalyzerWrapper"

    @property
    def analyzer_type(self) -> str:
        return AnalyzerType.QWEN.value

    def get_analyzer_info(self) -> Dict[str, Any]:
        # Try to import legacy analyzer and reuse its info if available
        try:
            from archive.qwen_analyzer import QwenAnalyzer as LegacyQwen
            legacy = LegacyQwen(self._legacy_config)
            info = legacy.get_analyzer_info()
            info['type'] = self.analyzer_type
            return info
        except Exception:
            return {
                'name': 'QwenAnalyzerWrapper',
                'version': 'wrapper-1.0',
                'description': 'Wrapper for legacy Qwen analyzer',
                'type': self.analyzer_type,
                'available': self.available,
                'supported_features': []
            }

    @property
    def supported_features(self) -> List[str]:
        try:
            from archive.qwen_analyzer import QwenAnalyzer as LegacyQwen
            legacy = LegacyQwen(self._legacy_config)
            return legacy.supported_features
        except Exception:
            return []

    async def analyze_song(self, artist: str, title: str, lyrics: str,
                           track_id: Optional[int] = None) -> AnalysisResult:
        # Validate input using base helper
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")

        # Run the legacy, synchronous analysis in a thread to avoid blocking
        def sync_analyze():
            from archive.qwen_analyzer import QwenAnalyzer as LegacyQwen

            legacy = LegacyQwen(self._legacy_config)

            if not legacy.available:
                raise RuntimeError("Legacy Qwen analyzer not available")

            start = time.time()

            # reuse legacy helpers to build prompts and call model
            system_prompt, user_prompt = legacy._create_analysis_prompts(artist, title, legacy.preprocess_lyrics(lyrics))

            response = legacy.client.chat.completions.create(
                model=legacy.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=legacy.max_tokens,
                temperature=legacy.temperature
            )

            raw = legacy._parse_response(response.choices[0].message.content)
            confidence = legacy._calculate_confidence(raw)
            processing_time = time.time() - start

            # Adapt to the new AnalysisResult schema
            result = AnalysisResult(
                artist=artist,
                title=title,
                analyzer_type=self.analyzer_type,
                analysis_data=raw,
                confidence=confidence,
                processing_time=processing_time,
                track_id=track_id,
                status=AnalysisStatus.SUCCESS,
                metadata={
                    'model_name': legacy.model_name,
                    'provider': 'Novita AI',
                    'usage': getattr(response, 'usage', {}),
                },
                raw_output=raw,
                timestamp=datetime.now().isoformat()
            )

            return result

        # Execute synchronous legacy logic in background
        return await asyncio.to_thread(sync_analyze)


@register_analyzer("advanced_algorithmic")
class AdvancedAlgorithmicAnalyzerWrapper(BaseAnalyzer):
    """
    Wrapper for the legacy AdvancedAlgorithmicAnalyzer (sync implementation).
    Runs legacy analysis in a background thread and adapts the result to AnalysisResult.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self._legacy_config = config or {}
        self.name = "AdvancedAlgorithmicAnalyzerWrapper"

    @property
    def analyzer_type(self) -> str:
        return AnalyzerType.ALGORITHMIC.value

    def get_analyzer_info(self) -> Dict[str, Any]:
        try:
            from src.analyzers.algorithmic_analyzer import AdvancedAlgorithmicAnalyzer as Legacy
            legacy = Legacy(self._legacy_config)
            info = legacy.get_analyzer_info() if hasattr(legacy, 'get_analyzer_info') else {}
            info['type'] = self.analyzer_type
            return info
        except Exception:
            return {
                'name': 'AdvancedAlgorithmicAnalyzerWrapper',
                'version': 'wrapper-1.0',
                'description': 'Wrapper for legacy algorithmic analyzer',
                'type': self.analyzer_type,
                'available': self.available,
                'supported_features': []
            }

    @property
    def supported_features(self) -> List[str]:
        try:
            from src.analyzers.algorithmic_analyzer import AdvancedAlgorithmicAnalyzer as Legacy
            legacy = Legacy(self._legacy_config)
            return getattr(legacy, 'supported_features', [])
        except Exception:
            return []

    async def analyze_song(self, artist: str, title: str, lyrics: str,
                           track_id: Optional[int] = None) -> AnalysisResult:
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")

        def sync_analyze():
            from src.analyzers.algorithmic_analyzer import AdvancedAlgorithmicAnalyzer as Legacy

            legacy = Legacy(self._legacy_config)

            if not getattr(legacy, 'available', True):
                raise RuntimeError("Legacy algorithmic analyzer not available")

            start = time.time()
            raw = legacy.analyze_song(artist, title, lyrics)
            processing_time = time.time() - start
            # Helper to coerce various legacy result types into a dict
            def to_plain_dict(obj):
                if obj is None:
                    return {}
                if isinstance(obj, dict):
                    # ensure keys are str
                    try:
                        return {str(k): v for k, v in obj.items()}
                    except Exception:
                        return obj
                # pydantic
                if hasattr(obj, 'model_dump'):
                    try:
                        return obj.model_dump()
                    except Exception:
                        pass
                if hasattr(obj, 'dict'):
                    try:
                        return obj.dict()
                    except Exception:
                        pass
                # dataclass or simple object
                if hasattr(obj, '__dict__'):
                    try:
                        return {k: v for k, v in vars(obj).items() if not k.startswith('_')}
                    except Exception:
                        pass
                # last resort: string-serialize then parse JSON if possible
                try:
                    import json as _json
                    return _json.loads(_json.dumps(obj, default=lambda o: getattr(o, '__dict__', str(o))))
                except Exception:
                    return {}

            # Extract fields from legacy result
            analyzer_type = getattr(raw, 'analyzer_type', None) or getattr(raw, 'analysis_type', None) or self.analyzer_type
            confidence = getattr(raw, 'confidence', getattr(raw, 'confidence_score', 0.0))
            # analysis payload could be under several attributes
            analysis_payload = getattr(raw, 'analysis_data', None) or getattr(raw, 'raw_output', None) or getattr(raw, 'raw', None) or raw
            analysis_data = to_plain_dict(analysis_payload)

            # Defensive cleanup: if legacy payload still includes wrapper-level keys like 'analysis_type', remove them
            if isinstance(analysis_data, dict):
                if 'analysis_type' in analysis_data:
                    # remove legacy top-level marker from payload to avoid duplication
                    analysis_data.pop('analysis_type', None)
                if 'analyzer_type' in analysis_data and analysis_data.get('analyzer_type') != analyzer_type:
                    # avoid inconsistent analyzer_type inside analysis_data
                    analysis_data.pop('analyzer_type', None)

            result = AnalysisResult(
                artist=artist,
                title=title,
                analyzer_type=analyzer_type,
                analysis_data=analysis_data,
                confidence=float(confidence or 0.0),
                processing_time=processing_time,
                track_id=track_id,
                status=AnalysisStatus.SUCCESS,
                metadata={
                    'source': 'advanced_algorithmic',
                },
                raw_output=analysis_data,
                timestamp=datetime.now().isoformat()
            )

            return result

        return await asyncio.to_thread(sync_analyze)


@register_analyzer("emotion_analyzer")
class EmotionAnalyzerWrapper(BaseAnalyzer):
    """Thin wrapper for the async EmotionAnalyzer implementation."""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self._legacy_config = config or {}
        self.name = "EmotionAnalyzerWrapper"
        self._instance = None

    @property
    def analyzer_type(self) -> str:
        return AnalyzerType.EMOTIONAL.value

    def get_analyzer_info(self) -> Dict[str, Any]:
        try:
            from src.analyzers.emotion_analyzer import EmotionAnalyzer as Legacy
            legacy = Legacy(self._legacy_config)
            return getattr(legacy, 'get_analyzer_info', lambda: {})()
        except Exception:
            return {
                'name': 'EmotionAnalyzerWrapper',
                'version': 'wrapper-1.0',
                'description': 'Wrapper for async emotion analyzer',
                'type': self.analyzer_type,
                'available': self.available,
                'supported_features': []
            }

    @property
    def supported_features(self) -> List[str]:
        try:
            from src.analyzers.emotion_analyzer import EmotionAnalyzer as Legacy
            legacy = Legacy(self._legacy_config)
            return getattr(legacy, 'supported_features', [])
        except Exception:
            return []

    async def analyze_song(self, artist: str, title: str, lyrics: str,
                           track_id: Optional[int] = None) -> AnalysisResult:
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")

        # Lazy create instance
        if self._instance is None:
            from src.analyzers.emotion_analyzer import EmotionAnalyzer as Legacy
            self._instance = Legacy(self._legacy_config)

        legacy = self._instance

        # delegate to async implementation
        emotion_result = await legacy.analyze_song(artist, title, lyrics)

        # If the legacy analyzer already returned the canonical AnalysisResult, return it
        if isinstance(emotion_result, AnalysisResult):
            return emotion_result

        # Helper to coerce various legacy result types into a plain dict
        def to_plain_dict(obj):
            if obj is None:
                return {}
            if isinstance(obj, dict):
                try:
                    return {str(k): v for k, v in obj.items()}
                except Exception:
                    return obj
            # pydantic
            if hasattr(obj, 'model_dump'):
                try:
                    return obj.model_dump()
                except Exception:
                    pass
            if hasattr(obj, 'dict'):
                try:
                    return obj.dict()
                except Exception:
                    pass
            # dataclass or simple object
            if hasattr(obj, '__dict__'):
                try:
                    return {k: v for k, v in vars(obj).items() if not k.startswith('_')}
                except Exception:
                    pass
            # last resort: JSON serialize/deserialize
            try:
                import json as _json
                return _json.loads(_json.dumps(obj, default=lambda o: getattr(o, '__dict__', str(o))))
            except Exception:
                return {}

        # Normalize legacy return into canonical AnalysisResult
        analyzer_type = getattr(emotion_result, 'analyzer_type', None) or getattr(emotion_result, 'analysis_type', None) or self.analyzer_type
        confidence = getattr(emotion_result, 'confidence', getattr(emotion_result, 'confidence_score', 0.0))
        analysis_payload = getattr(emotion_result, 'analysis_data', None) or getattr(emotion_result, 'raw_output', None) or getattr(emotion_result, 'raw', None) or emotion_result
        analysis_data = to_plain_dict(analysis_payload)

        # Defensive cleanup: remove legacy markers from payload
        if isinstance(analysis_data, dict):
            analysis_data.pop('analysis_type', None)
            if 'analyzer_type' in analysis_data and analysis_data.get('analyzer_type') != analyzer_type:
                analysis_data.pop('analyzer_type', None)

        return AnalysisResult(
            artist=artist,
            title=title,
            analyzer_type=analyzer_type,
            analysis_data=analysis_data or {},
            confidence=float(confidence or 0.0),
            processing_time=float(getattr(emotion_result, 'analysis_time', getattr(emotion_result, 'processing_time', 0.0)) or 0.0),
            track_id=track_id,
            status=AnalysisStatus.SUCCESS,
            metadata=getattr(emotion_result, 'metadata', {}) or {},
            raw_output=analysis_data if isinstance(analysis_data, dict) else {},
            timestamp=datetime.now().isoformat()
        )


@register_analyzer("multimodal")
class MultiModelAnalyzerWrapper(BaseAnalyzer):
    """Wrapper for legacy MultiModelAnalyzer orchestrator."""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self._legacy_config = config or {}
        self.name = "MultiModelAnalyzerWrapper"

    @property
    def analyzer_type(self) -> str:
        return AnalyzerType.MULTIMODAL.value

    def get_analyzer_info(self) -> Dict[str, Any]:
        try:
            from src.analyzers.multi_model_analyzer import MultiModelAnalyzer as Legacy
            legacy = Legacy()
            return getattr(legacy, 'get_analyzer_info', lambda: {})()
        except Exception:
            return {
                'name': 'MultiModelAnalyzerWrapper',
                'version': 'wrapper-1.0',
                'description': 'Wrapper for multi-model analyzer',
                'type': self.analyzer_type,
                'available': self.available,
                'supported_features': []
            }

    @property
    def supported_features(self) -> List[str]:
        try:
            from src.analyzers.multi_model_analyzer import MultiModelAnalyzer as Legacy
            legacy = Legacy()
            return getattr(legacy, 'supported_features', [])
        except Exception:
            return []

    async def analyze_song(self, artist: str, title: str, lyrics: str,
                           track_id: Optional[int] = None) -> AnalysisResult:
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")

        def sync_analyze():
            from src.analyzers.multi_model_analyzer import MultiModelAnalyzer as Legacy

            legacy = Legacy()
            raw = legacy.analyze_song(artist, title, lyrics)
            processing_time = getattr(raw, 'analysis_time', 0.0) if raw else 0.0

            # Convert EnhancedSongData or dict to AnalysisResult
            analysis_data = raw if isinstance(raw, dict) else getattr(raw, 'model_dump', lambda: {})()
            # normalize quality_metrics (could be dict, pydantic model, or object)
            def norm_metrics(m):
                if m is None:
                    return {}
                if isinstance(m, dict):
                    return m
                if hasattr(m, 'model_dump'):
                    try:
                        return m.model_dump()
                    except Exception:
                        pass
                if hasattr(m, 'dict'):
                    try:
                        return m.dict()
                    except Exception:
                        pass
                if hasattr(m, '__dict__'):
                    try:
                        return {k: v for k, v in vars(m).items() if not k.startswith('_')}
                    except Exception:
                        pass
                return {}

            quality_metrics = norm_metrics(getattr(raw, 'quality_metrics', None)) if raw else {}
            confidence = float(quality_metrics.get('authenticity_score', 0.0)) if isinstance(quality_metrics, dict) else 0.0

            return AnalysisResult(
                artist=artist,
                title=title,
                analyzer_type=self.analyzer_type,
                analysis_data=analysis_data or {},
                confidence=confidence,
                processing_time=processing_time,
                track_id=track_id,
                status=AnalysisStatus.SUCCESS,
                metadata={'source': 'multi_model'},
                raw_output=analysis_data if isinstance(analysis_data, dict) else {},
                timestamp=datetime.now().isoformat()
            )

        return await asyncio.to_thread(sync_analyze)


@register_analyzer("ollama")
class OllamaAnalyzerWrapper(BaseAnalyzer):
    """Wrapper for legacy OllamaAnalyzer (sync)."""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self._legacy_config = config or {}
        self.name = "OllamaAnalyzerWrapper"

    @property
    def analyzer_type(self) -> str:
        return AnalyzerType.OLLAMA.value

    def get_analyzer_info(self) -> Dict[str, Any]:
        try:
            from src.analyzers.ollama_analyzer import OllamaAnalyzer as Legacy
            legacy = Legacy(self._legacy_config)
            return getattr(legacy, 'get_analyzer_info', lambda: {})()
        except Exception:
            return {
                'name': 'OllamaAnalyzerWrapper',
                'version': 'wrapper-1.0',
                'description': 'Wrapper for Ollama analyzer',
                'type': self.analyzer_type,
                'available': self.available,
                'supported_features': []
            }

    @property
    def supported_features(self) -> List[str]:
        try:
            from src.analyzers.ollama_analyzer import OllamaAnalyzer as Legacy
            legacy = Legacy(self._legacy_config)
            return getattr(legacy, 'supported_features', [])
        except Exception:
            return []

    async def analyze_song(self, artist: str, title: str, lyrics: str,
                           track_id: Optional[int] = None) -> AnalysisResult:
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")

        def sync_analyze():
            from src.analyzers.ollama_analyzer import OllamaAnalyzer as Legacy
            legacy = Legacy(self._legacy_config)

            if not getattr(legacy, 'available', True):
                raise RuntimeError("Legacy Ollama analyzer not available")

            start = time.time()
            res = legacy.analyze_song(artist, title, lyrics)
            processing_time = time.time() - start

            if isinstance(res, AnalysisResult):
                return res

            # Fallback conversion
            return AnalysisResult(
                artist=artist,
                title=title,
                analyzer_type=self.analyzer_type,
                analysis_data=getattr(res, 'raw_output', {}) or {},
                confidence=getattr(res, 'confidence', 0.0),
                processing_time=processing_time,
                track_id=track_id,
                status=AnalysisStatus.SUCCESS,
                metadata={'model': getattr(res, 'model_name', legacy.model_name if hasattr(legacy, 'model_name') else None)},
                raw_output=getattr(res, 'raw_output', {}) or {},
                timestamp=datetime.now().isoformat()
            )

        return await asyncio.to_thread(sync_analyze)


@register_analyzer("simplified_features")
class SimplifiedFeatureAnalyzerWrapper(BaseAnalyzer):
    """Wrapper for simplified feature analyzer (sync or async)."""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self._legacy_config = config or {}
        self.name = "SimplifiedFeatureAnalyzerWrapper"

    @property
    def analyzer_type(self) -> str:
        return AnalyzerType.SIMPLIFIED.value

    def get_analyzer_info(self) -> Dict[str, Any]:
        try:
            # The simplified feature analyzer exposes a LyricsAnalyzer class
            from src.analyzers.simplified_feature_analyzer import LyricsAnalyzer as Legacy
            legacy = Legacy()
            # Legacy analyzer does not implement get_analyzer_info in older versions
            info = getattr(legacy, 'get_analyzer_info', lambda: {})()
            if not info:
                info = {
                    'name': 'LyricsAnalyzer',
                    'version': getattr(legacy, 'version', 'unknown'),
                    'description': 'Simplified features lyrics analyzer',
                }
            return info
        except Exception:
            return {
                'name': 'SimplifiedFeatureAnalyzerWrapper',
                'version': 'wrapper-1.0',
                'description': 'Wrapper for simplified feature analyzer',
                'type': self.analyzer_type,
                'available': self.available,
                'supported_features': []
            }

    @property
    def supported_features(self) -> List[str]:
        try:
            from src.analyzers.simplified_feature_analyzer import LyricsAnalyzer as Legacy
            legacy = Legacy()
            return getattr(legacy, 'supported_features', [])
        except Exception:
            return []

    async def analyze_song(self, artist: str, title: str, lyrics: str,
                           track_id: Optional[int] = None) -> AnalysisResult:
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")

        # The simplified analyzer exposes LyricsAnalyzer.analyze(lyrics, track_id)
        try:
            from src.analyzers.simplified_feature_analyzer import LyricsAnalyzer as Legacy
            legacy = Legacy()

            def sync_run():
                # Legacy analyze returns a pydantic LyricsFeatures instance
                return legacy.analyze(lyrics, track_id)

            features = await asyncio.to_thread(sync_run)

            # Convert pydantic model to dict
            if hasattr(features, 'model_dump'):
                features_dict = features.model_dump()
            else:
                features_dict = getattr(features, 'dict', lambda: {})()

            confidence = features_dict.get('confidence_score', features_dict.get('confidence', 0.0))
            processing_ms = features_dict.get('processing_time_ms', features_dict.get('processing_time', 0.0))

            return AnalysisResult(
                artist=artist,
                title=title,
                analyzer_type=self.analyzer_type,
                analysis_data=features_dict or {},
                confidence=confidence,
                processing_time=(processing_ms / 1000.0) if processing_ms else 0.0,
                track_id=track_id,
                status=AnalysisStatus.SUCCESS,
                metadata={'analyzer_version': features_dict.get('analyzer_version')},
                raw_output=features_dict,
                timestamp=datetime.now().isoformat()
            )

        except ImportError:
            raise
        except Exception as e:
            logger.error(f"SimplifiedFeatureAnalyzerWrapper failed: {e}")
            raise


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è CLI

async def test_analyzer(analyzer_name: str, test_lyrics: Optional[str] = None) -> None:
    """–¢–µ—Å—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    if test_lyrics is None:
        test_lyrics = """
        I've been working on my confidence
        Every day I'm getting better at it
        Started from the bottom now I'm here
        Money trees is the perfect place for shade
        """
    
    try:
        analyzer = AnalyzerFactory.create(analyzer_name)
        
        print(f"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {analyzer_name}")
        print(f"üìä –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å: {analyzer.available}")
        print(f"üéØ –¢–∏–ø: {analyzer.analyzer_type}")
        print(f"üîß –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∏—á–∏: {analyzer.supported_features}")
        
        if analyzer.available:
            result = await analyzer.analyze_song("Test Artist", "Test Song", test_lyrics)
            
            print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:")
            print(f"  Confidence: {result.confidence:.2f}")
            print(f"  Processing time: {result.processing_time:.2f}s")
            print(f"  Status: {result.status.value}")
            print(f"  Data keys: {list(result.analysis_data.keys())}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è {analyzer_name}: {e}")


async def test_all_analyzers() -> None:
    """–¢–µ—Å—Ç –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤...\n")
    
    available = AnalyzerFactory.list_available()
    print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã: {available}\n")
    
    for name in available:
        await test_analyzer(name)
        print("-" * 50)


if __name__ == "__main__":
    """
    Standalone –∑–∞–ø—É—Å–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        python src/interfaces/analyzer_interface.py
        python src/interfaces/analyzer_interface.py test qwen
        python src/interfaces/analyzer_interface.py stats
    """
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "test":
            if len(sys.argv) > 2:
                analyzer_name = sys.argv[2]
                asyncio.run(test_analyzer(analyzer_name))
            else:
                asyncio.run(test_all_analyzers())
        
        elif command == "list":
            available = AnalyzerFactory.list_available()
            print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã: {available}")
        
        elif command == "stats":
            stats = AnalyzerFactory.get_all_stats()
            print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤:")
            for name, stat in stats.items():
                print(f"  {name}: {stat}")
    
    else:
        print("üß† Analyzer Interface –¥–ª—è Rap Scraper –ø—Ä–æ–µ–∫—Ç–∞")
        print("–ö–æ–º–∞–Ω–¥—ã:")
        print("  test [analyzer_name] - —Ç–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞(–æ–≤)")
        print("  list                 - —Å–ø–∏—Å–æ–∫ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤") 
        print("  stats                - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤")