# üöÄ –ü–†–û–î–í–ò–ù–£–¢–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø –î–õ–Ø POSTGRESQL –°–ö–†–ê–ü–ï–†–ê
"""
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥:
1. –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∞–ø–µ—Ä (–≤–∞—à —Ç–µ–∫—É—â–∏–π rap_scraper_postgres.py)

–°—Ç–∞–±–∏–ª—å–Ω–∞—è, –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
–†–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (Redis, Prometheus)
–ë—ã—Å—Ç—Ä–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∏ –ª–µ–≥–∫–æ –æ—Ç–ª–∞–∂–∏–≤–∞–µ—Ç—Å—è
–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

2. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –≤–µ—Ä—Å–∏—è (–Ω–æ–≤—ã–π —Ñ–∞–π–ª ultra_rap_scraper_postgres.py)

–í—Å–µ —É–ª—É—á—à–µ–Ω–∏—è: Redis, Prometheus, async pool
–¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É
–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è production –∏–ª–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö
–ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ
"""

# –ë–∞–∑–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã
import logging
import os
import sys
from typing import List

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
try:
    from dotenv import load_dotenv
    # –ó–∞–≥—Ä—É–∂–∞–µ–º .env –∏–∑ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞
    env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')
    load_dotenv(env_path)
    print(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω .env —Ñ–∞–π–ª: {env_path}")
except ImportError:
    print("‚ö†Ô∏è python-dotenv –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ —Å–∏—Å—Ç–µ–º—ã")

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –±–∞–∑–æ–≤–æ–≥–æ —Å–∫—Ä–∞–ø–µ—Ä–∞
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

try:
    from .rap_scraper_postgres import OptimizedPostgreSQLScraper, ScrapingStatus
except ImportError:
    # Fallback –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
    from rap_scraper_postgres import OptimizedPostgreSQLScraper, ScrapingStatus

logger = logging.getLogger(__name__)

# –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è (–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è)
GENIUS_TOKEN = (os.getenv('GENIUS_TOKEN') or 
                os.getenv('GENIUS_ACCESS_TOKEN') or 
                os.getenv('GENIUS_API_TOKEN') or '')

if GENIUS_TOKEN:
    print(f"‚úÖ GENIUS_TOKEN –Ω–∞–π–¥–µ–Ω: {GENIUS_TOKEN[:20]}...")
else:
    print("‚ùå GENIUS_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")

# 1. REDIS –ö–≠–®–ò–†–û–í–ê–ù–ò–ï (–¥–æ–±–∞–≤–∏—Ç—å –≤ requirements.txt: redis)
import redis
from typing import Optional
import pickle
import json

class RedisCache:
    """Redis –∫—ç—à –¥–ª—è –∞—Ä—Ç–∏—Å—Ç–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, host='localhost', port=6379, db=0, ttl=3600):
        try:
            self.redis = redis.Redis(host=host, port=port, db=db, decode_responses=False)
            self.redis.ping()  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            self.ttl = ttl
            self.enabled = True
            logger.info("‚úÖ Redis –∫—ç—à –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à: {e}")
            self.enabled = False
            self.local_cache = {}
    
    def get_artist_songs(self, artist_name: str) -> Optional[List]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Å–µ–Ω –∞—Ä—Ç–∏—Å—Ç–∞ –∏–∑ –∫—ç—à–∞"""
        if not self.enabled:
            return self.local_cache.get(f"artist:{artist_name}")
        
        try:
            key = f"artist_songs:{artist_name.lower()}"
            cached = self.redis.get(key)
            if cached:
                return pickle.loads(cached)
        except Exception as e:
            logger.debug(f"Redis get error: {e}")
        return None
    
    def cache_artist_songs(self, artist_name: str, songs: List, ttl_override=None):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Å–µ–Ω –∞—Ä—Ç–∏—Å—Ç–∞"""
        ttl = ttl_override or self.ttl
        
        if not self.enabled:
            self.local_cache[f"artist:{artist_name}"] = songs
            return
        
        try:
            key = f"artist_songs:{artist_name.lower()}"
            self.redis.setex(key, ttl, pickle.dumps(songs))
            logger.debug(f"üìã –ö—ç—à–∏—Ä–æ–≤–∞–Ω –∞—Ä—Ç–∏—Å—Ç {artist_name}")
        except Exception as e:
            logger.debug(f"Redis cache error: {e}")
    
    def is_song_processed(self, song_hash: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–µ—Å–Ω–∏ –ø–æ —Ö—ç—à—É"""
        if not self.enabled:
            return song_hash in self.local_cache.get('processed_songs', set())
        
        try:
            return bool(self.redis.sismember('processed_songs', song_hash))
        except:
            return False
    
    def mark_song_processed(self, song_hash: str):
        """–û—Ç–º–µ—Ç–∫–∞ –ø–µ—Å–Ω–∏ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π"""
        if not self.enabled:
            if 'processed_songs' not in self.local_cache:
                self.local_cache['processed_songs'] = set()
            self.local_cache['processed_songs'].add(song_hash)
            return
        
        try:
            self.redis.sadd('processed_songs', song_hash)
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º TTL –¥–ª—è set (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            self.redis.expire('processed_songs', self.ttl * 24)  # 24 —á–∞—Å–∞
        except Exception as e:
            logger.debug(f"Redis mark error: {e}")

# 2. PROMETHEUS –ú–ï–¢–†–ò–ö–ò (–¥–æ–±–∞–≤–∏—Ç—å –≤ requirements.txt: prometheus-client)
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import threading

class PrometheusMetrics:
    """Prometheus –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    
    def __init__(self, port=8090):
        # –°—á–µ—Ç—á–∏–∫–∏
        self.songs_processed = Counter('scraper_songs_processed_total', 'Total processed songs')
        self.songs_added = Counter('scraper_songs_added_total', 'Total added songs')
        self.songs_skipped = Counter('scraper_songs_skipped_total', 'Total skipped songs', ['reason'])
        self.api_calls = Counter('scraper_api_calls_total', 'Total API calls', ['endpoint'])
        self.errors = Counter('scraper_errors_total', 'Total errors', ['type'])
        
        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –≤—Ä–µ–º–µ–Ω–∏
        self.processing_time = Histogram('scraper_processing_duration_seconds', 'Song processing time')
        self.batch_save_time = Histogram('scraper_batch_save_duration_seconds', 'Batch save time')
        self.api_response_time = Histogram('scraper_api_response_duration_seconds', 'API response time')
        
        # Gauge –¥–ª—è —Ç–µ–∫—É—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        self.memory_usage = Gauge('scraper_memory_usage_mb', 'Current memory usage in MB')
        self.cpu_usage = Gauge('scraper_cpu_usage_percent', 'Current CPU usage')
        self.queue_size = Gauge('scraper_batch_queue_size', 'Current batch queue size')
        self.circuit_breaker_state = Gauge('scraper_circuit_breaker_open', 'Circuit breaker state (1=open, 0=closed)')
        
        # –ó–∞–ø—É—Å–∫ HTTP —Å–µ—Ä–≤–µ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        try:
            start_http_server(port)
            logger.info(f"üìä Prometheus –º–µ—Ç—Ä–∏–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞ –ø–æ—Ä—Ç—É {port}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å Prometheus —Å–µ—Ä–≤–µ—Ä: {e}")
    
    def record_song_processed(self):
        self.songs_processed.inc()
    
    def record_song_added(self):
        self.songs_added.inc()
    
    def record_song_skipped(self, reason: str):
        self.songs_skipped.labels(reason=reason).inc()
    
    def record_api_call(self, endpoint: str, duration: float):
        self.api_calls.labels(endpoint=endpoint).inc()
        self.api_response_time.observe(duration)
    
    def record_error(self, error_type: str):
        self.errors.labels(type=error_type).inc()
    
    def update_memory_usage(self, mb: float):
        self.memory_usage.set(mb)
    
    def update_cpu_usage(self, percent: float):
        self.cpu_usage.set(percent)
    
    def update_queue_size(self, size: int):
        self.queue_size.set(size)
    
    def update_circuit_breaker_state(self, is_open: bool):
        self.circuit_breaker_state.set(1 if is_open else 0)

# 3. –ê–°–ò–ù–•–†–û–ù–ù–´–ô POSTGRESQL CONNECTION POOL
import asyncpg
import asyncio
from contextlib import asynccontextmanager

class AsyncPostgreSQLManager:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä PostgreSQL —Å connection pool"""
    
    def __init__(self, connection_string: str, min_size=5, max_size=20):
        self.connection_string = connection_string
        self.min_size = min_size
        self.max_size = max_size
        self.pool = None
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è connection pool"""
        try:
            self.pool = await asyncpg.create_pool(
                self.connection_string,
                min_size=self.min_size,
                max_size=self.max_size,
                command_timeout=60
            )
            logger.info(f"üèä PostgreSQL pool —Å–æ–∑–¥–∞–Ω: {self.min_size}-{self.max_size} —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è PostgreSQL pool: {e}")
            raise
    
    @asynccontextmanager
    async def get_connection(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π context manager –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if not self.pool:
            await self.initialize()
        
        async with self.pool.acquire() as conn:
            yield conn
    
    async def batch_add_songs_optimized(self, songs_batch: List[dict]) -> int:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –±–∞—Ç—á–µ–≤–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Å–µ–Ω"""
        if not songs_batch:
            return 0
        
        async with self.get_connection() as conn:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è COPY
            rows = []
            for song in songs_batch:
                rows.append((
                    song['artist'],
                    song['title'],
                    song['lyrics'],
                    song['url'],
                    song.get('genius_id'),
                    json.dumps(song.get('metadata', {}))
                ))
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º COPY –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            try:
                result = await conn.copy_records_to_table(
                    'tracks',
                    records=rows,
                    columns=['artist', 'title', 'lyrics', 'url', 'genius_id', 'metadata_json'],
                    timeout=30
                )
                return len(rows)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ COPY: {e}")
                # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É INSERT
                return await self._fallback_batch_insert(conn, songs_batch)
    
    async def _fallback_batch_insert(self, conn, songs_batch: List[dict]) -> int:
        """Fallback –º–µ—Ç–æ–¥ —Å –æ–±—ã—á–Ω—ã–º–∏ INSERT"""
        saved_count = 0
        async with conn.transaction():
            for song in songs_batch:
                try:
                    await conn.execute("""
                        INSERT INTO tracks (artist, title, lyrics, url, genius_id, metadata_json)
                        VALUES ($1, $2, $3, $4, $5, $6)
                        ON CONFLICT (url) DO NOTHING
                    """, song['artist'], song['title'], song['lyrics'], 
                        song['url'], song.get('genius_id'), 
                        json.dumps(song.get('metadata', {})))
                    saved_count += 1
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –≤—Å—Ç–∞–≤–∫–∏ –ø–µ—Å–Ω–∏ {song['title']}: {e}")
        return saved_count
    
    async def song_exists_batch(self, urls: List[str]) -> set:
        """–ë–∞—Ç—á–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø–µ—Å–µ–Ω –ø–æ URL"""
        if not urls:
            return set()
        
        async with self.get_connection() as conn:
            result = await conn.fetch(
                "SELECT url FROM tracks WHERE url = ANY($1)",
                urls
            )
            return {row['url'] for row in result}
    
    async def get_stats_async(self) -> dict:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        async with self.get_connection() as conn:
            stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_songs,
                    COUNT(DISTINCT artist) as unique_artists,
                    AVG(array_length(string_to_array(lyrics, ' '), 1)) as avg_words,
                    COUNT(CASE WHEN metadata_json IS NOT NULL THEN 1 END) as with_metadata
                FROM tracks
            """)
            return dict(stats) if stats else {}
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ connection pool"""
        if self.pool:
            await self.pool.close()

# 4. INTELLIGENT RATE LIMITER
from collections import deque
import time

class IntelligentRateLimiter:
    """–£–º–Ω—ã–π rate limiter —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—É–∑–∞–º–∏"""
    
    def __init__(self, base_requests_per_minute=30):
        self.base_rpm = base_requests_per_minute
        self.current_rpm = base_requests_per_minute
        self.request_times = deque()
        self.recent_errors = deque()
        self.success_streak = 0
        self.last_adjustment = time.time()
        
    def can_make_request(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞"""
        now = time.time()
        
        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (—Å—Ç–∞—Ä—à–µ –º–∏–Ω—É—Ç—ã)
        while self.request_times and now - self.request_times[0] > 60:
            self.request_times.popleft()
        
        return len(self.request_times) < self.current_rpm
    
    def record_request(self, success: bool = True):
        """–ó–∞–ø–∏—Å—å –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        now = time.time()
        self.request_times.append(now)
        
        if success:
            self.success_streak += 1
            self.recent_errors = deque()  # –°–±—Ä–æ—Å –æ—à–∏–±–æ–∫ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
        else:
            self.recent_errors.append(now)
            self.success_streak = 0
        
        self._adjust_rate_limit()
    
    def _adjust_rate_limit(self):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ª–∏–º–∏—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        now = time.time()
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –Ω–µ —á–∞—â–µ —Ä–∞–∑–∞ –≤ 30 —Å–µ–∫—É–Ω–¥
        if now - self.last_adjustment < 30:
            return
        
        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –æ—à–∏–±–æ–∫
        while self.recent_errors and now - self.recent_errors[0] > 300:  # 5 –º–∏–Ω—É—Ç
            self.recent_errors.popleft()
        
        error_rate = len(self.recent_errors) / 5  # –æ—à–∏–±–æ–∫ –≤ –º–∏–Ω—É—Ç—É –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –º–∏–Ω—É—Ç
        
        if error_rate > 2:  # –ú–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ - –∑–∞–º–µ–¥–ª—è–µ–º—Å—è
            self.current_rpm = max(10, self.current_rpm * 0.7)
            logger.info(f"üêå Rate limit —Å–Ω–∏–∂–µ–Ω –¥–æ {self.current_rpm:.0f} req/min")
        elif self.success_streak > 50 and error_rate < 0.5:  # –ú–∞–ª–æ –æ—à–∏–±–æ–∫ - —É—Å–∫–æ—Ä—è–µ–º—Å—è
            self.current_rpm = min(self.base_rpm * 1.5, self.current_rpm * 1.2)
            logger.info(f"üöÄ Rate limit —É–≤–µ–ª–∏—á–µ–Ω –¥–æ {self.current_rpm:.0f} req/min")
        
        self.last_adjustment = now
    
    async def wait_if_needed(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–∞—É–∑–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        while not self.can_make_request():
            await asyncio.sleep(1)

# 5. SMART RETRY DECORATOR
from functools import wraps
import random

def smart_retry(max_retries=3, base_delay=1.0, backoff_factor=2.0, jitter=True):
    """–£–º–Ω—ã–π retry –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä —Å jitter –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–µ–π –æ—à–∏–±–æ–∫"""
    
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    error_msg = str(e).lower()
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏
                    if any(keyword in error_msg for keyword in ['rate limit', '429', 'too many requests']):
                        delay = base_delay * (backoff_factor ** attempt) + random.uniform(10, 30)
                        logger.warning(f"üîí Rate limit, –ø–∞—É–∑–∞ {delay:.1f}—Å")
                    elif any(keyword in error_msg for keyword in ['timeout', 'connection', 'network']):
                        delay = base_delay * (backoff_factor ** attempt)
                        if jitter:
                            delay += random.uniform(0, delay * 0.1)
                        logger.warning(f"üåê –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞, –ø–∞—É–∑–∞ {delay:.1f}—Å")
                    elif '404' in error_msg or 'not found' in error_msg:
                        # –ù–µ —Ä–µ—Ç—Ä–∞–∏–º 404 –æ—à–∏–±–∫–∏
                        logger.debug(f"üì≠ 404 –æ—à–∏–±–∫–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: {e}")
                        raise e
                    else:
                        delay = base_delay * (backoff_factor ** attempt)
                        logger.warning(f"‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞, –ø–∞—É–∑–∞ {delay:.1f}—Å: {e}")
                    
                    if attempt < max_retries:
                        await asyncio.sleep(delay)
                    else:
                        logger.error(f"‚ùå –ò—Å—á–µ—Ä–ø–∞–Ω—ã –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è {func.__name__}")
            
            raise last_exception
        return wrapper
    return decorator

# 6. ENHANCED BATCH PROCESSOR –° –ü–†–ò–û–†–ò–¢–ï–¢–ê–ú–ò
from queue import PriorityQueue
import threading
from dataclasses import dataclass, field
from typing import Any

@dataclass
class PriorityTask:
    priority: int
    data: Any = field(compare=False)
    created_at: float = field(default_factory=time.time, compare=False)

class PriorityBatchProcessor:
    """–ë–∞—Ç—á–µ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∏ —É–º–Ω–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π"""
    
    def __init__(self, batch_size=15, flush_interval=30.0, max_queue_size=1000):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.max_queue_size = max_queue_size
        
        self.priority_queue = PriorityQueue(maxsize=max_queue_size)
        self.last_flush = time.time()
        self.stats = {
            'high_priority': 0,
            'normal_priority': 0,
            'low_priority': 0,
            'batches_flushed': 0
        }
        
    def add_song(self, song_data: dict, priority: int = 5) -> bool:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Å–Ω–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º (1=–≤—ã—Å–æ–∫–∏–π, 10=–Ω–∏–∑–∫–∏–π)"""
        try:
            task = PriorityTask(priority=priority, data=song_data)
            self.priority_queue.put_nowait(task)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º
            if priority <= 3:
                self.stats['high_priority'] += 1
            elif priority <= 7:
                self.stats['normal_priority'] += 1
            else:
                self.stats['low_priority'] += 1
            
            return self._should_flush()
        except:
            logger.warning("‚ö†Ô∏è –û—á–µ—Ä–µ–¥—å –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∞, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π flush")
            return True
    
    def _should_flush(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–±—Ä–æ—Å–∞ –±–∞—Ç—á–∞"""
        current_time = time.time()
        queue_size = self.priority_queue.qsize()
        
        return (queue_size >= self.batch_size or 
                current_time - self.last_flush > self.flush_interval or
                queue_size > self.max_queue_size * 0.8)
    
    def get_priority_batch(self) -> List[dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞—Ç—á–∞ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤"""
        batch = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –±–∞—Ç—á, –Ω–∞—á–∏–Ω–∞—è —Å –≤—ã—Å–æ–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        while len(batch) < self.batch_size and not self.priority_queue.empty():
            try:
                task = self.priority_queue.get_nowait()
                batch.append(task.data)
            except:
                break
        
        if batch:
            self.last_flush = time.time()
            self.stats['batches_flushed'] += 1
            
        return batch
    
    def get_queue_stats(self) -> dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–µ—Ä–µ–¥–∏"""
        return {
            **self.stats,
            'current_queue_size': self.priority_queue.qsize(),
            'queue_utilization': f"{(self.priority_queue.qsize() / self.max_queue_size) * 100:.1f}%"
        }

# 7. –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –í–°–ï–• –£–õ–£–ß–®–ï–ù–ò–ô –í –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–°
class UltraOptimizedScraper(OptimizedPostgreSQLScraper):
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä —Å–æ –≤—Å–µ–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏"""
    
    def __init__(self, token: str, memory_limit_mb: int = 4096, batch_size: int = 15, 
                 redis_host='localhost', enable_prometheus=True):
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        super().__init__(token, memory_limit_mb, batch_size)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.redis_cache = RedisCache(host=redis_host)
        self.rate_limiter = IntelligentRateLimiter(base_requests_per_minute=45)
        self.priority_processor = PriorityBatchProcessor(batch_size=batch_size)
        
        # –ó–∞–º–µ–Ω–∞ –æ–±—ã—á–Ω–æ–≥–æ batch_processor –Ω–∞ priority
        self.batch_processor = self.priority_processor
        
        # Prometheus –º–µ—Ç—Ä–∏–∫–∏
        if enable_prometheus:
            try:
                self.prometheus = PrometheusMetrics()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Prometheus –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.prometheus = None
        else:
            self.prometheus = None
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ë–î
        try:
            from ..utils.config import get_db_config
            db_config = get_db_config()
            connection_string = (f"postgresql://{db_config['user']}:{db_config['password']}"
                               f"@{db_config['host']}:{db_config['port']}/{db_config['database']}")
            self.async_db = AsyncPostgreSQLManager(connection_string)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Async DB –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
            self.async_db = None
        
        logger.info("üöÄ UltraOptimizedScraper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å–æ –≤—Å–µ–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏")
    
    @smart_retry(max_retries=3, base_delay=2.0)
    async def enhanced_get_artist_songs(self, artist_name: str, max_songs: int = 500):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–µ—Å–µ–Ω —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ rate limiting"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Redis –∫—ç—à
        cached_songs = self.redis_cache.get_artist_songs(artist_name)
        if cached_songs:
            logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ –≤ Redis –∫—ç—à–µ: {len(cached_songs)} –ø–µ—Å–µ–Ω –¥–ª—è {artist_name}")
            self.metrics.cache_hits += len(cached_songs)
            for i, song in enumerate(cached_songs):
                yield song, i + 1
            return
        
        # Rate limiting
        await self.rate_limiter.wait_if_needed()
        
        # API –∑–∞–ø—Ä–æ—Å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        api_start = time.time()
        try:
            artist = await asyncio.get_event_loop().run_in_executor(
                self.executor,
                lambda: self.genius.search_artist(
                    artist_name, 
                    max_songs=min(max_songs, 100),
                    sort="popularity"
                )
            )
            
            api_duration = time.time() - api_start
            self.rate_limiter.record_request(success=True)
            
            if self.prometheus:
                self.prometheus.record_api_call('search_artist', api_duration)
            
        except Exception as e:
            self.rate_limiter.record_request(success=False)
            if self.prometheus:
                self.prometheus.record_error('api_search')
            raise e
        
        if not artist or not artist.songs:
            return
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Redis
        self.redis_cache.cache_artist_songs(artist_name, artist.songs[:50])
        
        logger.info(f"üìÄ –ù–∞–π–¥–µ–Ω–æ {len(artist.songs)} –ø–µ—Å–µ–Ω –¥–ª—è {artist_name}")
        
        for i, song in enumerate(artist.songs):
            if self.shutdown_requested:
                break
            yield song, i + 1
    
    async def ultra_process_song(self, song, song_number: int, artist_name: str) -> ScrapingStatus:
        """–£–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Å–Ω–∏"""
        
        if self.prometheus:
            self.prometheus.record_song_processed()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö—ç—à —Ä–∞–Ω–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ Redis
        song_hash = self.generate_song_hash(artist_name, song.title, 
                                           getattr(song, 'lyrics', '')[:200])
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Redis –∫—ç—à–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–µ—Å–µ–Ω
        if self.redis_cache.is_song_processed(song_hash):
            if self.prometheus:
                self.prometheus.record_song_skipped('redis_cache')
            return ScrapingStatus.SKIPPED_DUPLICATE
        
        # –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏...
        status = await super().process_single_song(song, song_number, artist_name)
        
        # –û—Ç–º–µ—á–∞–µ–º –≤ Redis –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é
        if status == ScrapingStatus.SUCCESS:
            self.redis_cache.mark_song_processed(song_hash)
        
        # Prometheus –º–µ—Ç—Ä–∏–∫–∏
        if self.prometheus:
            if status == ScrapingStatus.SUCCESS:
                self.prometheus.record_song_added()
            else:
                reason = status.value
                self.prometheus.record_song_skipped(reason)
        
        return status
    
    async def run_ultra_session(self, artists: List[str], songs_per_artist: int = 500):
        """–ó–∞–ø—É—Å–∫ —É–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–µ—Å—Å–∏–∏"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ ULTRA-–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô —Å–µ—Å—Å–∏–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è async –ë–î
        if self.async_db:
            await self.async_db.initialize()
        
        try:
            await self.run_async_scraping_session(artists, songs_per_artist)
        finally:
            if self.async_db:
                await self.async_db.close()
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            if self.prometheus:
                logger.info("üìä Prometheus –º–µ—Ç—Ä–∏–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã")
            
            queue_stats = self.priority_processor.get_queue_stats()
            logger.info(f"üì¶ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–µ—Ä–µ–¥–∏: {queue_stats}")

# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –£–õ–£–ß–®–ï–ù–ò–ô

async def run_ultra_scraper():
    """–ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ —É–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–µ—Ä–∞"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–∞
    if not GENIUS_TOKEN:
        print("‚ùå GENIUS_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        print("üìù –î–ª—è —Ä–∞–±–æ—Ç—ã ultra-—Å–∫—Ä–∞–ø–µ—Ä–∞ –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω –æ—Ç Genius API")
        print("üîß –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: set GENIUS_TOKEN=your_token")
        print()
        print("üß™ –ó–∞–ø—É—Å–∫–∞–µ–º –î–ï–ú–û-–¢–ï–°–¢ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –±–µ–∑ —Ç–æ–∫–µ–Ω–∞...")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞
        await test_ultra_components()
        return
    
    scraper = UltraOptimizedScraper(
        token=GENIUS_TOKEN,
        memory_limit_mb=6144,  # 6GB
        batch_size=20,         # –ë–æ–ª—å—à–∏–π –±–∞—Ç—á
        redis_host='localhost',
        enable_prometheus=True
    )
    
    artists = [
        "Kendrick Lamar", "J. Cole", "Drake", "Travis Scott",
        "Future", "21 Savage", "Tyler, The Creator"
    ]
    
    await scraper.run_ultra_session(artists, songs_per_artist=300)

async def test_ultra_components():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ultra-—Å–∫—Ä–∞–ø–µ—Ä–∞ –±–µ–∑ —Ç–æ–∫–µ–Ω–∞"""
    print("=" * 60)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–û–ú–ü–û–ù–ï–ù–¢–û–í ULTRA-–°–ö–†–ê–ü–ï–†–ê")
    print("=" * 60)
    
    # 1. –¢–µ—Å—Ç Redis Cache
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º Redis Cache...")
    try:
        cache = RedisCache()
        print(f"   ‚úÖ Redis Cache –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (enabled: {cache.enabled})")
        
        # –¢–µ—Å—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        test_songs = [{'title': 'Test Song', 'artist': 'Test Artist'}]
        cache.cache_artist_songs('Test Artist', test_songs)
        cached = cache.get_artist_songs('Test Artist')
        print(f"   ‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {len(cached) if cached else 0} –ø–µ—Å–µ–Ω")
    except Exception as e:
        print(f"   ‚ùå Redis Cache –æ—à–∏–±–∫–∞: {e}")
    
    # 2. –¢–µ—Å—Ç Rate Limiter
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º Rate Limiter...")
    try:
        limiter = IntelligentRateLimiter(base_requests_per_minute=60)
        can_request = limiter.can_make_request()
        print(f"   ‚úÖ Rate Limiter: –º–æ–∂–Ω–æ –¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å = {can_request}")
        
        # –°–∏–º—É–ª—è—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
        for i in range(5):
            limiter.record_request(success=True)
        print(f"   ‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ 5 —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, RPM = {limiter.current_rpm}")
    except Exception as e:
        print(f"   ‚ùå Rate Limiter –æ—à–∏–±–∫–∞: {e}")
    
    # 3. –¢–µ—Å—Ç Priority Processor
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º Priority Processor...")
    try:
        processor = PriorityBatchProcessor(batch_size=5)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
        test_tasks = [
            ({'artist': 'Eminem', 'song': 'Lose Yourself'}, 1),
            ({'artist': 'Unknown', 'song': 'Random Song'}, 8),
            ({'artist': 'Kendrick Lamar', 'song': 'HUMBLE'}, 2),
        ]
        
        for task_data, priority in test_tasks:
            processor.add_song(task_data, priority)
        
        batch = processor.get_priority_batch()
        print(f"   ‚úÖ Priority Processor: –ø–æ–ª—É—á–µ–Ω –±–∞—Ç—á –∏–∑ {len(batch)} –∑–∞–¥–∞—á")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Ä—è–¥–æ–∫ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        for i, task in enumerate(batch, 1):
            print(f"      {i}. {task['artist']} - {task['song']}")
        
        stats = processor.get_queue_stats()
        print(f"   ‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")
        
    except Exception as e:
        print(f"   ‚ùå Priority Processor –æ—à–∏–±–∫–∞: {e}")
    
    # 4. –¢–µ—Å—Ç Prometheus Metrics
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º Prometheus Metrics...")
    try:
        metrics = PrometheusMetrics(port=8091)  # –î—Ä—É–≥–æ–π –ø–æ—Ä—Ç –Ω–∞ —Å–ª—É—á–∞–π –∑–∞–Ω—è—Ç–æ—Å—Ç–∏
        print("   ‚úÖ Prometheus Metrics –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        
        # –¢–µ—Å—Ç –º–µ—Ç—Ä–∏–∫
        metrics.record_song_processed()
        metrics.record_song_added()
        metrics.record_song_skipped('test_reason')
        metrics.update_memory_usage(1024.5)
        metrics.update_cpu_usage(45.2)
        
        print("   ‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–ø–∏—Å–∞–Ω—ã")
        print("   üìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞ http://localhost:8091")
        
    except Exception as e:
        print(f"   ‚ùå Prometheus Metrics –æ—à–∏–±–∫–∞: {e}")
    
    # 5. –¢–µ—Å—Ç Smart Retry
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º Smart Retry...")
    try:
        @smart_retry(max_retries=2, base_delay=0.1)
        async def test_function():
            import random
            if random.random() < 0.7:  # 70% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—à–∏–±–∫–∏
                raise Exception("Test error for retry")
            return "Success!"
        
        try:
            result = await test_function()
            print(f"   ‚úÖ Smart Retry —É—Å–ø–µ—à–Ω–æ: {result}")
        except:
            print("   ‚ö†Ô∏è Smart Retry: –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã (—ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∞)")
            
    except Exception as e:
        print(f"   ‚ùå Smart Retry –æ—à–∏–±–∫–∞: {e}")
    
    print("=" * 60)
    print("üéâ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–û–ú–ü–û–ù–ï–ù–¢–û–í –ó–ê–í–ï–†–®–ï–ù–û")
    print("üí° –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ GENIUS_TOKEN")
    print("üìã –ó–∞–ø—É—Å—Ç–∏—Ç–µ Redis: docker run -d -p 6379:6379 redis")
    print("=" * 60)

# DOCKER COMPOSE –î–û–ü–û–õ–ù–ï–ù–ò–Ø
"""
# –î–æ–±–∞–≤–∏—Ç—å –≤ docker-compose.yml:

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin

volumes:
  redis_data:
  prometheus_data:
  grafana_data:
"""

# PROMETHEUS –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
"""
# monitoring/prometheus.yml:
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'rap-scraper'
    static_configs:
      - targets: ['localhost:8090']
    scrape_interval: 10s
"""

if __name__ == "__main__":
    import asyncio
    
    print("üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è PostgreSQL —Å–∫—Ä–∞–ø–µ—Ä–∞ –≥–æ—Ç–æ–≤—ã!")
    print("üìã –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ requirements.txt: redis prometheus-client")
    print("üê≥ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ docker-compose –¥–ª—è Redis –∏ Prometheus")
    print("‚ö° –ó–∞–ø—É—Å–∫–∞–µ–º ultra-—Å–∫—Ä–∞–ø–µ—Ä...")
    print()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º ultra-—Å–∫—Ä–∞–ø–µ—Ä
    asyncio.run(run_ultra_scraper())
