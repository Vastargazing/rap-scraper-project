#!/usr/bin/env python3
"""
ü§ñ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π Qwen –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (PostgreSQL + –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Qwen)

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Qwen –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–∏–∑ archive/qwen_analyzer.py)
- –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ PostgreSQL –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å–∏—Å—Ç–µ–º–æ–π

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
python src/analyzers/mass_qwen_analysis.py --test      # –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º
python src/analyzers/mass_qwen_analysis.py --stats    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
python src/analyzers/mass_qwen_analysis.py --batch 100 # –ö–∞—Å—Ç–æ–º–Ω—ã–π –±–∞—Ç—á
python src/analyzers/mass_qwen_analysis.py --resume   # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å

–ê–í–¢–û–†: AI Assistant
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
–í–ï–†–°–ò–Ø: 3.0 (Unified)
"""

import argparse
import asyncio
import json
import logging
import os
import sys
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –≤ path
project_root = os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
sys.path.insert(0, project_root)

# –£—Å–ª–æ–≤–Ω—ã–π –∏–º–ø–æ—Ä—Ç –¥–ª—è OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
try:
    import openai

    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    from src.core.app import create_app
    from src.database.postgres_adapter import PostgreSQLManager
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –∑–∞–ø—É—Å–∫–∞–µ—Ç–µ —Å–∫—Ä–∏–ø—Ç –∏–∑ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞")
    sys.exit(1)

logger = logging.getLogger(__name__)


# ============================================================================
# –í–°–¢–†–û–ï–ù–ù–´–ô QWEN –ê–ù–ê–õ–ò–ó–ê–¢–û–† (–∏–∑ archive/qwen_analyzer.py)
# ============================================================================


@dataclass
class AnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–∞—Å—Å–æ–≤—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º"""

    artist: str
    title: str
    analyzer_type: str
    confidence: float
    metadata: dict[str, Any]
    raw_output: dict[str, Any]
    processing_time: float
    timestamp: str


class EmbeddedQwenAnalyzer:
    """
    –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Qwen –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∏–∑ archive/qwen_analyzer.py)
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ Qwen –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.config = config or {}

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
        self.model_name = self.config.get("model_name", "qwen/qwen3-4b-fp8")
        self.base_url = self.config.get("base_url", "https://api.novita.ai/openai/v1")
        self.temperature = self.config.get("temperature", 0.1)
        self.max_tokens = self.config.get("max_tokens", 1500)
        self.timeout = self.config.get("timeout", 30)

        # API –∫–ª—é—á
        self.api_key = self.config.get("api_key") or os.getenv("NOVITA_API_KEY")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        self.available = self._check_availability()

        if self.available:
            self.client = openai.OpenAI(
                api_key=self.api_key, base_url=self.base_url, timeout=self.timeout
            )
            logger.info(
                f"‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Qwen –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.model_name}"
            )
        else:
            logger.warning("‚ö†Ô∏è –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Qwen –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

    def _check_availability(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Novita AI API"""
        if not HAS_OPENAI:
            logger.error("‚ùå openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
            return False

        if not self.api_key:
            logger.error(
                "‚ùå NOVITA_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"
            )
            return False

        try:
            # –¢–µ—Å—Ç–æ–≤–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)

            response = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Test"},
                ],
                max_tokens=10,
                temperature=0.1,
            )

            if response.choices and response.choices[0].message:
                logger.info("‚úÖ Novita AI Qwen API —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω")
                return True
            logger.error("‚ùå –ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Qwen API")
            return False

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Qwen API: {e}")
            return False

    def validate_input(self, artist: str, title: str, lyrics: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not all([artist, title, lyrics]):
            return False
        if len(lyrics.strip()) < 10:
            return False
        return True

    def preprocess_lyrics(self, lyrics: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Å–Ω–∏"""
        lyrics = lyrics.strip()

        # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        import re

        lyrics = re.sub(r"\s+", " ", lyrics)

        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–∏–º–≤–æ–ª–æ–≤
        lyrics = re.sub(r"(.)\1{3,}", r"\1\1\1", lyrics)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        lyrics = re.sub(r"\n{3,}", "\n\n", lyrics)

        # –£–¥–∞–ª–µ–Ω–∏–µ URL –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        lyrics = re.sub(r"http[s]?://\S+", "", lyrics)
        lyrics = re.sub(r'[^\w\s\n.,!?\'"-]', "", lyrics)

        return lyrics.strip()

    def analyze_song(self, artist: str, title: str, lyrics: str) -> AnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑ –ø–µ—Å–Ω–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Qwen –º–æ–¥–µ–ª–∏
        """
        start_time = time.time()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")

        if not self.available:
            raise RuntimeError("Qwen analyzer is not available")

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        processed_lyrics = self.preprocess_lyrics(lyrics)

        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            system_prompt, user_prompt = self._create_analysis_prompts(
                artist, title, processed_lyrics
            )

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
            )

            if not response.choices or not response.choices[0].message.content:
                raise RuntimeError("Empty response from Qwen model")

            # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            analysis_data = self._parse_response(response.choices[0].message.content)

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = self._calculate_confidence(analysis_data)

            processing_time = time.time() - start_time

            return AnalysisResult(
                artist=artist,
                title=title,
                analyzer_type="qwen",  # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø–æ–ª–µ!
                confidence=confidence,
                metadata={
                    "model_name": self.model_name,
                    "model_version": "qwen3-4b-fp8",
                    "processing_date": datetime.now().isoformat(),
                    "lyrics_length": len(processed_lyrics),
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens,
                    "provider": "Novita AI",
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens
                        if response.usage
                        else 0,
                        "completion_tokens": response.usage.completion_tokens
                        if response.usage
                        else 0,
                        "total_tokens": response.usage.total_tokens
                        if response.usage
                        else 0,
                    },
                },
                raw_output=analysis_data,
                processing_time=processing_time,
                timestamp=datetime.now().isoformat(),
            )

        except Exception as e:
            logger.error(
                f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ Qwen –¥–ª—è {artist} - {title}: {e}"
            )
            raise RuntimeError(f"Qwen analysis failed: {e}") from e

    def _create_analysis_prompts(
        self, artist: str, title: str, lyrics: str
    ) -> tuple[str, str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è Qwen –º–æ–¥–µ–ª–∏"""
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è API
        max_lyrics_length = 2000
        if len(lyrics) > max_lyrics_length:
            lyrics = lyrics[:max_lyrics_length] + "..."

        system_prompt = """You are a rap lyrics analyzer. You MUST respond with ONLY a JSON object, no other text.

CRITICAL: Do not include ANY explanations, thoughts, or text outside the JSON. 
NO <think> tags, NO explanations, NO additional text.
Start your response with { and end with }.

Analyze rap songs and return JSON with this structure only."""

        user_prompt = f"""Artist: {artist}
Title: {title}
Lyrics: {lyrics}

Return ONLY this JSON structure (fill with actual analysis):
{{
    "genre_analysis": {{
        "primary_genre": "rap",
        "subgenre": "string",
        "confidence": 0.9
    }},
    "mood_analysis": {{
        "primary_mood": "confident",
        "emotional_intensity": "high",
        "energy_level": "high", 
        "valence": "positive"
    }},
    "content_analysis": {{
        "explicit_content": false,
        "explicit_level": "none",
        "main_themes": ["success"],
        "narrative_style": "boastful"
    }},
    "technical_analysis": {{
        "rhyme_scheme": "complex",
        "flow_pattern": "varied",
        "complexity_level": "advanced",
        "wordplay_quality": "excellent",
        "metaphor_usage": "moderate",
        "structure": "traditional"
    }},
    "quality_metrics": {{
        "lyrical_creativity": 0.8,
        "technical_skill": 0.9,
        "authenticity": 0.9,
        "commercial_appeal": 0.8,
        "originality": 0.8,
        "overall_quality": 0.8,
        "ai_generated_likelihood": 0.1
    }},
    "cultural_context": {{
        "era_estimate": "2020s",
        "regional_style": "mainstream",
        "cultural_references": [],
        "social_commentary": false
    }}
}}"""

        return system_prompt, user_prompt

    def _parse_response(self, response_text: str) -> dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –æ—Ç Qwen –º–æ–¥–µ–ª–∏"""
        try:
            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            response_text = response_text.strip()

            # –£–¥–∞–ª—è–µ–º —Ç–µ–≥–∏ <think>...</think>
            import re

            response_text = re.sub(
                r"<think>.*?</think>", "", response_text, flags=re.DOTALL
            )
            response_text = response_text.strip()

            # –õ–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            logger.debug(f"Cleaned response (first 500 chars): {response_text[:500]}")

            # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —É–∂–µ JSON, –ø–∞—Ä—Å–∏–º –Ω–∞–ø—Ä—è–º—É—é
            if response_text.startswith("{") and response_text.endswith("}"):
                try:
                    analysis_data = json.loads(response_text)
                    logger.debug("‚úÖ Direct JSON parsing successful")
                except json.JSONDecodeError as e:
                    logger.warning(f"Direct JSON parsing failed: {e}")
                    # –ü–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∏ –ø–∞—Ä—Å–∏—Ç—å —Å–Ω–æ–≤–∞
                    fixed_json = self._fix_common_json_issues(response_text)
                    try:
                        analysis_data = json.loads(fixed_json)
                        logger.debug("‚úÖ JSON parsing successful after fixes")
                    except json.JSONDecodeError:
                        logger.error("JSON still invalid after fixes")
                        analysis_data = self._create_fallback_analysis()
            else:
                # –ü–æ–∏—Å–∫ JSON –±–ª–æ–∫–∞ –≤ —Ç–µ–∫—Å—Ç–µ
                json_start = response_text.find("{")
                json_end = response_text.rfind("}") + 1

                if json_start == -1 or json_end == 0:
                    logger.error(
                        f"No JSON found in response. Full response: {response_text}"
                    )
                    # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç
                    analysis_data = self._create_fallback_analysis()
                else:
                    json_str = response_text[json_start:json_end]
                    try:
                        analysis_data = json.loads(json_str)
                    except json.JSONDecodeError:
                        logger.error(f"Invalid JSON in response: {json_str}")
                        analysis_data = self._create_fallback_analysis()

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            self._validate_analysis_structure(analysis_data)

            return analysis_data

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            logger.error(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response_text[:500]}...")
            return self._create_fallback_analysis()

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return self._create_fallback_analysis()

    def _fix_common_json_issues(self, json_str: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å JSON"""
        import re

        # –£–¥–∞–ª—è–µ–º trailing commas
        json_str = re.sub(r",(\s*[}\]])", r"\1", json_str)

        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –Ω–∞ –¥–≤–æ–π–Ω—ã–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        json_str = re.sub(r"'([^']*)':", r'"\1":', json_str)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ –∫–æ–Ω—Ü–µ
        json_str = json_str.strip()

        return json_str

    def _validate_analysis_structure(self, data: dict[str, Any]) -> None:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        required_sections = [
            "genre_analysis",
            "mood_analysis",
            "content_analysis",
            "technical_analysis",
            "quality_metrics",
            "cultural_context",
        ]

        for section in required_sections:
            if section not in data:
                logger.warning(f"Missing section: {section}")
                data[section] = {}

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        quality_metrics = data.get("quality_metrics", {})
        for metric in [
            "lyrical_creativity",
            "technical_skill",
            "authenticity",
            "commercial_appeal",
            "originality",
            "overall_quality",
        ]:
            if metric in quality_metrics:
                value = quality_metrics[metric]
                if not isinstance(value, (int, float)) or not 0 <= value <= 1:
                    logger.warning(f"Invalid metric value for {metric}: {value}")

    def _calculate_confidence(self, analysis_data: dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∞–Ω–∞–ª–∏–∑–∞"""
        confidence_factors = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        sections_completed = 0
        total_sections = 6

        for section_name in [
            "genre_analysis",
            "mood_analysis",
            "content_analysis",
            "technical_analysis",
            "quality_metrics",
            "cultural_context",
        ]:
            if analysis_data.get(section_name):
                sections_completed += 1

        completeness_score = sections_completed / total_sections
        confidence_factors.append(completeness_score)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∂–∞–Ω—Ä–µ
        genre_analysis = analysis_data.get("genre_analysis", {})
        if "confidence" in genre_analysis:
            genre_confidence = genre_analysis["confidence"]
            if (
                isinstance(genre_confidence, (int, float))
                and 0 <= genre_confidence <= 1
            ):
                confidence_factors.append(genre_confidence)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        quality_metrics = analysis_data.get("quality_metrics", {})
        if quality_metrics:
            # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
            valid_metrics = []
            for metric_value in quality_metrics.values():
                if isinstance(metric_value, (int, float)) and 0 <= metric_value <= 1:
                    valid_metrics.append(metric_value)

            if valid_metrics:
                avg_quality = sum(valid_metrics) / len(valid_metrics)
                confidence_factors.append(avg_quality)

        # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if confidence_factors:
            return sum(confidence_factors) / len(confidence_factors)
        return 0.5  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–∞–Ω–Ω—ã—Ö

    def _create_fallback_analysis(self) -> dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞"""
        return {
            "genre_analysis": {
                "primary_genre": "rap",
                "subgenre": "unknown",
                "confidence": 0.3,
            },
            "mood_analysis": {
                "primary_mood": "neutral",
                "emotional_intensity": "unknown",
                "energy_level": "unknown",
                "valence": "neutral",
            },
            "content_analysis": {
                "explicit_content": False,
                "explicit_level": "unknown",
                "main_themes": ["general"],
                "narrative_style": "unknown",
            },
            "technical_analysis": {
                "rhyme_scheme": "unknown",
                "flow_pattern": "unknown",
                "complexity_level": "unknown",
                "wordplay_quality": "unknown",
                "metaphor_usage": "unknown",
                "structure": "unknown",
            },
            "quality_metrics": {
                "lyrical_creativity": 0.3,
                "technical_skill": 0.3,
                "authenticity": 0.3,
                "commercial_appeal": 0.3,
                "originality": 0.3,
                "overall_quality": 0.3,
                "ai_generated_likelihood": 0.5,
            },
            "cultural_context": {
                "era_estimate": "unknown",
                "regional_style": "unknown",
                "cultural_references": [],
                "social_commentary": False,
            },
        }


# ============================================================================
# –ú–ê–°–°–û–í–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† (–∏–∑ src/analyzers/mass_qwen_analysis.py)
# ============================================================================


@dataclass
class AnalysisStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"""

    total_records: int = 0
    processed: int = 0
    errors: int = 0
    skipped: int = 0
    start_time: datetime | None = None
    current_batch: int = 0
    total_batches: int = 0

    @property
    def success_rate(self) -> float:
        """–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏"""
        return (self.processed / max(self.total_records, 1)) * 100

    @property
    def processing_rate(self) -> float:
        """–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∑–∞–ø–∏—Å–µ–π/–º–∏–Ω—É—Ç—É)"""
        if not self.start_time:
            return 0.0
        elapsed = (datetime.now() - self.start_time).total_seconds()
        return (self.processed / max(elapsed, 1)) * 60

    @property
    def estimated_remaining(self) -> timedelta:
        """–û—Ü–µ–Ω–∫–∞ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏"""
        remaining = self.total_records - self.processed
        if remaining <= 0 or self.processing_rate == 0:
            return timedelta(0)
        minutes = remaining / self.processing_rate
        return timedelta(minutes=minutes)


class UnifiedQwenMassAnalyzer:
    """–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º Qwen"""

    def __init__(self):
        self.app = None
        self.analyzer = None
        self.db_manager = None
        self.stats = AnalysisStats()
        self.last_processed_id = 0
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint –≤ –ø–∞–ø–∫—É results
        self.checkpoint_file = Path("results") / "qwen_analysis_checkpoint.json"

    async def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        try:
            print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
            self.app = create_app()

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            self.analyzer = EmbeddedQwenAnalyzer()
            if not self.analyzer or not self.analyzer.available:
                print("‚ùå –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Qwen –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
                print("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ NOVITA_API_KEY –≤ .env —Ñ–∞–π–ª–µ")
                return False

            print(f"‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Qwen –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤: {self.analyzer.model_name}")

            # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL
            self.db_manager = PostgreSQLManager()
            await self.db_manager.initialize()

            print("‚úÖ PostgreSQL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            return False

    async def get_database_stats(self) -> dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            async with self.db_manager.connection_pool.acquire() as conn:
                stats_query = """
                SELECT 
                    COUNT(*) as total_tracks,
                    COUNT(CASE WHEN lyrics IS NOT NULL AND lyrics != '' THEN 1 END) as tracks_with_lyrics,
                    COUNT(DISTINCT ar.track_id) as qwen_analyzed,
                    COUNT(CASE WHEN ar.track_id IS NULL THEN 1 END) as unanalyzed
                FROM tracks t
                LEFT JOIN analysis_results ar ON t.id = ar.track_id 
                    AND ar.analyzer_type LIKE '%qwen%'
                WHERE t.lyrics IS NOT NULL AND t.lyrics != ''
                """

                result = await conn.fetchrow(stats_query)
                return dict(result) if result else {}

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}

    async def load_checkpoint(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã"""
        try:
            if not self.checkpoint_file.exists():
                return False

            with open(self.checkpoint_file, encoding="utf-8") as f:
                data = json.load(f)

            self.last_processed_id = data.get("last_processed_id", 0)
            print(
                f"üìç –ó–∞–≥—Ä—É–∂–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: –ø–æ—Å–ª–µ–¥–Ω—è—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å ID {self.last_processed_id}"
            )
            return True

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
            return False

    async def save_checkpoint(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É results –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            self.checkpoint_file.parent.mkdir(exist_ok=True)

            data = {
                "last_processed_id": self.last_processed_id,
                "timestamp": datetime.now().isoformat(),
                "processed": self.stats.processed,
                "errors": self.stats.errors,
            }

            with open(self.checkpoint_file, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")

    async def get_unanalyzed_records(
        self, limit: int | None = None, resume: bool = False
    ) -> list[dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
        try:
            async with self.db_manager.connection_pool.acquire() as conn:
                # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                query = """
                SELECT t.id, t.artist, t.title, t.lyrics 
                FROM tracks t
                WHERE t.lyrics IS NOT NULL 
                AND t.lyrics != '' 
                AND t.id NOT IN (
                    SELECT DISTINCT track_id 
                    FROM analysis_results 
                    WHERE analyzer_type LIKE '%qwen%'
                )
                """

                # –£—Å–ª–æ–≤–∏–µ –¥–ª—è resume —Ä–µ–∂–∏–º–∞
                if resume and self.last_processed_id > 0:
                    query += f" AND t.id > {self.last_processed_id}"

                query += " ORDER BY t.id"

                # –õ–∏–º–∏—Ç
                if limit:
                    query += f" LIMIT {limit}"

                records = await conn.fetch(query)
                return [dict(record) for record in records]

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–ø–∏—Å–µ–π: {e}")
            return []

    async def analyze_single_record(self, record: dict[str, Any]) -> bool:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏"""
        track_id = record["id"]
        artist = record["artist"]
        title = record["title"]
        lyrics = record["lyrics"]

        try:
            # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä)
            result = self.analyzer.analyze_song(artist, title, lyrics)

            if result is None:
                return False

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PostgreSQL
            analysis_data = {
                "track_id": track_id,
                "analyzer_type": "qwen-3-4b-fp8",
                "sentiment": self._extract_sentiment(result),
                "confidence": result.confidence or 0.5,
                "complexity_score": self._extract_complexity(result),
                "themes": self._extract_themes(result),
                "analysis_data": result.raw_output or {},
                "processing_time_ms": int((result.processing_time or 0) * 1000),
                "model_version": result.metadata.get("model_name", "qwen-3-4b-fp8"),
            }

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É
            success = await self._save_analysis_to_database(analysis_data)

            if success:
                self.last_processed_id = track_id
                return True
            return False

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø–∏—Å–∏ {track_id}: {e}")
            return False

    async def _save_analysis_to_database(self, analysis_data: dict[str, Any]) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            async with self.db_manager.connection_pool.acquire() as conn:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –∞–Ω–∞–ª–∏–∑ –¥–ª—è —ç—Ç–æ–≥–æ —Ç—Ä–µ–∫–∞
                existing = await conn.fetchrow(
                    "SELECT id FROM analysis_results WHERE track_id = $1 AND analyzer_type = $2",
                    analysis_data["track_id"],
                    analysis_data["analyzer_type"],
                )

                if existing:
                    print(
                        f"  ‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–ª—è —Ç—Ä–µ–∫–∞ {analysis_data['track_id']}"
                    )
                    return True

                # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
                insert_query = """
                INSERT INTO analysis_results 
                (track_id, analyzer_type, analysis_data, confidence, sentiment, complexity_score, themes, processing_time_ms, model_version, created_at)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW())
                RETURNING id
                """

                result = await conn.fetchrow(
                    insert_query,
                    analysis_data["track_id"],
                    analysis_data["analyzer_type"],
                    json.dumps(analysis_data["analysis_data"]),
                    analysis_data["confidence"],
                    analysis_data["sentiment"],
                    analysis_data["complexity_score"],
                    json.dumps(analysis_data["themes"]),
                    analysis_data["processing_time_ms"],
                    analysis_data["model_version"],
                )

                return result is not None

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return False

    def _extract_sentiment(self, result) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            if hasattr(result, "raw_output") and result.raw_output:
                mood_analysis = result.raw_output.get("mood_analysis", {})
                return mood_analysis.get("primary_mood", "neutral")
            return "neutral"
        except:
            return "neutral"

    def _extract_complexity(self, result) -> float:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        try:
            if hasattr(result, "raw_output") and result.raw_output:
                quality_metrics = result.raw_output.get("quality_metrics", {})
                return float(quality_metrics.get("overall_quality", 0.5)) * 5.0
            return 3.0
        except:
            return 3.0

    def _extract_themes(self, result) -> list[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            if hasattr(result, "raw_output") and result.raw_output:
                content_analysis = result.raw_output.get("content_analysis", {})
                return content_analysis.get("main_themes", ["general"])
            return ["general"]
        except:
            return ["general"]

    async def process_batch(self, batch: list[dict[str, Any]]) -> tuple[int, int]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –∑–∞–ø–∏—Å–µ–π"""
        processed = 0
        errors = 0

        for i, record in enumerate(batch, 1):
            track_id = record["id"]
            artist = record.get("artist", "Unknown")
            title = record.get("title", "Unknown")

            # –ü—Ä–æ–≥—Ä–µ—Å—Å –≤–Ω—É—Ç—Ä–∏ –±–∞—Ç—á–∞
            print(f"  üéµ [{i}/{len(batch)}] {artist} - {title} (ID: {track_id})")

            try:
                if await self.analyze_single_record(record):
                    processed += 1
                    print("    ‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
                else:
                    errors += 1
                    print("    ‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞")

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è rate limiting
                await asyncio.sleep(0.5)

            except Exception as e:
                errors += 1
                print(f"    ‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")

        return processed, errors

    def print_progress(self):
        """–í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        print("\nüìä –ü–†–û–ì–†–ï–°–°:")
        print(f"  üìà –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats.processed}/{self.stats.total_records}")
        print(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {self.stats.success_rate:.1f}%")
        print(f"  ‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {self.stats.processing_rate:.1f} –∑–∞–ø–∏—Å–µ–π/–º–∏–Ω")
        print(f"  ‚è±Ô∏è  –û—Å—Ç–∞–ª–æ—Å—å: {self.stats.estimated_remaining}")
        print(f"  üì¶ –ë–∞—Ç—á: {self.stats.current_batch}/{self.stats.total_batches}")

    async def run_analysis(
        self,
        batch_size: int = 100,
        max_records: int | None = None,
        resume: bool = False,
        test_mode: bool = False,
    ) -> dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –º–∞—Å—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""

        print("üéµ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π Qwen –º–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (PostgreSQL v3.0)")
        print("=" * 70)

        # –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if resume:
            await self.load_checkpoint()

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã
        db_stats = await self.get_database_stats()
        print("üìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"  üìÅ –í—Å–µ–≥–æ —Ç—Ä–µ–∫–æ–≤: {db_stats.get('total_tracks', 0)}")
        print(f"  üìù –° —Ç–µ–∫—Å—Ç–∞–º–∏: {db_stats.get('tracks_with_lyrics', 0)}")
        print(f"  ü§ñ –£–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ Qwen: {db_stats.get('qwen_analyzed', 0)}")
        print(f"  ‚è≥ –û–∂–∏–¥–∞–µ—Ç –∞–Ω–∞–ª–∏–∑–∞: {db_stats.get('unanalyzed', 0)}")

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
        if test_mode:
            max_records = 10
            batch_size = 5
            print(f"\nüß™ –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ {max_records} –∑–∞–ø–∏—Å–µ–π")

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        print("\nüîç –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")
        records = await self.get_unanalyzed_records(limit=max_records, resume=resume)

        if not records:
            print("‚úÖ –í—Å–µ –∑–∞–ø–∏—Å–∏ —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")
            return {"status": "completed", "message": "No records to process"}

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats.total_records = len(records)
        self.stats.start_time = datetime.now()
        self.stats.total_batches = (len(records) + batch_size - 1) // batch_size

        print("\nüéØ –ü–ª–∞–Ω –∞–Ω–∞–ª–∏–∑–∞:")
        print(f"  üìä –ó–∞–ø–∏—Å–µ–π –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(records)}")
        print(f"  üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        print(f"  üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π: {self.stats.total_batches}")
        print(f"  ‚è±Ô∏è  –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: {(len(records) * 15) // 60} –º–∏–Ω—É—Ç")
        print("  üÜì –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å Qwen —á–µ—Ä–µ–∑ Novita AI - –±–µ–∑ –∑–∞—Ç—Ä–∞—Ç!")

        if not test_mode:
            print("\n‚è≥ –ù–∞—á–∏–Ω–∞–µ–º —á–µ—Ä–µ–∑ 3 —Å–µ–∫—É–Ω–¥—ã...")
            await asyncio.sleep(3)

        # –ú–∞—Å—Å–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –±–∞—Ç—á–∞–º
        print("\nüöÄ –ù–ê–ß–ò–ù–ê–ï–ú –ú–ê–°–°–û–í–´–ô –ê–ù–ê–õ–ò–ó")
        print("=" * 50)

        for i in range(0, len(records), batch_size):
            self.stats.current_batch += 1
            batch = records[i : i + batch_size]
            batch_start = time.time()

            print(f"\nüì¶ –ë–∞—Ç—á {self.stats.current_batch}/{self.stats.total_batches}")
            print(
                f"üìä –ó–∞–ø–∏—Å–∏ {i + 1}-{min(i + batch_size, len(records))} –∏–∑ {len(records)}"
            )

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
            batch_processed, batch_errors = await self.process_batch(batch)

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.stats.processed += batch_processed
            self.stats.errors += batch_errors

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            await self.save_checkpoint()

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á–∞
            batch_time = time.time() - batch_start
            print(f"  ‚è±Ô∏è  –ë–∞—Ç—á –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {batch_time:.1f}—Å")
            print(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ: {batch_processed}")
            print(f"  ‚ùå –û—à–∏–±–æ–∫: {batch_errors}")

            # –û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            self.print_progress()

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ)
            if i + batch_size < len(records):
                print("  ‚è∏Ô∏è  –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏...")
                await asyncio.sleep(2)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_time = (datetime.now() - self.stats.start_time).total_seconds()

        print("\nüèÜ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
        print("=" * 50)
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {self.stats.processed}")
        print(f"‚ùå –û—à–∏–±–æ–∫: {self.stats.errors}")
        print(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {self.stats.total_records}")
        print(f"üéØ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {self.stats.success_rate:.1f}%")
        print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time // 60:.0f}–º {total_time % 60:.0f}—Å")
        print(f"‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {self.stats.processing_rate:.1f} –∑–∞–ø–∏—Å–µ–π/–º–∏–Ω")

        # –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã
        final_db_stats = await self.get_database_stats()
        print("\nüìà –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã:")
        print(f"  ü§ñ Qwen –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {final_db_stats.get('qwen_analyzed', 0)}")
        print(f"  ‚è≥ –û—Å—Ç–∞–ª–æ—Å—å: {final_db_stats.get('unanalyzed', 0)}")

        # –£–¥–∞–ª–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
        if self.stats.errors == 0 and self.checkpoint_file.exists():
            self.checkpoint_file.unlink()
            print("üóëÔ∏è  –ß–µ–∫–ø–æ–∏–Ω—Ç —É–¥–∞–ª–µ–Ω (–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –±–µ–∑ –æ—à–∏–±–æ–∫)")

        return {
            "status": "completed",
            "processed": self.stats.processed,
            "errors": self.stats.errors,
            "success_rate": self.stats.success_rate,
            "total_time": total_time,
            "processing_rate": self.stats.processing_rate,
        }

    async def show_stats_only(self) -> dict[str, Any]:
        """–ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞"""
        print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ Qwen –∞–Ω–∞–ª–∏–∑–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
        print("=" * 60)

        db_stats = await self.get_database_stats()

        print(f"üìÅ –í—Å–µ–≥–æ —Ç—Ä–µ–∫–æ–≤: {db_stats.get('total_tracks', 0)}")
        print(f"üìù –° —Ç–µ–∫—Å—Ç–∞–º–∏: {db_stats.get('tracks_with_lyrics', 0)}")
        print(f"ü§ñ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ Qwen: {db_stats.get('qwen_analyzed', 0)}")
        print(f"‚è≥ –û–∂–∏–¥–∞–µ—Ç –∞–Ω–∞–ª–∏–∑–∞: {db_stats.get('unanalyzed', 0)}")

        if db_stats.get("tracks_with_lyrics", 0) > 0:
            coverage = (
                db_stats.get("qwen_analyzed", 0) / db_stats.get("tracks_with_lyrics", 1)
            ) * 100
            print(f"üìà –ü–æ–∫—Ä—ã—Ç–∏–µ –∞–Ω–∞–ª–∏–∑–æ–º: {coverage:.1f}%")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file) as f:
                    checkpoint = json.load(f)
                print("\nüìç –ù–∞–π–¥–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç:")
                print(
                    f"  üìÑ –ü–æ—Å–ª–µ–¥–Ω—è—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å: {checkpoint.get('last_processed_id', 0)}"
                )
                print(f"  üìÖ –î–∞—Ç–∞: {checkpoint.get('timestamp', 'unknown')}")
                print(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤ —Å–µ—Å—Å–∏–∏: {checkpoint.get('processed', 0)}")
                print(f"  ‚ùå –û—à–∏–±–æ–∫ –≤ —Å–µ—Å—Å–∏–∏: {checkpoint.get('errors', 0)}")
            except:
                print("\n‚ö†Ô∏è –ù–∞–π–¥–µ–Ω –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç")

        return db_stats

    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.db_manager:
            await self.db_manager.close()
        print("üßπ –†–µ—Å—É—Ä—Å—ã –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤"""
    parser = argparse.ArgumentParser(
        description="–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ Qwen (PostgreSQL v3.0)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python src/analyzers/mass_qwen_analysis.py                    # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
  python src/analyzers/mass_qwen_analysis.py --batch 50         # –ë–∞—Ç—á 50 –∑–∞–ø–∏—Å–µ–π
  python src/analyzers/mass_qwen_analysis.py --test             # –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º
  python src/analyzers/mass_qwen_analysis.py --max 1000         # –õ–∏–º–∏—Ç 1000 –∑–∞–ø–∏—Å–µ–π
  python src/analyzers/mass_qwen_analysis.py --resume           # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞
  python src/analyzers/mass_qwen_analysis.py --stats            # –¢–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        """,
    )

    parser.add_argument(
        "--batch",
        type=int,
        default=100,
        metavar="N",
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (default: 100)",
    )
    parser.add_argument(
        "--max",
        type=int,
        metavar="N",
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (—Ç–æ–ª—å–∫–æ 10 –∑–∞–ø–∏—Å–µ–π —Å –±–∞—Ç—á–µ–º 5)",
    )
    parser.add_argument(
        "--resume", action="store_true", help="–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"
    )
    parser.add_argument(
        "--stats", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"
    )

    args = parser.parse_args()

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = UnifiedQwenMassAnalyzer()

    try:
        # –†–µ–∂–∏–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if args.stats:
            if await analyzer.initialize():
                await analyzer.show_stats_only()
            return

        # –û—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞
        if not await analyzer.initialize():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—É")
            return

        # –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        result = await analyzer.run_analysis(
            batch_size=args.batch,
            max_records=args.max,
            resume=args.resume,
            test_mode=args.test,
        )

        print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['status']}")

    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        print("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --resume –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∑–∏—Ü–∏–∏")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    finally:
        await analyzer.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
