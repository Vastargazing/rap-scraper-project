#!/usr/bin/env python3
"""
üß™ Benchmark-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è pytest

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è benchmark —Ç–µ—Å—Ç–æ–≤
- –§–∏–∫—Å—Ç—É—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
- –ö–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞

–ê–í–¢–û–†: RapAnalyst AI Assistant
–î–ê–¢–ê: 19.09.2025
"""

import asyncio
import sys
from pathlib import Path

import pytest

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ Python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from core.app import Application

# === BENCHMARK –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===


@pytest.fixture(scope="session")
def benchmark_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö CI —Ç–µ—Å—Ç–æ–≤"""
    return {
        "min_rounds": 3,  # –ú–∏–Ω–∏–º—É–º —Ä–∞—É–Ω–¥–æ–≤ (–±—ã–ª–æ 5)
        "max_time": 10.0,  # –ú–∞–∫—Å–∏–º—É–º –≤—Ä–µ–º–µ–Ω–∏ (–±—ã–ª–æ 30)
        "min_time": 0.05,  # –ú–∏–Ω–∏–º—É–º –≤—Ä–µ–º–µ–Ω–∏ (–±—ã–ª–æ 0.1)
        "timer": "time.perf_counter",
        "disable_gc": True,
        "warmup": True,
        "warmup_iterations": 2,  # –ú–µ–Ω—å—à–µ –ø—Ä–æ–≥—Ä–µ–≤–æ–≤ (–±—ã–ª–æ 3)
    }


# === –§–ò–ö–°–¢–£–†–´ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê–¢–û–†–û–í ===


@pytest.fixture(scope="session")
def rap_app():
    """–§–∏–∫—Å—Ç—É—Ä–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –≤—Å–µ–π —Å–µ—Å—Å–∏–∏"""
    return Application()


@pytest.fixture(scope="session")
def quick_test_texts():
    """–ë—ã—Å—Ç—Ä—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è CI"""
    return [
        "Happy song",
        "Sad lyrics about love",
        "Energetic rap with messages",
    ]


@pytest.fixture(scope="session")
def available_analyzers(rap_app):
    """–î–æ—Å—Ç—É–ø–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã"""
    available = rap_app.list_analyzers()
    analyzers = {}

    for name in available:
        analyzer = rap_app.get_analyzer(name)
        if analyzer:
            analyzers[name] = analyzer

    return analyzers


# === –ö–ê–°–¢–û–ú–ù–´–ï –ú–ï–¢–†–ò–ö–ò ===


@pytest.fixture
def enhanced_benchmark(benchmark):
    """Benchmark —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    import psutil

    def custom_benchmark(func, *args, **kwargs):
        process = psutil.Process()

        # –°—Ç–∞—Ä—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        start_memory = process.memory_info().rss
        start_cpu_time = process.cpu_times()

        # –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞
        result = benchmark(func, *args, **kwargs)

        # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        end_memory = process.memory_info().rss
        end_cpu_time = process.cpu_times()

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        benchmark.extra_info.update(
            {
                "memory_growth_mb": (end_memory - start_memory) / 1024 / 1024,
                "cpu_time_delta": (end_cpu_time.user + end_cpu_time.system)
                - (start_cpu_time.user + start_cpu_time.system),
            }
        )

        return result

    return custom_benchmark


# === ASYNC –ü–û–î–î–ï–†–ñ–ö–ê ===


@pytest.fixture
def async_benchmark(benchmark):
    """Benchmark –¥–ª—è async —Ñ—É–Ω–∫—Ü–∏–π"""

    def run_async_benchmark(async_func, *args, **kwargs):
        def sync_wrapper():
            return asyncio.run(async_func(*args, **kwargs))

        return benchmark(sync_wrapper)

    return run_async_benchmark


# === PYTEST HOOKS ===


def pytest_benchmark_update_machine_info(config, machine_info):
    """–î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∞—à–∏–Ω–µ"""
    import platform

    import psutil

    machine_info.update(
        {
            "cpu_count": psutil.cpu_count(),
            "memory_gb": round(psutil.virtual_memory().total / 1024**3, 1),
            "python_version": platform.python_version(),
            "platform": platform.platform()[:50],  # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω–æ–µ –∏–º—è
            "rap_project": "rap-scraper-project",
        }
    )


def pytest_benchmark_group_stats(config, benchmarks, group_by):
    """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º"""

    # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ –≤ extra_info
    analyzer_groups = {}
    for benchmark in benchmarks:
        analyzer = benchmark.extra_info.get("analyzer", "unknown")
        if analyzer not in analyzer_groups:
            analyzer_groups[analyzer] = []
        analyzer_groups[analyzer].append(benchmark)

    return analyzer_groups if analyzer_groups else group_by


# === –ú–ê–†–ö–ï–†–´ ===


def pytest_configure(config):
    """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–∞—Ä–∫–µ—Ä–æ–≤"""
    config.addinivalue_line("markers", "benchmark: mark test as benchmark test")
    config.addinivalue_line("markers", "quick: mark test as quick benchmark for CI")
    config.addinivalue_line("markers", "comparison: mark test as analyzer comparison")
