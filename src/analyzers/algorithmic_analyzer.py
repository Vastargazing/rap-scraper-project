"""
üßÆ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤ –ø–µ—Å–µ–Ω

–ö–õ–Æ–ß–ï–í–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø:
‚ú® –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º (–≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–∫–æ–Ω—á–∞–Ω–∏–π)
üéØ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ flow –∏ —Ä–∏—Ç–º–∞
üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Å–ª–æ–≤–∞—Ä—è–º–∏
üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ (Flesch, SMOG, ARI)
üéµ –ê–Ω–∞–ª–∏–∑ –º—É–∑—ã–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
üîÑ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
‚ö° –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
üìà –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- üéØ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI –º–æ–¥–µ–ª–µ–π
- ‚ö° –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö (57K+ —Ç—Ä–µ–∫–æ–≤)
- üìä Baseline –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏
- üóÑÔ∏è Production-ready –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Å PostgreSQL –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
- üìà –î–µ—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
üñ•Ô∏è CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:
  python src/analyzers/algorithmic_analyzer.py --stats
  python src/analyzers/algorithmic_analyzer.py --analyze-all --limit 100
  python src/analyzers/algorithmic_analyzer.py --analyze-track 123

üìù –ü—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:
  analyzer = AdvancedAlgorithmicAnalyzer()
  result = analyzer.analyze_song("Artist", "Title", "Lyrics...")

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- üêç Python 3.8+
- üóÑÔ∏è PostgreSQL —Å asyncpg
- üìÑ PyYAML –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
- üîß src/interfaces/analyzer_interface.py

–†–ï–ó–£–õ–¨–¢–ê–¢:
- üéµ –ê–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º: —Å—Ö–µ–º–∞, –ø–ª–æ—Ç–Ω–æ—Å—Ç—å, —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
- üåä Flow –º–µ—Ç—Ä–∏–∫–∏: –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≥–æ–≤, —Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
- üìö –ß–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å: Flesch, SMOG, ARI, Coleman-Liau –∏–Ω–¥–µ–∫—Å—ã
- üí≠ –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å, –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å, —Å–ª–æ–∂–Ω–æ—Å—Ç—å
- üé® –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: –¥–µ–Ω—å–≥–∏, —É–ª–∏—Ü–∞, —É—Å–ø–µ—Ö, –æ—Ç–Ω–æ—à–µ–Ω–∏—è
- ‚úçÔ∏è –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–µ–º—ã: –º–µ—Ç–∞—Ñ–æ—Ä—ã, –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏—è, –ø–æ–≤—Ç–æ—Ä—ã
- üìä –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ, –∞—Ä—Ç–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å

–í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
- üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö PostgreSQL
- üîç –ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤ –ø–æ ID
- üöÄ –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –∏ –±–∞—Ç—á–∏–Ω–≥–æ–º
- üíæ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- üé≠ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º —Å –ø—Ä–∏–º–µ—Ä–æ–º

–ê–í–¢–û–†: Rap Scraper Project Team
–í–ï–†–°–ò–Ø: 2.0.0 Advanced
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""

import re
import time
import math
import json
import logging
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Set, Tuple, Optional
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from pathlib import Path
import hashlib

# –ò–º–ø–æ—Ä—Ç—ã —Å fallback –¥–ª—è standalone –∑–∞–ø—É—Å–∫–∞
try:
    from interfaces.analyzer_interface import BaseAnalyzer, AnalysisResult, register_analyzer
    PROJECT_IMPORT_SUCCESS = True
except ImportError:
    PROJECT_IMPORT_SUCCESS = False
    import sys
    from pathlib import Path
    
    # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –∏ –¥–æ–±–∞–≤–∏—Ç—å src –≤ –ø—É—Ç—å
    current_dir = Path(__file__).resolve().parent
    possible_roots = [current_dir.parent.parent, current_dir.parent, current_dir]
    
    for root in possible_roots:
        src_path = root / 'src'
        if src_path.exists() and (src_path / 'interfaces').exists():
            if str(src_path) not in sys.path:
                sys.path.append(str(src_path))
            try:
                from interfaces.analyzer_interface import BaseAnalyzer, AnalysisResult, register_analyzer
                PROJECT_IMPORT_SUCCESS = True
                break
            except ImportError:
                continue
    
    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏
    if not PROJECT_IMPORT_SUCCESS:
        # –ë–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è standalone —Ä–∞–±–æ—Ç—ã
        class BaseAnalyzer:
            def __init__(self, config: Dict[str, Any] = None):
                self.config = config or {}
            
            def validate_input(self, artist: str, title: str, lyrics: str) -> bool:
                return bool(artist and title and lyrics and len(lyrics.strip()) > 10)
            
            def preprocess_lyrics(self, lyrics: str) -> str:
                return re.sub(r'\s+', ' ', lyrics.strip())
        
        @dataclass
        class AnalysisResult:
            artist: str
            title: str
            analysis_type: str
            confidence: float
            metadata: Dict[str, Any]
            raw_output: Dict[str, Any]
            processing_time: float
            timestamp: str
        
        def register_analyzer(name: str):
            def decorator(cls):
                return cls
            return decorator

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PhoneticPattern:
    """–§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Ñ–º"""
    vowel_sounds: Dict[str, List[str]] = field(default_factory=lambda: {
        'ay': ['ai', 'ay', 'ey', 'a_e'],
        'ee': ['ee', 'ea', 'ie', 'y'],
        'oh': ['o', 'oa', 'ow', 'o_e'],
        'oo': ['oo', 'u', 'ew', 'ue'],
        'ah': ['a', 'au', 'aw'],
        'ih': ['i', 'y', 'ie'],
        'eh': ['e', 'ea', 'ai'],
        'uh': ['u', 'o', 'ou']
    })
    
    consonant_clusters: Set[str] = field(default_factory=lambda: {
        'ch', 'sh', 'th', 'wh', 'ck', 'ng', 'ph', 'gh', 
        'st', 'sp', 'sc', 'sk', 'sm', 'sn', 'sw', 'sl',
        'bl', 'br', 'cl', 'cr', 'dr', 'fl', 'fr', 'gl',
        'gr', 'pl', 'pr', 'tr', 'tw'
    })

class AdvancedLexicon:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    def __init__(self):
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≥—Ä–∞–¥–∞—Ü–∏–µ–π –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
        self.emotions = {
            'joy': {
                'weak': {'happy', 'good', 'nice', 'okay', 'fine', 'content'},
                'medium': {'great', 'wonderful', 'amazing', 'excellent', 'fantastic'},
                'strong': {'ecstatic', 'euphoric', 'blissful', 'overjoyed', 'triumphant'}
            },
            'anger': {
                'weak': {'annoyed', 'bothered', 'upset', 'frustrated', 'irritated'},
                'medium': {'angry', 'mad', 'furious', 'pissed', 'heated'},
                'strong': {'rage', 'wrath', 'livid', 'enraged', 'incensed'}
            },
            'sadness': {
                'weak': {'sad', 'down', 'blue', 'low', 'melancholy'},
                'medium': {'depressed', 'miserable', 'heartbroken', 'devastated'},
                'strong': {'suicidal', 'hopeless', 'despairing', 'anguished'}
            },
            'fear': {
                'weak': {'worried', 'nervous', 'anxious', 'concerned', 'uneasy'},
                'medium': {'scared', 'afraid', 'frightened', 'terrified'},
                'strong': {'petrified', 'horrified', 'panic', 'dread'}
            }
        }
        
        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ä—ç–ø–∞
        self.rap_themes = {
            'money_wealth': {
                'money', 'cash', 'bread', 'dough', 'paper', 'green', 'bills',
                'rich', 'wealth', 'fortune', 'bank', 'account', 'invest',
                'luxury', 'expensive', 'diamond', 'gold', 'platinum',
                'mansion', 'penthouse', 'yacht', 'lambo', 'ferrari', 'bentley'
            },
            'street_life': {
                'street', 'hood', 'block', 'corner', 'ghetto', 'projects',
                'hustle', 'grind', 'struggle', 'survive', 'real', 'raw',
                'concrete', 'pavement', 'alley', 'corner', 'neighborhood'
            },
            'success_ambition': {
                'success', 'win', 'victory', 'champion', 'boss', 'king',
                'queen', 'crown', 'throne', 'empire', 'legend', 'icon',
                'achieve', 'accomplish', 'conquer', 'dominate', 'rise'
            },
            'relationships_love': {
                'love', 'heart', 'baby', 'girl', 'woman', 'man', 'relationship',
                'kiss', 'hug', 'romance', 'passion', 'soul', 'forever',
                'together', 'commitment', 'loyalty', 'trust'
            },
            'violence_conflict': {
                'gun', 'shoot', 'kill', 'murder', 'death', 'blood', 'war',
                'fight', 'battle', 'enemy', 'revenge', 'violence', 'attack',
                'weapon', 'bullet', 'trigger', 'blast', 'destroy'
            },
            'drugs_party': {
                'weed', 'smoke', 'high', 'drunk', 'party', 'club', 'dance',
                'drink', 'bottle', 'shot', 'pill', 'lean', 'molly',
                'celebration', 'turn_up', 'lit', 'wild', 'crazy'
            }
        }
        
        # –°–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º—ã—à–ª–µ–Ω–∏—è
        self.complexity_indicators = {
            'philosophical': {
                'existence', 'reality', 'consciousness', 'purpose', 'meaning',
                'truth', 'wisdom', 'knowledge', 'understand', 'perspective',
                'philosophy', 'metaphysical', 'spiritual', 'transcend'
            },
            'abstract': {
                'concept', 'theory', 'principle', 'ideology', 'paradigm',
                'dimension', 'universe', 'infinity', 'eternal', 'essence',
                'phenomenon', 'manifestation', 'transformation', 'evolution'
            },
            'analytical': {
                'analyze', 'examine', 'investigate', 'evaluate', 'assess',
                'consider', 'contemplate', 'reflect', 'introspect', 'ponder',
                'calculate', 'measure', 'compare', 'contrast', 'deduce'
            }
        }
        
        # –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–µ–º—ã –≤ —Ç–µ–∫—Å—Ç–∞—Ö
        self.literary_devices = {
            'metaphor_indicators': {
                'like', 'as', 'than', 'seems', 'appears', 'resembles',
                'similar', 'reminds', 'symbolize', 'represent', 'embody'
            },
            'time_references': {
                'yesterday', 'today', 'tomorrow', 'past', 'present', 'future',
                'memory', 'remember', 'forget', 'history', 'destiny', 'fate'
            },
            'contrast_words': {
                'but', 'however', 'although', 'despite', 'nevertheless',
                'nonetheless', 'whereas', 'while', 'opposite', 'contrast'
            }
        }

class FlowAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä flow –∏ —Ä–∏—Ç–º–∞"""
    
    def __init__(self):
        self.phonetic_patterns = PhoneticPattern()
        
    def analyze_flow_patterns(self, lines: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ flow"""
        if not lines:
            return self._empty_flow_result()
        
        syllable_patterns = []
        stress_patterns = []
        line_lengths = []
        
        for line in lines:
            syllables = self._count_syllables_advanced(line)
            syllable_patterns.append(syllables)
            line_lengths.append(len(line.split()))
            
            # –ê–Ω–∞–ª–∏–∑ —É–¥–∞—Ä–µ–Ω–∏–π (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            stress_pattern = self._analyze_stress_pattern(line)
            stress_patterns.append(stress_pattern)
        
        return {
            'syllable_consistency': self._calculate_consistency(syllable_patterns),
            'average_syllables_per_line': sum(syllable_patterns) / len(syllable_patterns),
            'syllable_variance': self._calculate_variance(syllable_patterns),
            'line_length_consistency': self._calculate_consistency(line_lengths),
            'stress_pattern_regularity': self._analyze_stress_regularity(stress_patterns),
            'flow_breaks': self._count_flow_interruptions(lines),
            'rhythmic_density': self._calculate_rhythmic_density(lines)
        }
    
    def _count_syllables_advanced(self, text: str) -> int:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≥–æ–≤"""
        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        if not words:
            return 0
        
        total_syllables = 0
        for word in words:
            syllables = self._syllables_in_word(word)
            total_syllables += syllables
        
        return total_syllables
    
    def _syllables_in_word(self, word: str) -> int:
        """–¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≥–æ–≤ –≤ —Å–ª–æ–≤–µ"""
        if len(word) <= 2:
            return 1
        
        word = word.lower().strip()
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏
        special_cases = {
            'the': 1, 'a': 1, 'an': 1, 'and': 1, 'or': 1, 'but': 1,
            'through': 1, 'though': 1, 'every': 2, 'very': 2,
            'people': 2, 'little': 2, 'middle': 2, 'simple': 2
        }
        
        if word in special_cases:
            return special_cases[word]
        
        # –ü–æ–¥—Å—á–µ—Ç –≥–ª–∞—Å–Ω—ã—Ö –≥—Ä—É–ø–ø
        vowels = 'aeiouy'
        syllable_count = 0
        prev_was_vowel = False
        
        for i, char in enumerate(word):
            is_vowel = char in vowels
            
            if is_vowel:
                # –ù–æ–≤–∞—è –≥–ª–∞—Å–Ω–∞—è –≥—Ä—É–ø–ø–∞
                if not prev_was_vowel:
                    syllable_count += 1
                # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è –¥–∏—Ñ—Ç–æ–Ω–≥–æ–≤
                elif i > 0 and word[i-1:i+1] in ['ai', 'au', 'ea', 'ee', 'ei', 'ie', 'oa', 'oo', 'ou', 'ue']:
                    pass  # –ù–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
            
            prev_was_vowel = is_vowel
        
        # –ò—Å–∫–ª—é—á–µ–Ω–∏—è
        if word.endswith('e') and syllable_count > 1:
            # –£–±–∏—Ä–∞–µ–º silent e
            if not word.endswith(('le', 'se', 'me', 'ne', 've', 'ze', 'de', 'ge')):
                syllable_count -= 1
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        if word.endswith(('ed', 'es', 'er', 'ly')):
            pass  # –£–∂–µ —É—á—Ç–µ–Ω–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∞–ª–≥–æ—Ä–∏—Ç–º–µ
        
        return max(1, syllable_count)
    
    def _analyze_stress_pattern(self, line: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —É–¥–∞—Ä–µ–Ω–∏–π (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)"""
        words = line.split()
        if not words:
            return ""
        
        stress_pattern = []
        for word in words:
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è —É–¥–∞—Ä–µ–Ω–∏–π
            if len(word) <= 3:
                stress_pattern.append('1')  # –£–¥–∞—Ä–Ω—ã–π
            elif word.lower() in {'the', 'and', 'but', 'for', 'with', 'from', 'into'}:
                stress_pattern.append('0')  # –ë–µ–∑—É–¥–∞—Ä–Ω—ã–π
            else:
                # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤ —Å—Ç–∞–≤–∏–º —É–¥–∞—Ä–µ–Ω–∏–µ –Ω–∞ –ø–µ—Ä–≤—ã–π —Å–ª–æ–≥
                stress_pattern.append('1')
        
        return ''.join(stress_pattern)
    
    def _calculate_consistency(self, values: List[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–π"""
        if len(values) < 2:
            return 1.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º: —á–µ–º –º–µ–Ω—å—à–µ –≤–∞—Ä–∏–∞—Ü–∏—è, —Ç–µ–º –≤—ã—à–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
        consistency = 1 / (1 + variance)
        return min(consistency, 1.0)
    
    def _calculate_variance(self, values: List[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏"""
        if len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        return sum((x - mean) ** 2 for x in values) / len(values)
    
    def _analyze_stress_regularity(self, stress_patterns: List[str]) -> float:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —É–¥–∞—Ä–µ–Ω–∏–π"""
        if not stress_patterns:
            return 0.0
        
        # –ò—â–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        pattern_counts = Counter(stress_patterns)
        most_common = pattern_counts.most_common(1)
        
        if most_common:
            regularity = most_common[0][1] / len(stress_patterns)
            return regularity
        
        return 0.0
    
    def _count_flow_interruptions(self, lines: List[str]) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–π flow"""
        interruptions = 0
        punctuation = {'.', '!', '?', ';', ':', ',', '--', '...'}
        
        for line in lines:
            for punct in punctuation:
                interruptions += line.count(punct)
        
        return interruptions
    
    def _calculate_rhythmic_density(self, lines: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏"""
        if not lines:
            return 0.0
        
        total_words = sum(len(line.split()) for line in lines)
        total_syllables = sum(self._count_syllables_advanced(line) for line in lines)
        
        if total_words == 0:
            return 0.0
        
        # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å = —Å–ª–æ–≥–∏ –Ω–∞ —Å–ª–æ–≤–æ
        density = total_syllables / total_words
        return min(density / 2.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0, 1]
    
    def _empty_flow_result(self) -> Dict[str, Any]:
        """–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ flow"""
        return {
            'syllable_consistency': 0.0,
            'average_syllables_per_line': 0.0,
            'syllable_variance': 0.0,
            'line_length_consistency': 0.0,
            'stress_pattern_regularity': 0.0,
            'flow_breaks': 0,
            'rhythmic_density': 0.0
        }

class RhymeAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–∏—Ñ–º —Å —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"""
    
    def __init__(self):
        self.phonetic_patterns = PhoneticPattern()
        self.rhyme_cache = {}
    
    def analyze_rhyme_structure(self, lines: List[str]) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        if len(lines) < 2:
            return self._empty_rhyme_result()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è —Å—Ç—Ä–æ–∫
        line_endings = self._extract_line_endings(lines)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–∏—Ñ–º
        perfect_rhymes = self._find_perfect_rhymes(line_endings)
        near_rhymes = self._find_near_rhymes(line_endings)
        internal_rhymes = self._find_internal_rhymes(lines)
        
        # –°—Ö–µ–º–∞ —Ä–∏—Ñ–º–æ–≤–∫–∏
        rhyme_scheme = self._detect_complex_rhyme_scheme(line_endings)
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        phonetic_similarity = self._calculate_phonetic_similarity(line_endings)
        rhyme_density = self._calculate_rhyme_density(line_endings, perfect_rhymes, near_rhymes)
        
        return {
            'perfect_rhymes': len(perfect_rhymes),
            'near_rhymes': len(near_rhymes),
            'internal_rhymes': len(internal_rhymes),
            'rhyme_scheme': rhyme_scheme,
            'rhyme_scheme_complexity': self._evaluate_scheme_complexity(rhyme_scheme),
            'phonetic_similarity_score': phonetic_similarity,
            'rhyme_density': rhyme_density,
            'alliteration_score': self._calculate_alliteration(lines),
            'assonance_score': self._calculate_assonance(lines),
            'consonance_score': self._calculate_consonance(lines)
        }
    
    def _extract_line_endings(self, lines: List[str]) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏–π —Å—Ç—Ä–æ–∫ —Å –æ—á–∏—Å—Ç–∫–æ–π"""
        endings = []
        for line in lines:
            # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ
            words = re.findall(r'\b[a-zA-Z]+\b', line)
            if words:
                ending = words[-1].lower()
                endings.append(ending)
            else:
                endings.append("")
        return endings
    
    def _find_perfect_rhymes(self, endings: List[str]) -> List[Tuple[int, int]]:
        """–ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Ä–∏—Ñ–º"""
        perfect_rhymes = []
        
        for i in range(len(endings)):
            for j in range(i + 1, len(endings)):
                if self._is_perfect_rhyme(endings[i], endings[j]):
                    perfect_rhymes.append((i, j))
        
        return perfect_rhymes
    
    def _find_near_rhymes(self, endings: List[str]) -> List[Tuple[int, int]]:
        """–ü–æ–∏—Å–∫ –Ω–µ—Ç–æ—á–Ω—ã—Ö —Ä–∏—Ñ–º"""
        near_rhymes = []
        
        for i in range(len(endings)):
            for j in range(i + 1, len(endings)):
                if (not self._is_perfect_rhyme(endings[i], endings[j]) and 
                    self._is_near_rhyme(endings[i], endings[j])):
                    near_rhymes.append((i, j))
        
        return near_rhymes
    
    def _find_internal_rhymes(self, lines: List[str]) -> List[Tuple[int, str, str]]:
        """–ü–æ–∏—Å–∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ä–∏—Ñ–º"""
        internal_rhymes = []
        
        for line_idx, line in enumerate(lines):
            words = re.findall(r'\b[a-zA-Z]{3,}\b', line.lower())
            
            # –ò—â–µ–º —Ä–∏—Ñ–º—ã –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏
            for i in range(len(words)):
                for j in range(i + 1, len(words)):
                    if self._is_perfect_rhyme(words[i], words[j]) or self._is_near_rhyme(words[i], words[j]):
                        internal_rhymes.append((line_idx, words[i], words[j]))
        
        return internal_rhymes
    
    def _is_perfect_rhyme(self, word1: str, word2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ–π —Ä–∏—Ñ–º—ã —Å —É—á–µ—Ç–æ–º —Ñ–æ–Ω–µ—Ç–∏–∫–∏"""
        if not word1 or not word2 or word1 == word2:
            return False
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        cache_key = tuple(sorted([word1, word2]))
        if cache_key in self.rhyme_cache:
            return self.rhyme_cache[cache_key]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è–º —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
        result = False
        for suffix_len in range(2, min(len(word1), len(word2)) + 1):
            if word1[-suffix_len:] == word2[-suffix_len:]:
                result = True
                break
        
        # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        if not result:
            result = self._phonetic_rhyme_check(word1, word2)
        
        self.rhyme_cache[cache_key] = result
        return result
    
    def _is_near_rhyme(self, word1: str, word2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ—Ç–æ—á–Ω–æ–π —Ä–∏—Ñ–º—ã"""
        if not word1 or not word2 or len(word1) < 2 or len(word2) < 2:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–∑–≤—É—á–∏–µ –≥–ª–∞—Å–Ω—ã—Ö (assonance)
        vowels1 = [c for c in word1[-3:] if c in 'aeiou']
        vowels2 = [c for c in word2[-3:] if c in 'aeiou']
        
        if vowels1 and vowels2 and vowels1[-1] == vowels2[-1]:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–∑–≤—É—á–∏–µ —Å–æ–≥–ª–∞—Å–Ω—ã—Ö (consonance)
        consonants1 = [c for c in word1[-3:] if c not in 'aeiou']
        consonants2 = [c for c in word2[-3:] if c not in 'aeiou']
        
        if len(set(consonants1) & set(consonants2)) >= 1:
            return True
        
        return False
    
    def _phonetic_rhyme_check(self, word1: str, word2: str) -> bool:
        """–§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∏—Ñ–º—ã"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã phonetic matching –∞–ª–≥–æ—Ä–∏—Ç–º
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–∏–µ –∑–≤—É–∫–∏
        phonetic_groups = {
            'k_sounds': ['c', 'k', 'ck', 'q'],
            's_sounds': ['s', 'c', 'z'],
            'f_sounds': ['f', 'ph', 'gh'],
            'long_a': ['a', 'ai', 'ay', 'ei'],
            'long_e': ['e', 'ee', 'ea', 'ie'],
            'long_i': ['i', 'ie', 'y', 'igh'],
            'long_o': ['o', 'oa', 'ow', 'ough'],
            'long_u': ['u', 'ue', 'ew', 'ou']
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è –Ω–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        end1 = word1[-2:]
        end2 = word2[-2:]
        
        for group in phonetic_groups.values():
            if any(end1.endswith(sound) for sound in group) and any(end2.endswith(sound) for sound in group):
                return True
        
        return False
    
    def _detect_complex_rhyme_scheme(self, endings: List[str]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ–π —Å—Ö–µ–º—ã —Ä–∏—Ñ–º–æ–≤–∫–∏"""
        if len(endings) < 4:
            return "insufficient"
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 16 —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–µ–º—ã
        sample = endings[:16]
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–∏—Ñ–º—É—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞
        rhyme_groups = {}
        scheme = []
        next_letter = 'A'
        
        for ending in sample:
            assigned_letter = None
            
            # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≥—Ä—É–ø–ø—É
            for group_word, letter in rhyme_groups.items():
                if self._is_perfect_rhyme(ending, group_word) or self._is_near_rhyme(ending, group_word):
                    assigned_letter = letter
                    break
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
            if assigned_letter is None:
                assigned_letter = next_letter
                rhyme_groups[ending] = next_letter
                next_letter = chr(ord(next_letter) + 1)
            
            scheme.append(assigned_letter)
        
        return ''.join(scheme)
    
    def _evaluate_scheme_complexity(self, scheme: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å—Ö–µ–º—ã —Ä–∏—Ñ–º–æ–≤–∫–∏"""
        if not scheme or scheme == "insufficient":
            return 0.0
        
        # –§–∞–∫—Ç–æ—Ä—ã —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        unique_rhymes = len(set(scheme))
        total_lines = len(scheme)
        
        # –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        patterns = {
            'ABAB': 0.6,
            'AABB': 0.4,
            'ABCB': 0.7,
            'ABBA': 0.8,
            'AAAA': 0.2
        }
        
        complexity_score = unique_rhymes / total_lines
        
        # –ë–æ–Ω—É—Å –∑–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern, bonus in patterns.items():
            if pattern in scheme:
                complexity_score += bonus * 0.1
        
        return min(complexity_score, 1.0)
    
    def _calculate_phonetic_similarity(self, endings: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        if len(endings) < 2:
            return 0.0
        
        similarity_scores = []
        
        for i in range(len(endings)):
            for j in range(i + 1, len(endings)):
                score = self._phonetic_similarity_score(endings[i], endings[j])
                similarity_scores.append(score)
        
        return sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0.0
    
    def _phonetic_similarity_score(self, word1: str, word2: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–≤—É—Ö —Å–ª–æ–≤"""
        if not word1 or not word2:
            return 0.0
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
        max_len = min(len(word1), len(word2), 4)
        similarity = 0.0
        
        for i in range(1, max_len + 1):
            if word1[-i:] == word2[-i:]:
                similarity += i * 0.25
        
        return min(similarity, 1.0)
    
    def _calculate_rhyme_density(self, endings: List[str], perfect_rhymes: List, near_rhymes: List) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ä–∏—Ñ–º"""
        if len(endings) < 2:
            return 0.0
        
        total_rhymes = len(perfect_rhymes) + len(near_rhymes) * 0.7  # Near rhymes —Å—á–∏—Ç–∞—é—Ç—Å—è —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º
        max_possible_rhymes = len(endings) // 2  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ä–∏—Ñ–º
        
        return min(total_rhymes / max_possible_rhymes, 1.0) if max_possible_rhymes > 0 else 0.0
    
    def _calculate_alliteration(self, lines: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏"""
        if not lines:
            return 0.0
        
        alliteration_count = 0
        total_word_pairs = 0
        
        for line in lines:
            words = [word.lower() for word in re.findall(r'\b[a-zA-Z]{2,}\b', line)]
            if len(words) < 2:
                continue
            
            for i in range(len(words) - 1):
                total_word_pairs += 1
                if words[i][0] == words[i + 1][0]:
                    alliteration_count += 1
                    
                    # –ë–æ–Ω—É—Å –∑–∞ –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏—é —á–µ—Ä–µ–∑ —Å–ª–æ–≤–æ
                    if i < len(words) - 2 and words[i][0] == words[i + 2][0]:
                        alliteration_count += 0.5
        
        return alliteration_count / max(total_word_pairs, 1)
    
    def _calculate_assonance(self, lines: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∞—Å—Å–æ–Ω–∞–Ω—Å–∞ (–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –≥–ª–∞—Å–Ω—ã—Ö)"""
        if not lines:
            return 0.0
        
        vowels = 'aeiou'
        assonance_count = 0
        total_comparisons = 0
        
        for line in lines:
            words = [word.lower() for word in re.findall(r'\b[a-zA-Z]{3,}\b', line)]
            
            for i in range(len(words)):
                for j in range(i + 1, min(i + 3, len(words))):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–∏–∂–∞–π—à–∏–µ —Å–ª–æ–≤–∞
                    vowels_i = [c for c in words[i] if c in vowels]
                    vowels_j = [c for c in words[j] if c in vowels]
                    
                    if vowels_i and vowels_j:
                        total_comparisons += 1
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≥–ª–∞—Å–Ω—ã—Ö
                        common_vowels = set(vowels_i) & set(vowels_j)
                        if common_vowels:
                            assonance_count += len(common_vowels) / max(len(vowels_i), len(vowels_j))
        
        return assonance_count / max(total_comparisons, 1)
    
    def _calculate_consonance(self, lines: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ–Ω—Å–æ–Ω–∞–Ω—Å–∞ (–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–Ω—ã—Ö)"""
        if not lines:
            return 0.0
        
        vowels = 'aeiou'
        consonance_count = 0
        total_comparisons = 0
        
        for line in lines:
            words = [word.lower() for word in re.findall(r'\b[a-zA-Z]{3,}\b', line)]
            
            for i in range(len(words)):
                for j in range(i + 1, min(i + 3, len(words))):
                    consonants_i = [c for c in words[i] if c not in vowels and c.isalpha()]
                    consonants_j = [c for c in words[j] if c not in vowels and c.isalpha()]
                    
                    if consonants_i and consonants_j:
                        total_comparisons += 1
                        common_consonants = set(consonants_i) & set(consonants_j)
                        if common_consonants:
                            consonance_count += len(common_consonants) / max(len(consonants_i), len(consonants_j))
        
        return consonance_count / max(total_comparisons, 1)
    
    def _empty_rhyme_result(self) -> Dict[str, Any]:
        """–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Ñ–º"""
        return {
            'perfect_rhymes': 0,
            'near_rhymes': 0,
            'internal_rhymes': 0,
            'rhyme_scheme': 'insufficient',
            'rhyme_scheme_complexity': 0.0,
            'phonetic_similarity_score': 0.0,
            'rhyme_density': 0.0,
            'alliteration_score': 0.0,
            'assonance_score': 0.0,
            'consonance_score': 0.0
        }

class ReadabilityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    
    def analyze_readability(self, text: str) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not text.strip():
            return self._empty_readability_result()
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        sentences = self._count_sentences(text)
        words = self._count_words(text)
        syllables = self._count_total_syllables(text)
        
        if sentences == 0 or words == 0:
            return self._empty_readability_result()
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏
        flesch_score = self._calculate_flesch_reading_ease(sentences, words, syllables)
        flesch_kincaid_grade = self._calculate_flesch_kincaid_grade(sentences, words, syllables)
        smog_index = self._calculate_smog_index(text, sentences)
        ari_score = self._calculate_ari(sentences, words, text)
        coleman_liau = self._calculate_coleman_liau(text, sentences, words)
        
        return {
            'flesch_reading_ease': flesch_score,
            'flesch_kincaid_grade': flesch_kincaid_grade,
            'smog_index': smog_index,
            'automated_readability_index': ari_score,
            'coleman_liau_index': coleman_liau,
            'average_sentence_length': words / sentences,
            'average_syllables_per_word': syllables / words,
            'readability_consensus': self._calculate_consensus(flesch_score, flesch_kincaid_grade, smog_index)
        }
    
    def _count_sentences(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return len(sentences)
    
    def _count_words(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤"""
        words = re.findall(r'\b[a-zA-Z]+\b', text)
        return len(words)
    
    def _count_total_syllables(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≥–æ–≤"""
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        total_syllables = 0
        
        flow_analyzer = FlowAnalyzer()
        for word in words:
            total_syllables += flow_analyzer._syllables_in_word(word)
        
        return total_syllables
    
    def _calculate_flesch_reading_ease(self, sentences: int, words: int, syllables: int) -> float:
        """–ò–Ω–¥–µ–∫—Å —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ –§–ª–µ—à–∞"""
        if sentences == 0 or words == 0:
            return 0.0
        
        asl = words / sentences  # Average Sentence Length
        asw = syllables / words  # Average Syllables per Word
        
        score = 206.835 - (1.015 * asl) - (84.6 * asw)
        return max(0, min(100, score))
    
    def _calculate_flesch_kincaid_grade(self, sentences: int, words: int, syllables: int) -> float:
        """–ò–Ω–¥–µ–∫—Å —É—Ä–æ–≤–Ω—è –∫–ª–∞—Å—Å–∞ –§–ª–µ—à–∞-–ö–∏–Ω–∫–µ–π–¥–∞"""
        if sentences == 0 or words == 0:
            return 0.0
        
        asl = words / sentences
        asw = syllables / words
        
        grade = (0.39 * asl) + (11.8 * asw) - 15.59
        return max(0, grade)
    
    def _calculate_smog_index(self, text: str, sentences: int) -> float:
        """–ò–Ω–¥–µ–∫—Å SMOG"""
        if sentences < 3:
            return 0.0
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–∞ —Å 3+ —Å–ª–æ–≥–∞–º–∏
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        complex_words = 0
        flow_analyzer = FlowAnalyzer()
        
        for word in words:
            if flow_analyzer._syllables_in_word(word) >= 3:
                complex_words += 1
        
        if complex_words == 0:
            return 0.0
        
        # SMOG = 3 + ‚àö(complex_words * 30 / sentences)
        smog = 3 + math.sqrt(complex_words * 30 / sentences)
        return smog
    
    def _calculate_ari(self, sentences: int, words: int, text: str) -> float:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ (ARI)"""
        if sentences == 0 or words == 0:
            return 0.0
        
        characters = len(re.sub(r'[^a-zA-Z]', '', text))
        
        if characters == 0:
            return 0.0
        
        ari = (4.71 * (characters / words)) + (0.5 * (words / sentences)) - 21.43
        return max(0, ari)
    
    def _calculate_coleman_liau(self, text: str, sentences: int, words: int) -> float:
        """–ò–Ω–¥–µ–∫—Å –ö–æ—É–ª–º–∞–Ω–∞-–õ–∏–∞—É"""
        if words == 0 or sentences == 0:
            return 0.0
        
        characters = len(re.sub(r'[^a-zA-Z]', '', text))
        
        l = (characters / words) * 100  # Average letters per 100 words
        s = (sentences / words) * 100   # Average sentences per 100 words
        
        cli = (0.0588 * l) - (0.296 * s) - 15.8
        return max(0, cli)
    
    def _calculate_consensus(self, flesch: float, fk_grade: float, smog: float) -> str:
        """–ö–æ–Ω—Å–µ–Ω—Å—É—Å –ø–æ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Flesch –≤ –ø—Ä–∏–º–µ—Ä–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–ª–∞—Å—Å–∞
        if flesch >= 90:
            flesch_grade = 5
        elif flesch >= 80:
            flesch_grade = 6
        elif flesch >= 70:
            flesch_grade = 7
        elif flesch >= 60:
            flesch_grade = 8
        elif flesch >= 50:
            flesch_grade = 9
        elif flesch >= 30:
            flesch_grade = 12
        else:
            flesch_grade = 16
        
        # –°—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ —É—Ä–æ–≤–Ω–µ–π
        avg_grade = (flesch_grade + fk_grade + smog) / 3
        
        if avg_grade <= 6:
            return "elementary"
        elif avg_grade <= 8:
            return "middle_school"
        elif avg_grade <= 12:
            return "high_school"
        elif avg_grade <= 16:
            return "college"
        else:
            return "graduate"
    
    def _empty_readability_result(self) -> Dict[str, Any]:
        """–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return {
            'flesch_reading_ease': 0.0,
            'flesch_kincaid_grade': 0.0,
            'smog_index': 0.0,
            'automated_readability_index': 0.0,
            'coleman_liau_index': 0.0,
            'average_sentence_length': 0.0,
            'average_syllables_per_word': 0.0,
            'readability_consensus': 'insufficient'
        }

@register_analyzer("advanced_algorithmic")
class AdvancedAlgorithmicAnalyzer(BaseAnalyzer):
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º
    - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ flow –∏ —Ä–∏—Ç–º–∞
    - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏
    - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å –≥—Ä–∞–¥–∞—Ü–∏–µ–π —ç–º–æ—Ü–∏–π
    - –ê–Ω–∞–ª–∏–∑ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö –ø—Ä–∏–µ–º–æ–≤
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.lexicon = AdvancedLexicon()
        self.flow_analyzer = FlowAnalyzer()
        self.rhyme_analyzer = RhymeAnalyzer()
        self.readability_analyzer = ReadabilityAnalyzer()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.cache_enabled = self.config.get('cache_enabled', True)
        self.analysis_cache = {} if self.cache_enabled else None
        self.detailed_logging = self.config.get('detailed_logging', False)
        
        if self.detailed_logging:
            logger.setLevel(logging.DEBUG)
    
    def analyze_song(self, artist: str, title: str, lyrics: str) -> AnalysisResult:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Å–Ω–∏ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏
        """
        start_time = time.time()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = None
        if self.cache_enabled:
            cache_key = self._generate_cache_key(artist, title, lyrics)
            if cache_key in self.analysis_cache:
                cached_result = self.analysis_cache[cache_key]
                logger.debug(f"Returning cached result for {artist} - {title}")
                return cached_result
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        processed_lyrics = self.preprocess_lyrics(lyrics)
        lines = self._split_into_lines(processed_lyrics)
        words = self._extract_meaningful_words(processed_lyrics)
        
        if self.detailed_logging:
            logger.debug(f"Processing {artist} - {title}: {len(lines)} lines, {len(words)} words")
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑
        analysis_results = {
            'advanced_sentiment': self._analyze_advanced_sentiment(words, processed_lyrics),
            'rhyme_analysis': self.rhyme_analyzer.analyze_rhyme_structure(lines),
            'flow_analysis': self.flow_analyzer.analyze_flow_patterns(lines),
            'readability_metrics': self.readability_analyzer.analyze_readability(processed_lyrics),
            'thematic_analysis': self._analyze_themes_advanced(words),
            'literary_devices': self._analyze_literary_devices(processed_lyrics, words),
            'vocabulary_sophistication': self._analyze_vocabulary_sophistication(words),
            'structural_analysis': self._analyze_structure_advanced(lines, processed_lyrics),
            'creativity_metrics': self._analyze_creativity_advanced(processed_lyrics, words, lines)
        }
        
        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        composite_scores = self._calculate_advanced_composite_scores(analysis_results)
        analysis_results.update(composite_scores)
        
        # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = self._calculate_advanced_confidence(analysis_results, lines, words)
        
        processing_time = time.time() - start_time
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            "analyzer_version": "2.0.0",
            "processing_date": datetime.now().isoformat(),
            "lyrics_length": len(processed_lyrics),
            "word_count": len(words),
            "line_count": len(lines),
            "processing_components": list(analysis_results.keys()),
            "cache_used": False,
            "detailed_logging": self.detailed_logging
        }
        
        # Build plain dictionary result (legacy-friendly)
        result_dict = {
            'analysis_type': 'advanced_algorithmic',
            'analysis_data': analysis_results,
            'confidence': confidence,
            'processing_time': processing_time,
            'metadata': metadata,
            'raw_output': analysis_results,
            'timestamp': datetime.now().isoformat(),
            'artist': artist,
            'title': title
        }

        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if self.cache_enabled and cache_key:
            self.analysis_cache[cache_key] = result_dict

        if self.detailed_logging:
            logger.debug(f"Analysis completed for {artist} - {title} in {processing_time:.3f}s")

        return result_dict
    
    def _generate_cache_key(self, artist: str, title: str, lyrics: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –¥–ª—è –∫—ç—à–∞"""
        content = f"{artist}|{title}|{lyrics[:500]}"  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        return hashlib.md5(content.encode()).hexdigest()
    
    def _split_into_lines(self, lyrics: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–æ–∫–∏ —Å –æ—á–∏—Å—Ç–∫–æ–π"""
        lines = [line.strip() for line in lyrics.split('\n') if line.strip()]
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        filtered_lines = []
        for line in lines:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            if not re.match(r'^\[.*\]$|^\(.*\)$|^(Verse|Chorus|Bridge|Outro|Intro)[\s\d]*:', line, re.IGNORECASE):
                filtered_lines.append(line)
        
        return filtered_lines
    
    def _extract_meaningful_words(self, lyrics: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤"""
        words = re.findall(r'\b[a-zA-Z]{2,}\b', lyrics.lower())
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of',
            'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before',
            'after', 'above', 'below', 'between', 'among', 'this', 'that', 'these', 'those',
            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',
            'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',
            'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',
            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
            'having', 'do', 'does', 'did', 'doing', 'will', 'would', 'could', 'should',
            'yeah', 'uh', 'oh', 'got', 'get', 'gotta', 'wanna', 'gonna', 'ain', 'yall', 'em', 'ya',
            'like', 'just', 'now', 'know', 'see', 'come', 'go', 'say', 'said', 'tell', 'make',
            'way', 'time', 'want', 'need', 'take', 'give', 'put', 'keep', 'let', 'think'
        }
        
        return [word for word in words if word not in stop_words and len(word) >= 3]
    
    def _analyze_advanced_sentiment(self, words: List[str], full_text: str) -> Dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Å –≥—Ä–∞–¥–∞—Ü–∏–µ–π"""
        if not words:
            return self._empty_sentiment_result()
        
        emotion_scores = {}
        total_emotional_words = 0
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —ç–º–æ—Ü–∏–π —Å —É—á–µ—Ç–æ–º –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
        for emotion, intensity_levels in self.lexicon.emotions.items():
            emotion_score = 0.0
            emotion_word_count = 0
            
            for intensity, word_set in intensity_levels.items():
                matches = len(set(words) & word_set)
                if matches > 0:
                    # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
                    intensity_weight = {'weak': 1.0, 'medium': 2.0, 'strong': 3.0}[intensity]
                    emotion_score += matches * intensity_weight
                    emotion_word_count += matches
                    total_emotional_words += matches
            
            emotion_scores[emotion] = {
                'score': emotion_score,
                'word_count': emotion_word_count,
                'normalized_score': emotion_score / len(words) if words else 0
            }
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–π —ç–º–æ—Ü–∏–∏
        if emotion_scores:
            dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1]['score'])
            dominant_emotion_name = dominant_emotion[0]
            dominant_emotion_strength = dominant_emotion[1]['score']
        else:
            dominant_emotion_name = 'neutral'
            dominant_emotion_strength = 0
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–π —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        positive_emotions = emotion_scores.get('joy', {}).get('score', 0)
        negative_emotions = (emotion_scores.get('anger', {}).get('score', 0) + 
                           emotion_scores.get('sadness', {}).get('score', 0) + 
                           emotion_scores.get('fear', {}).get('score', 0))
        
        if total_emotional_words > 0:
            valence = (positive_emotions - negative_emotions) / total_emotional_words
        else:
            valence = 0.0
        
        return {
            'emotion_scores': emotion_scores,
            'dominant_emotion': dominant_emotion_name,
            'dominant_emotion_strength': dominant_emotion_strength,
            'emotional_valence': valence,
            'emotional_intensity': total_emotional_words / len(words) if words else 0,
            'total_emotional_words': total_emotional_words,
            'emotional_complexity': len([e for e in emotion_scores.values() if e['score'] > 0])
        }
    
    def _analyze_themes_advanced(self, words: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"""
        if not words:
            return {'theme_scores': {}, 'dominant_theme': 'neutral'}
        
        word_set = set(words)
        theme_scores = {}
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for theme, theme_words in self.lexicon.rap_themes.items():
            matches = len(word_set & theme_words)
            theme_scores[theme] = {
                'absolute_count': matches,
                'relative_score': matches / len(words),
                'theme_coverage': matches / len(theme_words) if theme_words else 0
            }
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏—Ö —Ç–µ–º
        sorted_themes = sorted(theme_scores.items(), 
                              key=lambda x: x[1]['absolute_count'], 
                              reverse=True)
        
        dominant_theme = sorted_themes[0][0] if sorted_themes and sorted_themes[0][1]['absolute_count'] > 0 else 'neutral'
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        active_themes = [theme for theme, scores in theme_scores.items() if scores['absolute_count'] > 0]
        thematic_diversity = len(active_themes) / len(self.lexicon.rap_themes)
        
        return {
            'theme_scores': theme_scores,
            'dominant_theme': dominant_theme,
            'secondary_themes': [theme[0] for theme in sorted_themes[1:4] if theme[1]['absolute_count'] > 0],
            'thematic_diversity': thematic_diversity,
            'total_thematic_words': sum(score['absolute_count'] for score in theme_scores.values())
        }
    
    def _analyze_literary_devices(self, lyrics: str, words: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö –ø—Ä–∏–µ–º–æ–≤"""
        if not lyrics or not words:
            return self._empty_literary_result()
        
        # –ü–æ–∏—Å–∫ –º–µ—Ç–∞—Ñ–æ—Ä –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–π
        metaphor_count = 0
        simile_count = 0
        lyrics_lower = lyrics.lower()
        
        for indicator in self.lexicon.literary_devices['metaphor_indicators']:
            if indicator in ['like', 'as']:
                simile_count += lyrics_lower.count(f' {indicator} ')
            else:
                metaphor_count += lyrics_lower.count(indicator)
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—Ç—Å—ã–ª–æ–∫
        time_references = sum(lyrics_lower.count(time_word) for time_word in self.lexicon.literary_devices['time_references'])
        
        # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–æ–≤ –∏ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π
        contrast_usage = sum(lyrics_lower.count(contrast_word) for contrast_word in self.lexicon.literary_devices['contrast_words'])
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ —Ä–µ—Ñ—Ä–µ–Ω–æ–≤
        line_repetitions = self._analyze_repetitions(lyrics)
        
        # –ü–µ—Ä—Å–æ–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        personification_indicators = ['speaks', 'whispers', 'calls', 'cries', 'laughs', 'dances']
        personification_count = sum(lyrics_lower.count(indicator) for indicator in personification_indicators)
        
        return {
            'metaphor_count': metaphor_count,
            'simile_count': simile_count,
            'time_references': time_references,
            'contrast_usage': contrast_usage,
            'repetition_analysis': line_repetitions,
            'personification_count': personification_count,
            'total_literary_devices': metaphor_count + simile_count + time_references + contrast_usage + personification_count
        }
    
    def _analyze_repetitions(self, lyrics: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        lines = [line.strip().lower() for line in lyrics.split('\n') if line.strip()]
        
        if not lines:
            return {'repeated_lines': 0, 'repetition_ratio': 0.0}
        
        line_counts = Counter(lines)
        repeated_lines = {line: count for line, count in line_counts.items() if count > 1}
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑ (2-4 —Å–ª–æ–≤–∞)
        phrase_repetitions = defaultdict(int)
        for line in lines:
            words = line.split()
            for i in range(len(words) - 1):
                for j in range(2, min(5, len(words) - i + 1)):
                    phrase = ' '.join(words[i:i+j])
                    if len(phrase) > 5:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã
                        phrase_repetitions[phrase] += 1
        
        repeated_phrases = {phrase: count for phrase, count in phrase_repetitions.items() if count > 1}
        
        return {
            'repeated_lines': len(repeated_lines),
            'repeated_phrases': len(repeated_phrases),
            'repetition_ratio': len(repeated_lines) / len(set(lines)) if lines else 0,
            'most_repeated_line': max(line_counts.items(), key=lambda x: x[1]) if line_counts else None,
            'total_line_repetitions': sum(count - 1 for count in line_counts.values() if count > 1)
        }
    
    def _analyze_vocabulary_sophistication(self, words: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤–∞—Ä—è"""
        if not words:
            return self._empty_vocabulary_result()
        
        word_set = set(words)
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º—ã—à–ª–µ–Ω–∏—è
        complexity_scores = {}
        total_complex_words = 0
        
        for category, category_words in self.lexicon.complexity_indicators.items():
            matches = len(word_set & category_words)
            complexity_scores[category] = matches
            total_complex_words += matches
        
        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤
        word_lengths = [len(word) for word in words]
        avg_word_length = sum(word_lengths) / len(word_lengths)
        long_words = len([w for w in words if len(w) >= 7])
        
        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞—Ä—è
        vocabulary_richness = len(word_set) / len(words)
        
        # –†–µ–¥–∫–∏–µ/–Ω–µ–æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        common_words = {
            'love', 'money', 'life', 'time', 'good', 'bad', 'right', 'wrong',
            'black', 'white', 'red', 'blue', 'big', 'small', 'old', 'new',
            'high', 'low', 'fast', 'slow', 'hot', 'cold', 'young', 'real',
            'hard', 'easy', 'free', 'game', 'play', 'work', 'home', 'house'
        }
        
        uncommon_words = len(word_set - common_words)
        uncommon_ratio = uncommon_words / len(words)
        
        return {
            'complexity_scores': complexity_scores,
            'total_complex_words': total_complex_words,
            'average_word_length': avg_word_length,
            'long_words_count': long_words,
            'vocabulary_richness': vocabulary_richness,
            'uncommon_words_ratio': uncommon_ratio,
            'lexical_diversity': len(word_set) / max(len(words), 1),
            'sophisticated_vocabulary_score': (total_complex_words + long_words + uncommon_words) / len(words)
        }
    
    def _analyze_structure_advanced(self, lines: List[str], full_text: str) -> Dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        if not lines:
            return self._empty_structure_result()
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        total_lines = len(lines)
        line_lengths = [len(line.split()) for line in lines]
        avg_line_length = sum(line_lengths) / len(line_lengths)
        line_length_variance = sum((x - avg_line_length) ** 2 for x in line_lengths) / len(line_lengths)
        
        # –ê–Ω–∞–ª–∏–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –ø–∞—É–∑—ã
        punctuation_analysis = self._analyze_punctuation(full_text)
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å
        structure_patterns = self._find_structure_patterns(lines)
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–ø–æ –ø—É—Å—Ç—ã–º —Å—Ç—Ä–æ–∫–∞–º)
        stanzas = self._identify_stanzas(full_text)
        
        return {
            'total_lines': total_lines,
            'average_line_length': avg_line_length,
            'line_length_variance': line_length_variance,
            'punctuation_analysis': punctuation_analysis,
            'structure_patterns': structure_patterns,
            'stanza_analysis': stanzas,
            'structural_consistency': 1.0 - (line_length_variance / max(avg_line_length, 1))
        }
    
    def _analyze_punctuation(self, text: str) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏"""
        punctuation_counts = {
            'periods': text.count('.'),
            'commas': text.count(','),
            'exclamations': text.count('!'),
            'questions': text.count('?'),
            'semicolons': text.count(';'),
            'colons': text.count(':'),
            'dashes': text.count('-') + text.count('--'),
            'ellipses': text.count('...'),
            'parentheses': text.count('(') + text.count(')'),
            'quotations': text.count('"') + text.count("'")
        }
        
        total_punctuation = sum(punctuation_counts.values())
        punctuation_counts['total'] = total_punctuation
        
        return punctuation_counts
    
    def _find_structure_patterns(self, lines: List[str]) -> Dict[str, Any]:
        """–ü–æ–∏—Å–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if len(lines) < 4:
            return {'pattern_found': False}
        
        # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª–∏–Ω—ã —Å—Ç—Ä–æ–∫
        line_lengths = [len(line.split()) for line in lines]
        
        # –ü–æ–∏—Å–∫ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, ABAB –ø–æ –¥–ª–∏–Ω–µ)
        pattern_length = 4
        patterns_found = []
        
        for i in range(len(line_lengths) - pattern_length + 1):
            pattern = line_lengths[i:i+pattern_length]
            # –ò—â–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —ç—Ç–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            for j in range(i + pattern_length, len(line_lengths) - pattern_length + 1):
                if line_lengths[j:j+pattern_length] == pattern:
                    patterns_found.append(pattern)
                    break
        
        return {
            'pattern_found': len(patterns_found) > 0,
            'patterns': patterns_found,
            'pattern_consistency': len(patterns_found) / max(len(line_lengths) // pattern_length, 1)
        }
    
    def _identify_stanzas(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫
        stanzas = [stanza.strip() for stanza in re.split(r'\n\s*\n', text) if stanza.strip()]
        
        if not stanzas:
            stanzas = [text]  # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω—ã—Ö —Å—Ç—Ä–æ—Ñ, –≤–µ—Å—å —Ç–µ–∫—Å—Ç - –æ–¥–Ω–∞ —Å—Ç—Ä–æ—Ñ–∞
        
        stanza_lengths = [len(stanza.split('\n')) for stanza in stanzas]
        
        return {
            'stanza_count': len(stanzas),
            'average_stanza_length': sum(stanza_lengths) / len(stanza_lengths) if stanza_lengths else 0,
            'stanza_length_consistency': self._calculate_consistency_score(stanza_lengths),
            'stanza_lengths': stanza_lengths
        }
    
    def _analyze_creativity_advanced(self, lyrics: str, words: List[str], lines: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        if not words or not lines:
            return self._empty_creativity_result()
        
        # –ù–µ–æ–ª–æ–≥–∏–∑–º—ã –∏ –Ω–µ–æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã
        neologisms = self._detect_neologisms(words)
        
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        unique_phrases = self._find_unique_phrases(lines)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–¥–≤–∏–≥–∏ –∏ –∏–≥—Ä–∞ —Å–ª–æ–≤
        wordplay_analysis = self._analyze_advanced_wordplay(lyrics, words)
        
        # –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å —Ä–∏—Ñ–º
        innovative_rhymes = self._analyze_rhyme_innovation(lines)
        
        # –û–±—â–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
        creativity_factors = [
            len(neologisms) / max(len(words), 1),
            len(unique_phrases) / max(len(lines), 1),
            wordplay_analysis['total_score'],
            innovative_rhymes['innovation_score']
        ]
        
        overall_creativity = sum(creativity_factors) / len(creativity_factors)
        
        return {
            'neologisms': neologisms,
            'unique_phrases': unique_phrases,
            'wordplay_analysis': wordplay_analysis,
            'innovative_rhymes': innovative_rhymes,
            'creativity_factors': creativity_factors,
            'overall_creativity_score': overall_creativity
        }
    
    def _detect_neologisms(self, words: List[str]) -> List[str]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–µ–æ–ª–æ–≥–∏–∑–º–æ–≤ –∏ –Ω–µ–æ–±—ã—á–Ω—ã—Ö —Å–ª–æ–≤"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω—ã—Ö –Ω–µ–æ–ª–æ–≥–∏–∑–º–æ–≤
        potential_neologisms = []
        
        for word in words:
            # –°–ª–æ–≤–∞ —Å –Ω–µ–æ–±—ã—á–Ω—ã–º–∏ —Å—É—Ñ—Ñ–∏–∫—Å–∞–º–∏ –∏–ª–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏
            if (len(word) > 6 and 
                (word.endswith('ness') or word.endswith('tion') or word.endswith('ism') or
                 word.startswith('un') or word.startswith('pre') or word.startswith('over'))):
                potential_neologisms.append(word)
            
            # –°–ª–æ–≤–∞ —Å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è —á–∞—Å—Ç—è–º–∏
            if len(word) > 4:
                mid = len(word) // 2
                if word[:mid] == word[mid:] or word[:mid] in word[mid:]:
                    potential_neologisms.append(word)
        
        return potential_neologisms[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    def _find_unique_phrases(self, lines: List[str]) -> List[str]:
        """–ü–æ–∏—Å–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑–æ–≤—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"""
        unique_phrases = []
        
        # –ò—â–µ–º —Ñ—Ä–∞–∑—ã —Å –Ω–µ–æ–±—ã—á–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
        for line in lines:
            words = line.split()
            if len(words) >= 3:
                # –ü–æ–∏—Å–∫ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
                if len(words) >= 4 and words[0].lower() in ['when', 'where', 'how', 'why']:
                    unique_phrases.append(line)
                
                # –ü–æ–∏—Å–∫ –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏–π –≤ —Ñ—Ä–∞–∑–∞—Ö
                if len([w for w in words[:3] if w and words[0] and w[0].lower() == words[0][0].lower()]) >= 2:
                    unique_phrases.append(line)
        
        return unique_phrases[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    def _analyze_advanced_wordplay(self, lyrics: str, words: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–µ–º–æ–≤ –∏–≥—Ä—ã —Å–ª–æ–≤"""
        wordplay_score = 0
        techniques_found = []
        
        # –î–≤–æ–π–Ω—ã–µ —Å–º—ã—Å–ª—ã (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        double_meanings = []
        for word in set(words):
            if len(word) > 4:
                # –ü–æ–∏—Å–∫ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –¥–≤–æ–π–Ω–æ–π —Å–º—ã—Å–ª
                if any(other in word for other in words if other != word and len(other) > 2):
                    double_meanings.append(word)
        
        if double_meanings:
            wordplay_score += len(double_meanings) * 0.1
            techniques_found.append('double_meanings')
        
        # –ó–≤—É–∫–æ–ø–æ–¥—Ä–∞–∂–∞–Ω–∏—è
        onomatopoeia = ['bang', 'boom', 'crash', 'pop', 'snap', 'crack', 'splash', 'whoosh']
        onomatopoeia_count = sum(lyrics.lower().count(sound) for sound in onomatopoeia)
        
        if onomatopoeia_count > 0:
            wordplay_score += onomatopoeia_count * 0.05
            techniques_found.append('onomatopoeia')
        
        # –ö–∞–ª–∞–º–±—É—Ä—ã (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        puns_detected = 0
        for i, word in enumerate(words[:-1]):
            next_word = words[i + 1]
            if (len(word) > 3 and len(next_word) > 3 and 
                word[:-1] == next_word[:-1] and word != next_word):
                puns_detected += 1
        
        if puns_detected > 0:
            wordplay_score += puns_detected * 0.15
            techniques_found.append('potential_puns')
        
        return {
            'total_score': min(wordplay_score, 1.0),
            'techniques_found': techniques_found,
            'double_meanings': double_meanings,
            'onomatopoeia_count': onomatopoeia_count,
            'potential_puns': puns_detected
        }
    
    def _analyze_rhyme_innovation(self, lines: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç–∏ —Ä–∏—Ñ–º"""
        if len(lines) < 4:
            return {'innovation_score': 0.0}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è
        endings = []
        for line in lines[:12]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 12 —Å—Ç—Ä–æ–∫
            words = line.split()
            if words:
                ending = re.sub(r'[^\w]', '', words[-1].lower())
                if len(ending) >= 2:
                    endings.append(ending)
        
        innovation_factors = []
        
        # –ú—É–ª—å—Ç–∏—Å–ª–æ–∂–Ω—ã–µ —Ä–∏—Ñ–º—ã
        multisyllabic_rhymes = 0
        flow_analyzer = FlowAnalyzer()
        for ending in endings:
            if flow_analyzer._syllables_in_word(ending) >= 3:
                multisyllabic_rhymes += 1
        
        if endings:
            multisyllabic_ratio = multisyllabic_rhymes / len(endings)
            innovation_factors.append(multisyllabic_ratio)
        
        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∏—Ñ–º—ã
        internal_rhyme_count = 0
        for line in lines:
            words = line.split()
            for i in range(len(words) - 1):
                for j in range(i + 1, len(words)):
                    if self._simple_rhyme_check(words[i], words[j]):
                        internal_rhyme_count += 1
        
        internal_rhyme_ratio = internal_rhyme_count / max(len(lines), 1)
        innovation_factors.append(min(internal_rhyme_ratio, 1.0))
        
        # –ù–µ–æ–±—ã—á–Ω—ã–µ —Ä–∏—Ñ–º—ã (–¥–ª–∏–Ω–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è)
        long_rhymes = len([e for e in endings if len(e) >= 6])
        long_rhyme_ratio = long_rhymes / max(len(endings), 1)
        innovation_factors.append(long_rhyme_ratio)
        
        overall_innovation = sum(innovation_factors) / len(innovation_factors) if innovation_factors else 0
        
        return {
            'innovation_score': overall_innovation,
            'multisyllabic_rhymes': multisyllabic_rhymes,
            'internal_rhymes': internal_rhyme_count,
            'long_rhymes': long_rhymes,
            'innovation_factors': innovation_factors
        }
    
    def _simple_rhyme_check(self, word1: str, word2: str) -> bool:
        """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∏—Ñ–º—ã"""
        if len(word1) < 2 or len(word2) < 2 or word1 == word2:
            return False
        return word1[-2:].lower() == word2[-2:].lower()
    
    def _calculate_advanced_composite_scores(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        rhyme_density = analysis_results.get('rhyme_analysis', {}).get('rhyme_density', 0)
        flow_consistency = analysis_results.get('flow_analysis', {}).get('syllable_consistency', 0)
        vocabulary_richness = analysis_results.get('vocabulary_sophistication', {}).get('vocabulary_richness', 0)
        creativity_score = analysis_results.get('creativity_metrics', {}).get('overall_creativity_score', 0)
        readability = analysis_results.get('readability_metrics', {}).get('flesch_reading_ease', 0)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º readability (Flesch scale: 0-100, higher = easier)
        normalized_readability = readability / 100
        
        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        technical_mastery = (rhyme_density * 0.4 + flow_consistency * 0.4 + vocabulary_richness * 0.2)
        
        artistic_sophistication = (creativity_score * 0.5 + vocabulary_richness * 0.3 + 
                                 (1 - normalized_readability) * 0.2)  # –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π —Ç–µ–∫—Å—Ç = –≤—ã—à–µ –∞—Ä—Ç–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å
        
        overall_quality = (technical_mastery * 0.4 + artistic_sophistication * 0.4 + 
                          creativity_score * 0.2)
        
        # –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å
        innovation_score = creativity_score * 0.6 + rhyme_density * 0.4
        
        return {
            'composite_scores': {
                'technical_mastery': technical_mastery,
                'artistic_sophistication': artistic_sophistication,
                'overall_quality': overall_quality,
                'innovation_score': innovation_score,
                'complexity_balance': (vocabulary_richness + (1 - normalized_readability)) / 2
            }
        }
    
    def _calculate_advanced_confidence(self, analysis_results: Dict[str, Any], lines: List[str], words: List[str]) -> float:
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        confidence_factors = []
        
        # –§–∞–∫—Ç–æ—Ä –æ–±—ä–µ–º–∞ —Ç–µ–∫—Å—Ç–∞
        text_volume_factor = min(len(words) / 100, 1.0) * min(len(lines) / 10, 1.0)
        confidence_factors.append(text_volume_factor)
        
        # –§–∞–∫—Ç–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞
        expected_analyses = ['advanced_sentiment', 'rhyme_analysis', 'flow_analysis', 'readability_metrics']
        completed_analyses = sum(1 for analysis in expected_analyses if analysis in analysis_results)
        completeness_factor = completed_analyses / len(expected_analyses)
        confidence_factors.append(completeness_factor)
        
        # –§–∞–∫—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        if words and lines:
            avg_line_length = sum(len(line.split()) for line in lines) / len(lines)
            quality_factor = min(avg_line_length / 8, 1.0)  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏ ~ 8 —Å–ª–æ–≤
            confidence_factors.append(quality_factor)
        
        # –§–∞–∫—Ç–æ—Ä —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å–ª–æ–≤–∞—Ä—è
        if words:
            vocab_diversity = len(set(words)) / len(words)
            diversity_factor = min(vocab_diversity * 2, 1.0)
            confidence_factors.append(diversity_factor)
        
        return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5
    
    def _calculate_consistency_score(self, values: List[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–π"""
        if len(values) < 2:
            return 1.0
        
        mean = sum(values) / len(values)
        if mean == 0:
            return 1.0
        
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        coefficient_of_variation = (variance ** 0.5) / mean
        
        # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: —á–µ–º –º–µ–Ω—å—à–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏, —Ç–µ–º –≤—ã—à–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
        consistency = 1 / (1 + coefficient_of_variation)
        return min(consistency, 1.0)
    
    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –ø—É—Å—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    def _empty_sentiment_result(self):
        return {
            'emotion_scores': {},
            'dominant_emotion': 'neutral',
            'dominant_emotion_strength': 0,
            'emotional_valence': 0.0,
            'emotional_intensity': 0.0,
            'total_emotional_words': 0,
            'emotional_complexity': 0
        }
    
    def _empty_literary_result(self):
        return {
            'metaphor_count': 0,
            'simile_count': 0,
            'time_references': 0,
            'contrast_usage': 0,
            'repetition_analysis': {'repeated_lines': 0, 'repetition_ratio': 0.0},
            'personification_count': 0,
            'total_literary_devices': 0
        }
    
    def _empty_vocabulary_result(self):
        return {
            'complexity_scores': {},
            'total_complex_words': 0,
            'average_word_length': 0.0,
            'long_words_count': 0,
            'vocabulary_richness': 0.0,
            'uncommon_words_ratio': 0.0,
            'lexical_diversity': 0.0,
            'sophisticated_vocabulary_score': 0.0
        }
    
    def _empty_structure_result(self):
        return {
            'total_lines': 0,
            'average_line_length': 0.0,
            'line_length_variance': 0.0,
            'punctuation_analysis': {},
            'structure_patterns': {'pattern_found': False},
            'stanza_analysis': {'stanza_count': 0},
            'structural_consistency': 0.0
        }
    
    def _empty_creativity_result(self):
        return {
            'neologisms': [],
            'unique_phrases': [],
            'wordplay_analysis': {'total_score': 0.0},
            'innovative_rhymes': {'innovation_score': 0.0},
            'creativity_factors': [0.0, 0.0, 0.0, 0.0],
            'overall_creativity_score': 0.0
        }
    
    def get_analyzer_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ"""
        return {
            "name": "AdvancedAlgorithmicAnalyzer",
            "version": "2.0.0",
            "description": "Advanced algorithmic lyrics analysis with phonetic rhyme analysis, flow metrics, readability indices, and semantic sophistication scoring",
            "author": "Rap Scraper Project - Advanced Edition",
            "type": self.analyzer_type,
            "supported_features": self.supported_features,
            "components": [
                "AdvancedLexicon", "FlowAnalyzer", "RhymeAnalyzer", 
                "ReadabilityAnalyzer", "LiteraryDevicesAnalyzer"
            ],
            "config_options": {
                "cache_enabled": "Enable result caching for performance (default: True)",
                "detailed_logging": "Enable detailed debug logging (default: False)",
                "min_word_length": "Minimum word length for analysis (default: 3)",
                "max_cache_size": "Maximum cache entries (default: 1000)"
            },
            "performance": {
                "typical_processing_time": "50-200ms per song",
                "memory_usage": "~5-10MB for cache",
                "scalability": "Excellent for batch processing"
            }
        }
    
    @property
    def analyzer_type(self) -> str:
        """–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        return "advanced_algorithmic"
    
    @property
    def supported_features(self) -> List[str]:
        """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        return [
            "phonetic_rhyme_analysis",
            "advanced_flow_metrics",
            "readability_indices",
            "emotional_gradient_analysis",
            "thematic_categorization",
            "literary_devices_detection",
            "vocabulary_sophistication",
            "structural_pattern_analysis",
            "creativity_assessment",
            "composite_scoring",
            "performance_caching"
        ]
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        if self.analysis_cache:
            self.analysis_cache.clear()
            logger.info("Analysis cache cleared")

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
async def demo_advanced_analysis():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    
    sample_lyrics = """
    Metaphors cascade like waterfalls in my mind
    Each syllable calculated, rhythmically designed  
    Philosophy meets poetry in this verbal shrine
    Where consciousness and consonance perfectly align

    The lexicon I wield cuts deeper than a blade
    Multisyllabic mastery in every word I've made
    Assonance and alliteration, my lyrical trade
    While lesser wordsmiths stumble, my foundation's never swayed

    In the labyrinth of language, I navigate with ease
    Semantic sophistication brings critics to their knees  
    Innovation flows through every line like autumn leaves
    This artistic architecture is what true genius achieves
    """
    
    print("üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–†–û–î–í–ò–ù–£–¢–û–ì–û –ê–õ–ì–û–†–ò–¢–ú–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê")
    print("=" * 70)
    
    analyzer = AdvancedAlgorithmicAnalyzer({
        'cache_enabled': True,
        'detailed_logging': True
    })
    
    try:
        result = analyzer.analyze_song("Demo Artist", "Advanced Analysis Demo", sample_lyrics)
        
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
        print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result.confidence:.3f}")
        print(f"‚ö° –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.processing_time:.3f}s")
        
        # –†–∏—Ñ–º—ã –∏ –∑–≤—É—á–∞–Ω–∏–µ
        rhyme_analysis = result.raw_output.get('rhyme_analysis', {})
        print(f"\nüéµ –†–ò–§–ú–´ –ò –ó–í–£–ß–ê–ù–ò–ï:")
        print(f"  –°—Ö–µ–º–∞ —Ä–∏—Ñ–º–æ–≤–∫–∏: {rhyme_analysis.get('rhyme_scheme', 'N/A')}")
        print(f"  –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∏—Ñ–º: {rhyme_analysis.get('rhyme_density', 0):.3f}")
        print(f"  –ê–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏—è: {rhyme_analysis.get('alliteration_score', 0):.3f}")
        print(f"  –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∏—Ñ–º—ã: {rhyme_analysis.get('internal_rhymes', 0)}")
        
        # Flow –∞–Ω–∞–ª–∏–∑
        flow_analysis = result.raw_output.get('flow_analysis', {})
        print(f"\nüåä FLOW –ò –†–ò–¢–ú:")
        print(f"  –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≥–æ–≤: {flow_analysis.get('syllable_consistency', 0):.3f}")
        print(f"  –°—Ä. —Å–ª–æ–≥–æ–≤ –Ω–∞ —Å—Ç—Ä–æ–∫—É: {flow_analysis.get('average_syllables_per_line', 0):.1f}")
        print(f"  –†–∏—Ç–º–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {flow_analysis.get('rhythmic_density', 0):.3f}")
        
        # –ß–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å
        readability = result.raw_output.get('readability_metrics', {})
        print(f"\nüìö –ß–ò–¢–ê–ë–ï–õ–¨–ù–û–°–¢–¨:")
        print(f"  Flesch Reading Ease: {readability.get('flesch_reading_ease', 0):.1f}")
        print(f"  SMOG Index: {readability.get('smog_index', 0):.1f}")
        print(f"  –ö–æ–Ω—Å–µ–Ω—Å—É—Å: {readability.get('readability_consensus', 'N/A')}")
        
        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        composite = result.raw_output.get('composite_scores', {})
        print(f"\nüèÜ –ö–û–ú–ü–û–ó–ò–¢–ù–´–ï –û–¶–ï–ù–ö–ò:")
        print(f"  –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ: {composite.get('technical_mastery', 0):.3f}")
        print(f"  –ê—Ä—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —É—Ç–æ–Ω—á—ë–Ω–Ω–æ—Å—Ç—å: {composite.get('artistic_sophistication', 0):.3f}")
        print(f"  –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {composite.get('overall_quality', 0):.3f}")
        print(f"  –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å: {composite.get('innovation_score', 0):.3f}")
        
        print("=" * 70)
        print("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()

# –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PostgreSQL
class PostgreSQLAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PostgreSQL –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PostgreSQL –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.analyzer = AdvancedAlgorithmicAnalyzer({
            'cache_enabled': True,
            'detailed_logging': False
        })
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è PostgreSQL (–±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ config.yaml)
        self.db_config = self._load_db_config()
    
    def _load_db_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ë–î"""
        try:
            import yaml
            config_path = Path(__file__).parent.parent.parent / "config.yaml"
            
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    return config.get('database', {})
            else:
                print("‚ö†Ô∏è –§–∞–π–ª config.yaml –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                return {
                    'host': 'localhost',
                    'port': 5432,
                    'name': 'rap_lyrics_db',
                    'user': 'postgres',
                    'password': 'password'
                }
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {}
    
    async def get_database_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            import asyncpg
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            conn = await asyncpg.connect(
                host=self.db_config.get('host', 'localhost'),
                port=self.db_config.get('port', 5432),
                database=self.db_config.get('name', 'rap_lyrics_db'),
                user=self.db_config.get('user', 'postgres'),
                password=self.db_config.get('password', 'password')
            )
            
            try:
                # –ó–∞–ø—Ä–æ—Å –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                total_stats_query = """
                SELECT 
                    COUNT(*) as total_songs,
                    COUNT(DISTINCT artist) as unique_artists,
                    COUNT(CASE WHEN lyrics IS NOT NULL THEN 1 END) as songs_with_lyrics,
                    COUNT(CASE WHEN lyrics IS NOT NULL AND LENGTH(lyrics) > 100 THEN 1 END) as analyzable_songs,
                    COUNT(CASE WHEN lyrics IS NULL OR LENGTH(lyrics) <= 100 THEN 1 END) as non_analyzable_songs
                FROM tracks;
                """
                
                # –ó–∞–ø—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Å–µ–Ω —Å —Ç–µ–∫—Å—Ç–∞–º–∏)
                lyrics_stats_query = """
                SELECT 
                    AVG(LENGTH(lyrics)) as avg_lyrics_length,
                    MIN(LENGTH(lyrics)) as min_lyrics_length,
                    MAX(LENGTH(lyrics)) as max_lyrics_length,
                    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY LENGTH(lyrics)) as median_lyrics_length
                FROM tracks 
                WHERE lyrics IS NOT NULL;
                """
                
                total_result = await conn.fetchrow(total_stats_query)
                lyrics_result = await conn.fetchrow(lyrics_stats_query)
                
                stats = {
                    'total_songs': total_result['total_songs'],
                    'unique_artists': total_result['unique_artists'],
                    'songs_with_lyrics': total_result['songs_with_lyrics'],
                    'analyzable_songs': total_result['analyzable_songs'],
                    'non_analyzable_songs': total_result['non_analyzable_songs'],
                    'avg_lyrics_length': float(lyrics_result['avg_lyrics_length']) if lyrics_result['avg_lyrics_length'] else 0,
                    'min_lyrics_length': lyrics_result['min_lyrics_length'],
                    'max_lyrics_length': lyrics_result['max_lyrics_length'],
                    'median_lyrics_length': float(lyrics_result['median_lyrics_length']) if lyrics_result['median_lyrics_length'] else 0
                }
                
                print(f"üìä –ü–û–õ–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•:")
                print(f"=" * 50)
                print(f"  üìÄ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –ë–î: {stats['total_songs']:,}")
                print(f"  üé§ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π: {stats['unique_artists']:,}")
                print(f"  üìù –ü–µ—Å–µ–Ω —Å —Ç–µ–∫—Å—Ç–∞–º–∏: {stats['songs_with_lyrics']:,}")
                print(f"  ‚úÖ –ü–µ—Å–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (>100 —Å–∏–º–≤–æ–ª–æ–≤): {stats['analyzable_songs']:,}")
                print(f"  ‚ùå –ù–µ–ø—Ä–∏–≥–æ–¥–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {stats['non_analyzable_songs']:,}")
                print(f"")
                print(f"üìè –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –î–õ–ò–ù–ï –¢–ï–ö–°–¢–û–í:")
                print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {stats['avg_lyrics_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"  –ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {stats['median_lyrics_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: {stats['min_lyrics_length']:,} - {stats['max_lyrics_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
                if stats['total_songs'] > 0:
                    lyrics_percent = (stats['songs_with_lyrics'] / stats['total_songs']) * 100
                    analyzable_percent = (stats['analyzable_songs'] / stats['total_songs']) * 100
                    print(f"")
                    print(f"üìä –ü–†–û–¶–ï–ù–¢–ù–´–ï –°–û–û–¢–ù–û–®–ï–ù–ò–Ø:")
                    print(f"  –ü–µ—Å–µ–Ω —Å —Ç–µ–∫—Å—Ç–∞–º–∏: {lyrics_percent:.1f}%")
                    print(f"  –ü—Ä–∏–≥–æ–¥–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {analyzable_percent:.1f}%")
                
                return stats
                
            finally:
                await conn.close()
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ë–î: {e}")
            return {}
    
    async def analyze_all_songs(self, limit: Optional[int] = None, batch_size: int = 100) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –ø–µ—Å–µ–Ω –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            import asyncpg
            
            conn = await asyncpg.connect(
                host=self.db_config.get('host', 'localhost'),
                port=self.db_config.get('port', 5432),
                database=self.db_config.get('name', 'rap_lyrics_db'),
                user=self.db_config.get('user', 'postgres'),
                password=self.db_config.get('password', 'password')
            )
            
            try:
                # –ó–∞–ø—Ä–æ—Å –ø–µ—Å–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                limit_clause = f"LIMIT {limit}" if limit else ""
                query = f"""
                SELECT id, artist, title, lyrics 
                FROM tracks 
                WHERE lyrics IS NOT NULL AND LENGTH(lyrics) > 100
                ORDER BY id
                {limit_clause};
                """
                
                tracks = await conn.fetch(query)
                total_songs = len(tracks)
                
                print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ {total_songs:,} –ø–µ—Å–µ–Ω (–±–∞—Ç—á–∏ –ø–æ {batch_size})")
                
                processed = 0
                results = []
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏
                for i in range(0, total_songs, batch_size):
                    batch = tracks[i:i + batch_size]
                    batch_results = []
                    
                    for song in batch:
                        try:
                            # –ê–Ω–∞–ª–∏–∑ –ø–µ—Å–Ω–∏
                            result = self.analyzer.analyze_song(
                                artist=song['artist'],
                                title=song['title'],
                                lyrics=song['lyrics']
                            )
                            
                            batch_results.append({
                                'song_id': song['id'],
                                'artist': song['artist'], 
                                'title': song['title'],
                                'analysis': result.raw_output,
                                'confidence': result.confidence,
                                'processing_time': result.processing_time
                            })
                            
                            processed += 1
                            
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Å–Ω–∏ {song['id']}: {e}")
                    
                    results.extend(batch_results)
                    
                    # –ü—Ä–æ–≥—Ä–µ—Å—Å
                    progress = (processed / total_songs) * 100
                    print(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed:,}/{total_songs:,} ({progress:.1f}%)")
                
                print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed:,} –ø–µ—Å–µ–Ω")
                
                return {
                    'total_processed': processed,
                    'results': results,
                    'summary_stats': self._calculate_summary_stats(results)
                }
                
            finally:
                await conn.close()
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Å–µ–Ω: {e}")
            return {}
    
    async def analyze_single_track(self, track_id: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–µ—Å–Ω–∏ –ø–æ ID"""
        try:
            import asyncpg
            
            conn = await asyncpg.connect(
                host=self.db_config.get('host', 'localhost'),
                port=self.db_config.get('port', 5432),
                database=self.db_config.get('name', 'rap_lyrics_db'),
                user=self.db_config.get('user', 'postgres'),
                password=self.db_config.get('password', 'password')
            )
            
            try:
                # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Å–Ω—é
                query = "SELECT id, artist, title, lyrics FROM tracks WHERE id = $1"
                song = await conn.fetchrow(query, track_id)
                
                if not song:
                    print(f"‚ùå –ü–µ—Å–Ω—è —Å ID {track_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                    return {}
                
                if not song['lyrics'] or len(song['lyrics']) < 100:
                    print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (ID: {track_id})")
                    return {}
                
                print(f"üéµ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {song['artist']} - {song['title']}")
                
                # –ê–Ω–∞–ª–∏–∑
                result = self.analyzer.analyze_song(
                    artist=song['artist'],
                    title=song['title'], 
                    lyrics=song['lyrics']
                )
                
                # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                self._print_analysis_results(result)
                
                return {
                    'song_id': song['id'],
                    'artist': song['artist'],
                    'title': song['title'],
                    'analysis': result.raw_output,
                    'confidence': result.confidence,
                    'processing_time': result.processing_time
                }
                
            finally:
                await conn.close()
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–∫–∞ {track_id}: {e}")
            return {}
    
    def _calculate_summary_stats(self, results: List[Dict]) -> Dict[str, Any]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not results:
            return {}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        confidences = [r['confidence'] for r in results]
        processing_times = [r['processing_time'] for r in results]
        
        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        technical_scores = []
        artistic_scores = []
        
        for result in results:
            composite = result.get('analysis', {}).get('composite_scores', {})
            if composite:
                technical_scores.append(composite.get('technical_mastery', 0))
                artistic_scores.append(composite.get('artistic_sophistication', 0))
        
        return {
            'avg_confidence': sum(confidences) / len(confidences),
            'avg_processing_time': sum(processing_times) / len(processing_times),
            'avg_technical_mastery': sum(technical_scores) / len(technical_scores) if technical_scores else 0,
            'avg_artistic_sophistication': sum(artistic_scores) / len(artistic_scores) if artistic_scores else 0,
            'total_results': len(results)
        }
    
    def _print_analysis_results(self, result: 'AnalysisResult'):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
        print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result.confidence:.3f}")
        print(f"‚ö° –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.processing_time:.3f}s")
        
        # –†–∏—Ñ–º—ã –∏ –∑–≤—É—á–∞–Ω–∏–µ
        rhyme_analysis = result.raw_output.get('rhyme_analysis', {})
        if rhyme_analysis:
            print(f"\nüéµ –†–ò–§–ú–´ –ò –ó–í–£–ß–ê–ù–ò–ï:")
            print(f"  –°—Ö–µ–º–∞ —Ä–∏—Ñ–º–æ–≤–∫–∏: {rhyme_analysis.get('rhyme_scheme', 'N/A')}")
            print(f"  –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∏—Ñ–º: {rhyme_analysis.get('rhyme_density', 0):.3f}")
            print(f"  –ê–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏—è: {rhyme_analysis.get('alliteration_score', 0):.3f}")
            print(f"  –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∏—Ñ–º—ã: {rhyme_analysis.get('internal_rhymes', 0)}")
        
        # Flow –∞–Ω–∞–ª–∏–∑
        flow_analysis = result.raw_output.get('flow_analysis', {})
        if flow_analysis:
            print(f"\nüåä FLOW –ò –†–ò–¢–ú:")
            print(f"  –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≥–æ–≤: {flow_analysis.get('syllable_consistency', 0):.3f}")
            print(f"  –°—Ä. —Å–ª–æ–≥–æ–≤ –Ω–∞ —Å—Ç—Ä–æ–∫—É: {flow_analysis.get('average_syllables_per_line', 0):.1f}")
            print(f"  –†–∏—Ç–º–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {flow_analysis.get('rhythmic_density', 0):.3f}")
        
        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        composite = result.raw_output.get('composite_scores', {})
        if composite:
            print(f"\nüèÜ –ö–û–ú–ü–û–ó–ò–¢–ù–´–ï –û–¶–ï–ù–ö–ò:")
            print(f"  –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ: {composite.get('technical_mastery', 0):.3f}")
            print(f"  –ê—Ä—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —É—Ç–æ–Ω—á—ë–Ω–Ω–æ—Å—Ç—å: {composite.get('artistic_sophistication', 0):.3f}")
            print(f"  –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {composite.get('overall_quality', 0):.3f}")
            print(f"  –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å: {composite.get('innovation_score', 0):.3f}")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PostgreSQL"""
    import argparse
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(description='–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è PostgreSQL')
    
    parser.add_argument('--stats', action='store_true', 
                       help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--analyze-all', action='store_true',
                       help='–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Ç—Ä–µ–∫–∏ –≤ –±–∞–∑–µ')
    parser.add_argument('--analyze-track', type=int, metavar='ID',
                       help='–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç—Ä–µ–∫ –ø–æ ID')
    parser.add_argument('--limit', type=int, metavar='N',
                       help='–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('--batch-size', type=int, default=100, metavar='N',
                       help='–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 100)')
    parser.add_argument('--demo', action='store_true',
                       help='–ó–∞–ø—É—Å—Ç–∏—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞')
    
    args = parser.parse_args()
    
    # –ï—Å–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –º–µ–Ω—é
    action_args = [args.stats, args.analyze_all, args.analyze_track, args.demo]
    if not any(action_args):
        print()
        print("üßÆ –ü–†–û–î–í–ò–ù–£–¢–´–ô –ê–õ–ì–û–†–ò–¢–ú–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –î–õ–Ø POSTGRESQL")
        print("=" * 65)
        print("üéØ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ AI –º–æ–¥–µ–ª–µ–π")
        print("‚ö° –†–∞–±–æ—Ç–∞ —Å 57K+ —Ç—Ä–µ–∫–æ–≤ –≤ PostgreSQL –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        print("üìä –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: —Ä–∏—Ñ–º—ã, flow, —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å, —ç–º–æ—Ü–∏–∏")
        print()
        print("üñ•Ô∏è CLI –ò–ù–¢–ï–†–§–ï–ô–°:")
        print("  --stats              üìä –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ë–î")
        print("  --analyze-all        üöÄ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Ç—Ä–µ–∫–∏")
        print("  --analyze-track ID   üéµ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç—Ä–µ–∫")
        print("  --limit N            üî¢ –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–∫–æ–≤")
        print("  --batch-size N       üì¶ –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        print("  --demo               üé≠ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞")
        print("  --help               ‚ùì –ü–æ–ª–Ω–∞—è —Å–ø—Ä–∞–≤–∫–∞")
        print()
        print("üí° –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø:")
        print("  python src/analyzers/algorithmic_analyzer.py --stats")
        print("  python src/analyzers/algorithmic_analyzer.py --analyze-all --limit 100")
        print("  python src/analyzers/algorithmic_analyzer.py --analyze-track 123")
        print("  python src/analyzers/algorithmic_analyzer.py --demo")
        print()
        print("üìà –ú–ï–¢–†–ò–ö–ò –ê–ù–ê–õ–ò–ó–ê:")
        print("  üéµ –†–∏—Ñ–º—ã: —Å—Ö–µ–º–∞, –ø–ª–æ—Ç–Ω–æ—Å—Ç—å, —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")
        print("  üåä Flow: –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≥–æ–≤, —Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å")
        print("  üìö –ß–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å: Flesch, SMOG, ARI –∏–Ω–¥–µ–∫—Å—ã")
        print("  üí≠ –≠–º–æ—Ü–∏–∏: –≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å, –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å, —Å–ª–æ–∂–Ω–æ—Å—Ç—å")
        print("  üé® –¢–µ–º—ã: –¥–µ–Ω—å–≥–∏, —É–ª–∏—Ü–∞, —É—Å–ø–µ—Ö, –æ—Ç–Ω–æ—à–µ–Ω–∏—è")
        print("=" * 65)
        return
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –æ–Ω –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω—É–∂–µ–Ω
    try:
        if args.demo:
            print("üöÄ –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...")
            await demo_advanced_analysis()
            
        else:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ë–î
            print("‚úÖ PostgreSQL –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            analyzer = PostgreSQLAnalyzer()
            
            if args.stats:
                print("üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
                await analyzer.get_database_stats()
                
            elif args.analyze_track:
                print(f"üéµ –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–∫–∞ ID: {args.analyze_track}")
                await analyzer.analyze_single_track(args.analyze_track)
                
            elif args.analyze_all:
                print("üöÄ –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ç—Ä–µ–∫–æ–≤...")
                results = await analyzer.analyze_all_songs(
                    limit=args.limit,
                    batch_size=args.batch_size
                )
                
                if results:
                    summary = results.get('summary_stats', {})
                    print(f"\nüìà –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç—Ä–µ–∫–æ–≤: {summary.get('total_results', 0):,}")
                    print(f"  –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {summary.get('avg_confidence', 0):.3f}")
                    print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {summary.get('avg_processing_time', 0):.3f}s")
                    print(f"  –°—Ä–µ–¥–Ω–µ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ: {summary.get('avg_technical_mastery', 0):.3f}")
                    print(f"  –°—Ä–µ–¥–Ω—è—è –∞—Ä—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —É—Ç–æ–Ω—á—ë–Ω–Ω–æ—Å—Ç—å: {summary.get('avg_artistic_sophistication', 0):.3f}")
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())