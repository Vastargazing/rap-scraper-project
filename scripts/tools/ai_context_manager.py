#!/usr/bin/env python3
"""
ü§ñ AI Context Manager ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –≤ VS Code

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á
- –ü—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (1-5)
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è VS Code workspace —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
- –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –¥–ª—è AI –æ legacy –∫–æ–¥–µ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
python scripts/tools/ai_context_manager.py --task debug

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- Python 3.8+
- dataclasses, pathlib

–†–ï–ó–£–õ–¨–¢–ê–¢:
- –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è AI
- –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ legacy –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Ä–∏—Å–∫–∞—Ö

–ê–í–¢–û–†: Vastargazing
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""

import json
import os
import subprocess
from pathlib import Path
from typing import Dict, List, Set, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import re

@dataclass
class FileContext:
    path: str
    priority: int  # 1-5, –≥–¥–µ 5 = –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–Ω—ã–π
    category: str  # database, analyzer, cli, config, test, legacy
    description: str
    last_modified: str
    size_lines: int
    dependencies: List[str]
    
@dataclass
class AIPromptContext:
    task_type: str  # debug, develop, analyze, refactor
    relevant_files: List[str]
    context_summary: str
    suggested_commands: List[str]
    warnings: List[str]

class AIContextManager:
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ results/ —Å–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø—Ä–æ–µ–∫—Ç–∞
        self.results_dir = self.project_root / "results"
        self.results_dir.mkdir(exist_ok=True)
        self.context_file = self.results_dir / ".ai_context.json"
        self.file_contexts = {}
        self.load_context()
        
    def load_context(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π"""
        if self.context_file.exists():
            with open(self.context_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.file_contexts = {
                    path: FileContext(**ctx) for path, ctx in data.items()
                }
        else:
            self._build_initial_context()
    
    def save_context(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ —Ñ–∞–π–ª"""
        data = {
            path: asdict(ctx) for path, ctx in self.file_contexts.items()
        }
        with open(self.context_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    def _build_initial_context(self):
        """–°—Ç—Ä–æ–∏—Ç –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞"""
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ —Ñ–∞–π–ª—ã
        critical_files = {
            "main.py": FileContext(
                path="main.py",
                priority=5,
                category="cli",
                description="–ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è",
                last_modified="",
                size_lines=0,
                dependencies=["src.analyzers", "src.cli", "src.models"]
            ),
            "src/database/postgres_adapter.py": FileContext(
                path="src/database/postgres_adapter.py",
                priority=5,
                category="database",
                description="PostgreSQL –∞–¥–∞–ø—Ç–µ—Ä, connection pooling, async –æ–ø–µ—Ä–∞—Ü–∏–∏",
                last_modified="",
                size_lines=0,
                dependencies=["asyncpg", "psycopg2"]
            ),
            "config.yaml": FileContext(
                path="config.yaml",
                priority=4,
                category="config",
                description="–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã",
                last_modified="",
                size_lines=0,
                dependencies=[]
            ),
            "docs/claude.md": FileContext(
                path="docs/claude.md",
                priority=5,
                category="docs",
                description="–û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤",
                last_modified="",
                size_lines=0,
                dependencies=[]
            )
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
        analyzer_dir = self.project_root / "src" / "analyzers"
        if analyzer_dir.exists():
            for analyzer_file in analyzer_dir.glob("*.py"):
                if analyzer_file.name != "__init__.py":
                    self.file_contexts[str(analyzer_file)] = FileContext(
                        path=str(analyzer_file),
                        priority=3,
                        category="analyzer",
                        description=f"AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {analyzer_file.stem}",
                        last_modified="",
                        size_lines=0,
                        dependencies=["src.models"]
                    )
        
        self.file_contexts.update(critical_files)
        self._update_file_stats()
        self.save_context()
    
    def _update_file_stats(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∞–π–ª–æ–≤"""
        for path, context in self.file_contexts.items():
            file_path = Path(path)
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        context.size_lines = len(lines)
                    
                    context.last_modified = datetime.fromtimestamp(
                        file_path.stat().st_mtime
                    ).strftime("%Y-%m-%d %H:%M")
                except:
                    pass
    
    def generate_ai_context(self, task_type: str, query: str = "") -> AIPromptContext:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        
        relevant_files = self._select_relevant_files(task_type, query)
        context_summary = self._generate_context_summary(relevant_files, task_type)
        commands = self._suggest_commands(task_type, relevant_files)
        warnings = self._generate_warnings(task_type, relevant_files)
        
        return AIPromptContext(
            task_type=task_type,
            relevant_files=relevant_files,
            context_summary=context_summary,
            suggested_commands=commands,
            warnings=warnings
        )
    
    def _select_relevant_files(self, task_type: str, query: str) -> List[str]:
        """–í—ã–±–∏—Ä–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –∑–∞–¥–∞—á–∏"""
        
        # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Ñ–∞–π–ª—ã
        always_include = [
            path for path, ctx in self.file_contexts.items() 
            if ctx.priority >= 4
        ]
        
        task_specific = []
        
        if task_type == "debug":
            # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ –≤–∞–∂–Ω—ã –ª–æ–≥–∏, —Ç–µ—Å—Ç—ã, –≥–ª–∞–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            task_specific = [
                path for path, ctx in self.file_contexts.items()
                if ctx.category in ["database", "analyzer", "cli"] or "test" in path.lower()
            ]
        
        elif task_type == "develop":
            # –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤–∞–∂–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
            task_specific = [
                path for path, ctx in self.file_contexts.items()
                if ctx.category in ["analyzer", "cli", "models"]
            ]
        
        elif task_type == "analyze":
            # –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–Ω—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã –∏ –ë–î
            task_specific = [
                path for path, ctx in self.file_contexts.items()
                if ctx.category in ["analyzer", "database"]
            ]
        
        elif task_type == "refactor":
            # –î–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –≤–∞–∂–Ω–æ –≤–∏–¥–µ—Ç—å –≤—Å—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
            task_specific = list(self.file_contexts.keys())
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∑–∞–ø—Ä–æ—Å–µ
        if query:
            query_lower = query.lower()
            query_specific = [
                path for path, ctx in self.file_contexts.items()
                if any(word in path.lower() or word in ctx.description.lower() 
                       for word in query_lower.split())
            ]
            task_specific.extend(query_specific)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        all_files = list(set(always_include + task_specific))
        all_files.sort(key=lambda x: self.file_contexts[x].priority, reverse=True)
        
        return all_files[:15]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 15 —Ñ–∞–π–ª–æ–≤
    
    def _generate_context_summary(self, files: List[str], task_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        categories = {}
        for file_path in files:
            ctx = self.file_contexts[file_path]
            categories.setdefault(ctx.category, []).append(ctx)
        
        summary_parts = [
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–¥–∞—á–∏: {task_type.upper()}",
            f"–§–∞–π–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: {len(files)}"
        ]
        
        for category, contexts in categories.items():
            files_in_category = len(contexts)
            high_priority = sum(1 for ctx in contexts if ctx.priority >= 4)
            summary_parts.append(
                f"- {category}: {files_in_category} —Ñ–∞–π–ª–æ–≤ ({high_priority} –≤—ã—Å–æ–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)"
            )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∑–∞–¥–∞—á–∏
        if task_type == "debug":
            summary_parts.append("–§–æ–∫—É—Å: –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º, –∞–Ω–∞–ª–∏–∑ –ª–æ–≥–∏–∫–∏, –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
        elif task_type == "develop":
            summary_parts.append("–§–æ–∫—É—Å: —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è")
        elif task_type == "analyze":
            summary_parts.append("–§–æ–∫—É—Å: –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å AI –º–æ–¥–µ–ª–µ–π")
        elif task_type == "refactor":
            summary_parts.append("–§–æ–∫—É—Å: —É–ª—É—á—à–µ–Ω–∏–µ –∫–æ–¥–∞, —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞")
        
        return "\n".join(summary_parts)
    
    def _suggest_commands(self, task_type: str, files: List[str]) -> List[str]:
        """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –∑–∞–¥–∞—á–∏"""
        
        base_commands = [
            "python main.py --info  # –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã",
            "python main.py --test  # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"
        ]
        
        if task_type == "debug":
            return base_commands + [
                "python check_stats.py  # PostgreSQL —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", 
                "python check_overlap.py  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö",
                "grep -r 'ERROR\\|Exception' src/ --include='*.py'  # –ü–æ–∏—Å–∫ –æ—à–∏–±–æ–∫",
                "docker-compose logs rap-analyzer-api  # –õ–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ (–µ—Å–ª–∏ Docker)"
            ]
        
        elif task_type == "develop":
            return base_commands + [
                "python main.py --analyze 'test text' --analyzer hybrid  # –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞",
                "python main.py --benchmark  # –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
                "find src/ -name '*.py' | head -10  # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤",
                "pytest tests/ -v  # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤"
            ]
        
        elif task_type == "analyze":
            return base_commands + [
                "python scripts/mass_qwen_analysis.py --test  # –¢–µ—Å—Ç –º–∞—Å—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
                "python scripts/db_browser.py  # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä –ë–î",
                "python main.py --benchmark  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"
            ]
        
        elif task_type == "refactor":
            return base_commands + [
                "python scripts/ai_code_audit.py  # –ê—É–¥–∏—Ç –∫–æ–¥–∞ (–µ—Å–ª–∏ —Å–æ–∑–¥–∞–Ω)",
                "grep -r 'TODO\\|FIXME' src/ --include='*.py'  # –ù–∞–π—Ç–∏ –º–µ—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è",
                "find src/ -name '*.py' -exec wc -l {} + | sort -nr  # –ù–∞–π—Ç–∏ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã"
            ]
        
        return base_commands
    
    def _generate_warnings(self, task_type: str, files: List[str]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –¥–ª—è AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        
        warnings = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ legacy —Ñ–∞–π–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        legacy_files = [
            f for f in files 
            if self.file_contexts[f].category == "legacy"
        ]
        if legacy_files:
            warnings.append(
                f"‚ö†Ô∏è –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ {len(legacy_files)} legacy —Ñ–∞–π–ª–æ–≤. "
                "–ò—Å–ø–æ–ª—å–∑—É–π –∏—Ö —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏, –Ω–µ –∫–æ–ø–∏—Ä—É–π —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã."
            )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—É—é —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å
        if task_type == "develop":
            has_database = any(
                self.file_contexts[f].category == "database" for f in files
            )
            has_models = any(
                self.file_contexts[f].category == "models" for f in files
            )
            if not has_database or not has_models:
                warnings.append(
                    "‚ö†Ô∏è –ù–µ–ø–æ–ª–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. "
                    "–£–±–µ–¥–∏—Å—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å PostgreSQL (–Ω–µ SQLite) –∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏."
                )
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –¥–ª—è PostgreSQL –ø—Ä–æ–µ–∫—Ç–∞
        warnings.extend([
            "üîÑ –ü—Ä–æ–µ–∫—Ç –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω –Ω–∞ PostgreSQL. –ò–∑–±–µ–≥–∞–π sqlite3 –∏–º–ø–æ—Ä—Ç–æ–≤.",
            "üèóÔ∏è –ò—Å–ø–æ–ª—å–∑—É–π main.py –∫–∞–∫ –µ–¥–∏–Ω—É—é —Ç–æ—á–∫—É –≤—Ö–æ–¥–∞, –Ω–µ –∑–∞–ø—É—Å–∫–∞–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞–ø—Ä—è–º—É—é.",
            "üß™ –í—Å–µ–≥–¥–∞ —Ç–µ—Å—Ç–∏—Ä—É–π –∏–∑–º–µ–Ω–µ–Ω–∏—è —á–µ—Ä–µ–∑ python main.py --test."
        ])
        
        return warnings
    
    def create_ai_workspace_file(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–ª—è AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        
        workspace_data = {
            "name": "Rap Scraper AI Workspace",
            "description": "PostgreSQL-powered rap lyrics analysis with microservices architecture",
            "version": "2.0.0-postgresql",
            "architecture": {
                "type": "microservices",
                "database": "PostgreSQL 15",
                "entry_point": "main.py",
                "key_components": [
                    "src/database/postgres_adapter.py",
                    "src/analyzers/",
                    "src/cli/",
                    "src/models/"
                ]
            },
            "ai_context": {
                "priority_files": [
                    {
                        "path": path,
                        "priority": ctx.priority,
                        "category": ctx.category,
                        "description": ctx.description
                    }
                    for path, ctx in self.file_contexts.items()
                    if ctx.priority >= 4
                ],
                "task_workflows": {
                    "debug": "Focus on database connectivity, error handling, integration issues",
                    "develop": "Use microservices patterns, PostgreSQL backend, test-driven development", 
                    "analyze": "Leverage 5 AI analyzers, batch processing, performance optimization",
                    "refactor": "Maintain architectural boundaries, eliminate SQLite legacy, DRY principles"
                }
            },
            "warnings": [
                "Project migrated from SQLite to PostgreSQL - avoid legacy patterns",
                "Use main.py unified interface instead of direct component calls",
                "Test all changes with comprehensive test suite",
                "Maintain microservices architectural boundaries"
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º workspace –≤ results/ —Å–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
        workspace_file = self.results_dir / "ai_workspace.json"
        
        with open(workspace_file, 'w', encoding='utf-8') as f:
            json.dump(workspace_data, f, indent=2, ensure_ascii=False)
        
        return str(workspace_file)

def create_ai_friendly_readme() -> str:
    """–°–æ–∑–¥–∞–µ—Ç AI-friendly README —Å –±—ã—Å—Ç—Ä—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    
    ai_readme_content = """# ü§ñ AI Assistant Quick Context

> **CRITICAL:** This project uses PostgreSQL, not SQLite. All legacy SQLite code is archived.

## ‚ö° 30-Second Context
- **Architecture:** Microservices with main.py orchestration
- **Database:** PostgreSQL 15 with connection pooling  
- **AI Models:** 5 analyzers (algorithmic_basic, qwen, ollama, emotion, hybrid)
- **Entry Point:** `python main.py` (unified interface)
- **Data:** 57,718 tracks, 54,170+ analyzed, concurrent processing capable

## üéØ AI Assistant Commands

### Quick Status
```bash
python main.py --info          # Complete system status
python main.py --test          # Validate all components
python check_stats.py          # PostgreSQL database health
```

### Analysis Tasks  
```bash
python main.py --analyze "text" --analyzer qwen     # Single analysis
python main.py --benchmark                          # Performance comparison
python scripts/mass_qwen_analysis.py --test         # Batch analysis test
```

### Development
```bash
python main.py --help          # All available options
pytest tests/ -v               # Run test suite
docker-compose up -d            # Deploy full stack
```

## üö® Critical Reminders for AI

1. **PostgreSQL ONLY** - No sqlite3 imports, use PostgreSQLManager
2. **Unified Interface** - Use main.py, not direct component calls
3. **Microservices** - Respect src/{analyzers,cli,models}/ boundaries
4. **Testing** - Always validate with python main.py --test
5. **Concurrent Processing** - Multiple scripts can run simultaneously

## üìÅ Priority Files for AI Analysis

| File | Priority | Purpose |
|------|----------|---------|
| `docs/claude.md` | üî•üî•üî•üî•üî• | Complete AI context |
| `main.py` | üî•üî•üî•üî•üî• | Central orchestration |  
| `src/database/postgres_adapter.py` | üî•üî•üî•üî•üî• | Database layer |
| `config.yaml` | üî•üî•üî•üî• | System configuration |
| `scripts/mass_qwen_analysis.py` | üî•üî•üî•üî• | Main analysis script |

## ‚ö†Ô∏è Deprecated/Legacy (Reference Only)
- `scripts/archive/` - SQLite legacy code
- `data/data_backup_*.db` - SQLite backups  
- Any file with `_sqlite.py` suffix

---
*Auto-generated for AI assistants. Human-readable docs in README.md*
"""
    
    ai_readme_path = Path("docs") / "AI_README.md"
    ai_readme_path.parent.mkdir(exist_ok=True)
    with open(ai_readme_path, 'w', encoding='utf-8') as f:
        f.write(ai_readme_content)
    
    return str(ai_readme_path)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - —Å–æ–∑–¥–∞–Ω–∏–µ AI-friendly –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    print("ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ AI-friendly —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    context_manager = AIContextManager()
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
    debug_context = context_manager.generate_ai_context("debug")
    develop_context = context_manager.generate_ai_context("develop") 
    analyze_context = context_manager.generate_ai_context("analyze")
    refactor_context = context_manager.generate_ai_context("refactor")
    
    print("üìä –°–æ–∑–¥–∞–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –¥–ª—è –∑–∞–¥–∞—á:")
    print(f"  - Debug: {len(debug_context.relevant_files)} —Ñ–∞–π–ª–æ–≤")
    print(f"  - Develop: {len(develop_context.relevant_files)} —Ñ–∞–π–ª–æ–≤")
    print(f"  - Analyze: {len(analyze_context.relevant_files)} —Ñ–∞–π–ª–æ–≤") 
    print(f"  - Refactor: {len(refactor_context.relevant_files)} —Ñ–∞–π–ª–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º AI workspace
    workspace_file = context_manager.create_ai_workspace_file()
    print(f"üìÅ AI workspace —Å–æ–∑–¥–∞–Ω: results\\ai_workspace.json")
    
    # –°–æ–∑–¥–∞–µ–º AI-friendly README
    ai_readme = create_ai_friendly_readme()
    print(f"üìÑ AI README —Å–æ–∑–¥–∞–Ω: {ai_readme}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ results/
    examples_dir = context_manager.results_dir / "ai_examples"
    examples_dir.mkdir(exist_ok=True)
    
    contexts = {
        "debug": debug_context,
        "develop": develop_context,
        "analyze": analyze_context,
        "refactor": refactor_context
    }
    
    for task_type, context in contexts.items():
        example_file = examples_dir / f"{task_type}_context.json"
        with open(example_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(context), f, indent=2, ensure_ascii=False)
    
    print(f"üí° –ü—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ results\\ai_examples\\")
    
    # –°–æ–∑–¥–∞–µ–º VS Code task –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
    vscode_tasks = {
        "version": "2.0.0",
        "tasks": [
            {
                "label": "AI: Debug Context",
                "type": "shell",
                "command": "python",
                "args": ["scripts/tools/ai_context_manager.py", "debug"],
                "group": "build",
                "problemMatcher": []
            },
            {
                "label": "AI: Analysis Context", 
                "type": "shell",
                "command": "python",
                "args": ["scripts/tools/ai_context_manager.py", "analyze"],
                "group": "build",
                "problemMatcher": []
            },
            {
                "label": "AI: System Status",
                "type": "shell", 
                "command": "python",
                "args": ["main.py", "--info"],
                "group": "build",
                "problemMatcher": []
            }
        ]
    }
    
    vscode_dir = Path(".vscode")
    vscode_dir.mkdir(exist_ok=True)
    
    with open(vscode_dir / "tasks.json", 'w', encoding='utf-8') as f:
        json.dump(vscode_tasks, f, indent=2)
    
    print("‚öôÔ∏è VS Code tasks –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤")
    
    print("\n‚úÖ AI-friendly –æ–∫—Ä—É–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ!")
    print("\n–¢–µ–ø–µ—Ä—å AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –º–æ–∂–µ—Ç:")
    print("  1. –ë—ã—Å—Ç—Ä–æ –ø–æ–Ω—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ AI_README.md")
    print("  2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ñ–∞–π–ª—ã –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏")
    print("  3. –ü–æ–ª—É—á–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –∏ –∫–æ–º–∞–Ω–¥—ã")
    print("  4. –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏ —á–µ—Ä–µ–∑ VS Code tasks")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # –†–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏
        task_type = sys.argv[1]
        context_manager = AIContextManager()
        context = context_manager.generate_ai_context(task_type)
        
        print(f"\nüéØ AI –ö–û–ù–¢–ï–ö–°–¢ –î–õ–Ø –ó–ê–î–ê–ß–ò: {task_type.upper()}")
        print("=" * 50)
        print(context.context_summary)
        print(f"\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã ({len(context.relevant_files)}):")
        for file_path in context.relevant_files[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10
            ctx = context_manager.file_contexts[file_path]
            print(f"  üî•{'üî•' * ctx.priority} {Path(file_path).name} - {ctx.description}")
        
        print(f"\n–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫–æ–º–∞–Ω–¥—ã:")
        for cmd in context.suggested_commands:
            print(f"  $ {cmd}")
        
        print(f"\n–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
        for warning in context.warnings:
            print(f"  {warning}")
    else:
        # –ü–æ–ª–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        main()
