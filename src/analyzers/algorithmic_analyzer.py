"""
ðŸ§® Advanced Algorithmic Song Lyrics Analyzer

KEY IMPROVEMENTS:
âœ¨ Phonetic rhyme analysis (instead of simple ending comparison)
ðŸŽ¯ Advanced flow and rhythm analysis
ðŸ§  Semantic theme analysis with extended dictionaries
ðŸ“Š Statistical readability metrics (Flesch, SMOG, ARI)
ðŸŽµ Musicality and alliteration analysis
ðŸ”„ Caching for performance
âš¡ Asynchronous processing of large texts
ðŸ“ˆ Detailed composite metrics

PURPOSE:
- ðŸŽ¯ Professional text analysis without using AI models
- âš¡ Fast processing of large data volumes (57K+ tracks)
- ðŸ“Š Baseline metrics for comparison with AI analyzers
- ðŸ—„ï¸ Production-ready component with PostgreSQL integration
- ðŸ“ˆ Detailed composite quality assessments

USAGE:
ðŸ–¥ï¸ CLI interface:
  python src/analyzers/algorithmic_analyzer.py --stats
  python src/analyzers/algorithmic_analyzer.py --analyze-all --limit 100
  python src/analyzers/algorithmic_analyzer.py --analyze-track 123

ðŸ“ Programmatic interface:
  analyzer = AdvancedAlgorithmicAnalyzer()
  result = analyzer.analyze_song("Artist", "Title", "Lyrics...")

DEPENDENCIES:
- ðŸ Python 3.8+
- ðŸ—„ï¸ PostgreSQL with asyncpg
- ðŸ“„ PyYAML for configuration
- ðŸ”§ src/interfaces/analyzer_interface.py

RESULTS:
- ðŸŽµ Rhyme analysis: scheme, density, phonetic similarity
- ðŸŒŠ Flow metrics: syllable consistency, rhythmic density
- ðŸ“š Readability: Flesch, SMOG, ARI, Coleman-Liau indices
- ðŸ’­ Emotional analysis: valence, intensity, complexity
- ðŸŽ¨ Thematic analysis: money, street, success, relationships
- âœï¸ Literary devices: metaphors, alliteration, repetitions
- ðŸ“Š Composite scores: technical mastery, artistry

FEATURES:
- ðŸ“ˆ PostgreSQL database statistics
- ðŸ” Individual track analysis by ID
- ðŸš€ Mass analysis with progress and batching
- ðŸ’¾ Result caching for performance
- ðŸŽ­ Demo mode with examples

AUTHOR: Vastargazing
VERSION: 2.0.0 Advanced
DATE: September 2025
"""

import asyncio
import hashlib
import logging
import math
import re
import time
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

# Imports with fallback for standalone execution
try:
    from interfaces.analyzer_interface import (
        AnalysisResult,
        BaseAnalyzer,
        register_analyzer,
    )

    PROJECT_IMPORT_SUCCESS = True
except ImportError:
    PROJECT_IMPORT_SUCCESS = False
    import sys
    from pathlib import Path

    # Attempt to find project root and add src to path
    current_dir = Path(__file__).resolve().parent
    possible_roots = [current_dir.parent.parent, current_dir.parent, current_dir]

    for root in possible_roots:
        src_path = root / "src"
        if src_path.exists() and (src_path / "interfaces").exists():
            if str(src_path) not in sys.path:
                sys.path.append(str(src_path))
            try:
                from interfaces.analyzer_interface import (
                    AnalysisResult,
                    BaseAnalyzer,
                    register_analyzer,
                )

                PROJECT_IMPORT_SUCCESS = True
                break
            except ImportError:
                continue

    # If import still failed, create stub classes
    if not PROJECT_IMPORT_SUCCESS:
        # Basic stub classes for standalone operation
        class BaseAnalyzer:
            def __init__(self, config: dict[str, Any] | None = None):
                self.config = config or {}

            def validate_input(self, artist: str, title: str, lyrics: str) -> bool:
                return bool(artist and title and lyrics and len(lyrics.strip()) > 10)

            def preprocess_lyrics(self, lyrics: str) -> str:
                return re.sub(r"\s+", " ", lyrics.strip())

        @dataclass
        class AnalysisResult:
            artist: str
            title: str
            analysis_type: str
            confidence: float
            metadata: dict[str, Any]
            raw_output: dict[str, Any]
            processing_time: float
            timestamp: str

        def register_analyzer(name: str):  # noqa: ARG001
            def decorator(cls):
                return cls

            return decorator


# Logging setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class PhoneticPattern:
    """Phonetic patterns for enhanced rhyme analysis.

    This class provides phonetic mappings and consonant clusters used for
    advanced rhyme detection and analysis. It includes vowel sound groups
    and consonant combinations that are commonly used in phonetic matching.

    Attributes:
        vowel_sounds (dict[str, list[str]]): Mapping of vowel sound groups to
            their phonetic variations. Keys represent base sounds, values are
            lists of similar-sounding variations.
        consonant_clusters (set[str]): Set of common consonant cluster
            combinations used in phonetic analysis.
    """

    vowel_sounds: dict[str, list[str]] = field(
        default_factory=lambda: {
            "ay": ["ai", "ay", "ey", "a_e"],
            "ee": ["ee", "ea", "ie", "y"],
            "oh": ["o", "oa", "ow", "o_e"],
            "oo": ["oo", "u", "ew", "ue"],
            "ah": ["a", "au", "aw"],
            "ih": ["i", "y", "ie"],
            "eh": ["e", "ea", "ai"],
            "uh": ["u", "o", "ou"],
        }
    )

    consonant_clusters: set[str] = field(
        default_factory=lambda: {
            "ch",
            "sh",
            "th",
            "wh",
            "ck",
            "ng",
            "ph",
            "gh",
            "st",
            "sp",
            "sc",
            "sk",
            "sm",
            "sn",
            "sw",
            "sl",
            "bl",
            "br",
            "cl",
            "cr",
            "dr",
            "fl",
            "fr",
            "gl",
            "gr",
            "pl",
            "pr",
            "tr",
            "tw",
        }
    )


class AdvancedLexicon:
    """Extended dictionaries for semantic analysis.

    This class provides comprehensive lexical resources for analyzing rap lyrics,
    including emotional categories with intensity gradation, thematic categories
    specific to rap culture, complexity indicators, and literary devices.

    The lexicon is designed to capture the nuanced emotional and thematic content
    commonly found in hip-hop lyrics, with special attention to street culture,
    success narratives, and artistic expression.

    Attributes:
        emotions (dict[str, dict[str, set[str]]]): Emotional categories with
            intensity levels (weak, medium, strong) containing relevant words.
        rap_themes (dict[str, set[str]]]): Thematic categories specific to rap
            culture including money, street life, success, relationships, etc.
        complexity_indicators (dict[str, set[str]]]): Words indicating
            philosophical, abstract, or analytical thinking.
        literary_devices (dict[str, set[str]]): Words and phrases indicating
            use of literary techniques like metaphors, time references, contrasts.
    """

    def __init__(self):
        # Emotional categories with intensity gradation
        self.emotions = {
            "joy": {
                "weak": {"happy", "good", "nice", "okay", "fine", "content"},
                "medium": {"great", "wonderful", "amazing", "excellent", "fantastic"},
                "strong": {
                    "ecstatic",
                    "euphoric",
                    "blissful",
                    "overjoyed",
                    "triumphant",
                },
            },
            "anger": {
                "weak": {"annoyed", "bothered", "upset", "frustrated", "irritated"},
                "medium": {"angry", "mad", "furious", "pissed", "heated"},
                "strong": {"rage", "wrath", "livid", "enraged", "incensed"},
            },
            "sadness": {
                "weak": {"sad", "down", "blue", "low", "melancholy"},
                "medium": {"depressed", "miserable", "heartbroken", "devastated"},
                "strong": {"suicidal", "hopeless", "despairing", "anguished"},
            },
            "fear": {
                "weak": {"worried", "nervous", "anxious", "concerned", "uneasy"},
                "medium": {"scared", "afraid", "frightened", "terrified"},
                "strong": {"petrified", "horrified", "panic", "dread"},
            },
        }

        # Thematic categories specific to rap
        self.rap_themes = {
            "money_wealth": {
                "money",
                "cash",
                "bread",
                "dough",
                "paper",
                "green",
                "bills",
                "rich",
                "wealth",
                "fortune",
                "bank",
                "account",
                "invest",
                "luxury",
                "expensive",
                "diamond",
                "gold",
                "platinum",
                "mansion",
                "penthouse",
                "yacht",
                "lambo",
                "ferrari",
                "bentley",
            },
            "street_life": {
                "street",
                "hood",
                "block",
                "corner",
                "ghetto",
                "projects",
                "hustle",
                "grind",
                "struggle",
                "survive",
                "real",
                "raw",
                "concrete",
                "pavement",
                "alley",
                "neighborhood",
            },
            "success_ambition": {
                "success",
                "win",
                "victory",
                "champion",
                "boss",
                "king",
                "queen",
                "crown",
                "throne",
                "empire",
                "legend",
                "icon",
                "achieve",
                "accomplish",
                "conquer",
                "dominate",
                "rise",
            },
            "relationships_love": {
                "love",
                "heart",
                "baby",
                "girl",
                "woman",
                "man",
                "relationship",
                "kiss",
                "hug",
                "romance",
                "passion",
                "soul",
                "forever",
                "together",
                "commitment",
                "loyalty",
                "trust",
            },
            "violence_conflict": {
                "gun",
                "shoot",
                "kill",
                "murder",
                "death",
                "blood",
                "war",
                "fight",
                "battle",
                "enemy",
                "revenge",
                "violence",
                "attack",
                "weapon",
                "bullet",
                "trigger",
                "blast",
                "destroy",
            },
            "drugs_party": {
                "weed",
                "smoke",
                "high",
                "drunk",
                "party",
                "club",
                "dance",
                "drink",
                "bottle",
                "shot",
                "pill",
                "lean",
                "molly",
                "celebration",
                "turn_up",
                "lit",
                "wild",
                "crazy",
            },
        }

        # Words indicating complexity of thinking
        self.complexity_indicators = {
            "philosophical": {
                "existence",
                "reality",
                "consciousness",
                "purpose",
                "meaning",
                "truth",
                "wisdom",
                "knowledge",
                "understand",
                "perspective",
                "philosophy",
                "metaphysical",
                "spiritual",
                "transcend",
            },
            "abstract": {
                "concept",
                "theory",
                "principle",
                "ideology",
                "paradigm",
                "dimension",
                "universe",
                "infinity",
                "eternal",
                "essence",
                "phenomenon",
                "manifestation",
                "transformation",
                "evolution",
            },
            "analytical": {
                "analyze",
                "examine",
                "investigate",
                "evaluate",
                "assess",
                "consider",
                "contemplate",
                "reflect",
                "introspect",
                "ponder",
                "calculate",
                "measure",
                "compare",
                "contrast",
                "deduce",
            },
        }

        # Literary devices in texts
        self.literary_devices = {
            "metaphor_indicators": {
                "like",
                "as",
                "than",
                "seems",
                "appears",
                "resembles",
                "similar",
                "reminds",
                "symbolize",
                "represent",
                "embody",
            },
            "time_references": {
                "yesterday",
                "today",
                "tomorrow",
                "past",
                "present",
                "future",
                "memory",
                "remember",
                "forget",
                "history",
                "destiny",
                "fate",
            },
            "contrast_words": {
                "but",
                "however",
                "although",
                "despite",
                "nevertheless",
                "nonetheless",
                "whereas",
                "while",
                "opposite",
                "contrast",
            },
        }


class FlowAnalyzer:
    """Advanced flow and rhythm analyzer for rap lyrics.

    This class provides comprehensive analysis of rhythmic patterns, syllable
    consistency, stress patterns, and flow characteristics in rap lyrics.
    It includes phonetic analysis and advanced metrics for understanding
    the musicality and delivery aspects of rap performance.

    Attributes:
        phonetic_patterns (PhoneticPattern): Instance of PhoneticPattern class
            for accessing vowel sound mappings and consonant clusters.
    """

    def __init__(self):
        """Initialize the FlowAnalyzer with phonetic pattern support."""
        self.phonetic_patterns = PhoneticPattern()

    def analyze_flow_patterns(self, lines: list[str]) -> dict[str, Any]:
        """Analyze comprehensive flow patterns in lyrics.

        Performs detailed analysis of syllable consistency, stress patterns,
        line length variations, and rhythmic density across the given lines.

        Args:
            lines (list[str]): List of lyric lines to analyze.

        Returns:
            dict[str, Any]: Dictionary containing flow analysis metrics:
                - syllable_consistency (float): Consistency of syllable counts (0-1)
                - average_syllables_per_line (float): Mean syllables per line
                - syllable_variance (float): Variance in syllable distribution
                - line_length_consistency (float): Consistency of line lengths (0-1)
                - stress_pattern_regularity (float): Regularity of stress patterns (0-1)
                - flow_breaks (int): Number of flow interruptions
                - rhythmic_density (float): Overall rhythmic density (0-1)

        Raises:
            None: Returns empty result for insufficient data.
        """
        if not lines:
            return self._empty_flow_result()

        syllable_patterns = []
        stress_patterns = []
        line_lengths = []

        for line in lines:
            syllables = self._count_syllables_advanced(line)
            syllable_patterns.append(syllables)
            line_lengths.append(len(line.split()))

            # Stress pattern analysis (simplified)
            stress_pattern = self._analyze_stress_pattern(line)
            stress_patterns.append(stress_pattern)

        return {
            "syllable_consistency": self._calculate_consistency(syllable_patterns),
            "average_syllables_per_line": sum(syllable_patterns)
            / len(syllable_patterns),
            "syllable_variance": self._calculate_variance(syllable_patterns),
            "line_length_consistency": self._calculate_consistency(line_lengths),
            "stress_pattern_regularity": self._analyze_stress_regularity(
                stress_patterns
            ),
            "flow_breaks": self._count_flow_interruptions(lines),
            "rhythmic_density": self._calculate_rhythmic_density(lines),
        }

    def _count_syllables_advanced(self, text: str) -> int:
        """Count syllables in text using advanced phonetic rules.

        Uses enhanced syllable counting algorithm that accounts for special
        cases, diphthongs, and phonetic variations.

        Args:
            text (str): Text to analyze for syllable count.

        Returns:
            int: Total number of syllables in the text.
        """
        words = re.findall(r"\b[a-zA-Z]+\b", text.lower())
        if not words:
            return 0

        total_syllables = 0
        for word in words:
            syllables = self._syllables_in_word(word)
            total_syllables += syllables

        return total_syllables

    def _syllables_in_word(self, word: str) -> int:
        """Count syllables in a single word with phonetic accuracy.

        Implements detailed syllable counting algorithm that handles special
        cases, diphthongs, and English phonetic rules.

        Args:
            word (str): Word to count syllables in.

        Returns:
            int: Number of syllables in the word (minimum 1).
        """
        if len(word) <= 2:
            return 1

        word = word.lower().strip()

        special_cases = {
            "the": 1,
            "a": 1,
            "an": 1,
            "and": 1,
            "or": 1,
            "but": 1,
            "through": 1,
            "though": 1,
            "every": 2,
            "very": 2,
            "people": 2,
            "little": 2,
            "middle": 2,
            "simple": 2,
        }

        if word in special_cases:
            return special_cases[word]

        vowels = "aeiouy"
        syllable_count = 0
        prev_was_vowel = False

        for i, char in enumerate(word):
            is_vowel = char in vowels

            if is_vowel:
                if not prev_was_vowel:
                    syllable_count += 1
                elif i > 0 and word[i - 1 : i + 1] in [
                    "ai",
                    "au",
                    "ea",
                    "ee",
                    "ei",
                    "ie",
                    "oa",
                    "oo",
                    "ou",
                    "ue",
                ]:
                    pass

            prev_was_vowel = is_vowel

        if (
            word.endswith("e")
            and syllable_count > 1
            and not word.endswith(("le", "se", "me", "ne", "ve", "ze", "de", "ge"))
        ):
            syllable_count -= 1

        if word.endswith(("ed", "es", "er", "ly")):
            pass

        return max(1, syllable_count)

    def _analyze_stress_pattern(self, line: str) -> str:
        """Analyze stress pattern in a line (simplified implementation).

        Creates a basic stress pattern representation for rhythm analysis.
        Uses simple heuristics based on word length and common function words.

        Args:
            line (str): Lyric line to analyze for stress pattern.

        Returns:
            str: String representation of stress pattern (e.g., "10101").
        """
        words = line.split()
        if not words:
            return ""

        stress_pattern = []
        for word in words:
            if len(word) <= 3:
                stress_pattern.append("1")
            elif word.lower() in {"the", "and", "but", "for", "with", "from", "into"}:
                stress_pattern.append("0")
            else:
                stress_pattern.append("1")

        return "".join(stress_pattern)

    def _calculate_consistency(self, values: list[float]) -> float:
        """Calculate consistency score for a list of values.

        Measures how consistent the values are around their mean.
        Higher scores indicate more consistent values.

        Args:
            values (list[float]): List of numeric values to analyze.

        Returns:
            float: Consistency score between 0.0 and 1.0.
        """
        if len(values) < 2:
            return 1.0

        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)

        consistency = 1 / (1 + variance)
        return min(consistency, 1.0)

    def _calculate_variance(self, values: list[float]) -> float:
        """Calculate variance of a list of values.

        Computes the statistical variance around the mean.

        Args:
            values (list[float]): List of numeric values.

        Returns:
            float: Variance value (0.0 for identical values).
        """
        if len(values) < 2:
            return 0.0

        mean = sum(values) / len(values)
        return sum((x - mean) ** 2 for x in values) / len(values)

    def _analyze_stress_regularity(self, stress_patterns: list[str]) -> float:
        """Analyze regularity of stress patterns across lines.

        Measures how consistent stress patterns are across multiple lines.
        Higher scores indicate more regular rhythmic patterns.

        Args:
            stress_patterns (list[str]): List of stress pattern strings.

        Returns:
            float: Regularity score between 0.0 and 1.0.
        """
        if not stress_patterns:
            return 0.0

        pattern_counts = Counter(stress_patterns)
        most_common = pattern_counts.most_common(1)

        if most_common:
            return most_common[0][1] / len(stress_patterns)

        return 0.0

    def _count_flow_interruptions(self, lines: list[str]) -> int:
        """Count flow interruptions caused by punctuation.

        Counts punctuation marks that could interrupt the flow of delivery.

        Args:
            lines (list[str]): List of lyric lines to analyze.

        Returns:
            int: Total number of flow-interrupting punctuation marks.
        """
        interruptions = 0
        punctuation = {".", "!", "?", ";", ":", ",", "--", "..."}

        for line in lines:
            for punct in punctuation:
                interruptions += line.count(punct)

        return interruptions

    def _calculate_rhythmic_density(self, lines: list[str]) -> float:
        """Calculate rhythmic density of the lyrics.

        Measures the overall rhythmic intensity based on syllables per word.

        Args:
            lines (list[str]): List of lyric lines to analyze.

        Returns:
            float: Rhythmic density score between 0.0 and 1.0.
        """
        if not lines:
            return 0.0

        total_words = sum(len(line.split()) for line in lines)
        total_syllables = sum(self._count_syllables_advanced(line) for line in lines)

        if total_words == 0:
            return 0.0

        density = total_syllables / total_words
        return min(density / 2.0, 1.0)

    def _empty_flow_result(self) -> dict[str, Any]:
        """Return empty flow analysis result structure.

        Returns:
            dict[str, Any]: Empty result dictionary with all metrics set to zero.
        """
        return {
            "syllable_consistency": 0.0,
            "average_syllables_per_line": 0.0,
            "syllable_variance": 0.0,
            "line_length_consistency": 0.0,
            "stress_pattern_regularity": 0.0,
            "flow_breaks": 0,
            "rhythmic_density": 0.0,
        }


class RhymeAnalyzer:
    """Advanced rhyme analyzer with phonetic pattern analysis and complex rhyme detection.

    This analyzer performs comprehensive rhyme analysis including perfect rhymes,
    near rhymes, internal rhymes, and phonetic similarity scoring. It uses
    advanced algorithms to detect various rhyme schemes and calculate rhyme density.
    The analyzer uses caching to optimize repeated rhyme checks and supports
    multiple rhyme types including perfect, near, internal, and phonetic rhymes.

    Attributes:
        phonetic_patterns (PhoneticPattern): Instance for phonetic analysis.
        rhyme_cache (dict): Cache for storing rhyme check results to improve performance.
    """

    def __init__(self):
        """Initialize the RhymeAnalyzer with phonetic patterns and caching.

        Sets up the phonetic pattern analyzer and initializes the rhyme cache
        for performance optimization during repeated rhyme checks.
        """
        self.phonetic_patterns = PhoneticPattern()
        self.rhyme_cache = {}

    def analyze_rhyme_structure(self, lines: list[str]) -> dict[str, Any]:
        """Perform comprehensive rhyme structure analysis on lyrics.

        Analyzes various types of rhymes including perfect, near, and internal rhymes,
        calculates phonetic similarity, rhyme density, and sound-based literary devices.

        Args:
            lines (list[str]): List of lyric lines to analyze for rhyme patterns.

        Returns:
            dict[str, Any]: Comprehensive rhyme analysis results containing:
                - perfect_rhymes (int): Count of perfect rhyme pairs
                - near_rhymes (int): Count of near rhyme pairs
                - internal_rhymes (int): Count of internal rhymes within lines
                - rhyme_scheme (str): Detected rhyme scheme pattern
                - rhyme_scheme_complexity (float): Complexity score of rhyme scheme
                - phonetic_similarity_score (float): Phonetic similarity between line endings
                - rhyme_density (float): Overall density of rhyming in the text
                - alliteration_score (float): Score for initial consonant repetition
                - assonance_score (float): Score for vowel sound repetition
                - consonance_score (float): Score for consonant sound repetition

        Raises:
            ValueError: If input validation fails (handled internally by returning empty results).

        Note:
            Returns empty result structure if fewer than 2 lines are provided.
            All scores are normalized between 0.0 and 1.0 where applicable.
        """
        if len(lines) < 2:
            return self._empty_rhyme_result()

        line_endings = self._extract_line_endings(lines)

        perfect_rhymes = self._find_perfect_rhymes(line_endings)
        near_rhymes = self._find_near_rhymes(line_endings)
        internal_rhymes = self._find_internal_rhymes(lines)

        rhyme_scheme = self._detect_complex_rhyme_scheme(line_endings)

        phonetic_similarity = self._calculate_phonetic_similarity(line_endings)
        rhyme_density = self._calculate_rhyme_density(
            line_endings, perfect_rhymes, near_rhymes
        )

        return {
            "perfect_rhymes": len(perfect_rhymes),
            "near_rhymes": len(near_rhymes),
            "internal_rhymes": len(internal_rhymes),
            "rhyme_scheme": rhyme_scheme,
            "rhyme_scheme_complexity": self._evaluate_scheme_complexity(rhyme_scheme),
            "phonetic_similarity_score": phonetic_similarity,
            "rhyme_density": rhyme_density,
            "alliteration_score": self._calculate_alliteration(lines),
            "assonance_score": self._calculate_assonance(lines),
            "consonance_score": self._calculate_consonance(lines),
        }

    def _extract_line_endings(self, lines: list[str]) -> list[str]:
        """Extract line endings from lyrics for rhyme analysis.

        Processes each line to extract the last meaningful word, removing punctuation
        and normalizing to lowercase for consistent rhyme comparison.

        Args:
            lines (list[str]): List of lyric lines to process.

        Returns:
            list[str]: List of line endings (last words), empty string for lines
                      with no meaningful words.

        Note:
            Uses regex to find alphabetic words only, ensuring clean endings
            for accurate rhyme detection.
        """
        endings = []
        for line in lines:
            words = re.findall(r"\b[a-zA-Z]+\b", line)
            if words:
                ending = words[-1].lower()
                endings.append(ending)
            else:
                endings.append("")
        return endings

    def _find_perfect_rhymes(self, endings: list[str]) -> list[tuple[int, int]]:
        """Find all perfect rhyme pairs in line endings.

        Identifies pairs of line endings that rhyme perfectly (exact sound match
        for the final syllables).

        Args:
            endings (list[str]): List of line endings to analyze.

        Returns:
            list[tuple[int, int]]: List of tuples (i, j) where i and j are indices
                                   of perfectly rhyming line endings.

        Note:
            Uses _is_perfect_rhyme method for accurate phonetic matching.
        """
        return [
            (i, j)
            for i in range(len(endings))
            for j in range(i + 1, len(endings))
            if self._is_perfect_rhyme(endings[i], endings[j])
        ]

    def _find_near_rhymes(self, endings: list[str]) -> list[tuple[int, int]]:
        """Find all near rhyme pairs in line endings.

        Identifies pairs of line endings that have near rhymes (similar but not
        perfect sound matches, including assonance and consonance).

        Args:
            endings (list[str]): List of line endings to analyze.

        Returns:
            list[tuple[int, int]]: List of tuples (i, j) where i and j are indices
                                   of near-rhyming line endings.

        Note:
            Excludes perfect rhymes, focusing on imperfect but related sounds.
        """
        return [
            (i, j)
            for i in range(len(endings))
            for j in range(i + 1, len(endings))
            if not self._is_perfect_rhyme(endings[i], endings[j])
            and self._is_near_rhyme(endings[i], endings[j])
        ]

    def _find_internal_rhymes(self, lines: list[str]) -> list[tuple[int, str, str]]:
        """Find internal rhymes within lyric lines.

        Searches for rhyming words within individual lines that create internal
        rhyme schemes, adding complexity and musicality to the lyrics.

        Args:
            lines (list[str]): List of lyric lines to analyze for internal rhymes.

        Returns:
            list[tuple[int, str, str]]: List of tuples (line_index, word1, word2)
                                   where word1 and word2 rhyme within the same line.

        Note:
            Internal rhymes occur when words within the same line rhyme with each other,
            creating complex rhythmic patterns common in advanced rap lyrics.
        """

    def _is_perfect_rhyme(self, word1: str, word2: str) -> bool:
        """Check for perfect rhyme with phonetic analysis.

        Determines if two words form a perfect rhyme by comparing their endings
        with phonetic awareness, using caching for performance optimization.

        Args:
            word1 (str): First word to compare for rhyming.
            word2 (str): Second word to compare for rhyming.

        Returns:
            bool: True if words form a perfect rhyme, False otherwise.

        Note:
            Uses multi-length suffix comparison and phonetic checking.
            Results are cached to improve performance on repeated checks.
        """
        if not word1 or not word2 or word1 == word2:
            return False

        cache_key = tuple(sorted([word1, word2]))
        if cache_key in self.rhyme_cache:
            return self.rhyme_cache[cache_key]

        result = False
        for suffix_len in range(2, min(len(word1), len(word2)) + 1):
            if word1[-suffix_len:] == word2[-suffix_len:]:
                result = True
                break

        if not result:
            result = self._phonetic_rhyme_check(word1, word2)

        self.rhyme_cache[cache_key] = result
        return result

    def _is_near_rhyme(self, word1: str, word2: str) -> bool:
        """Check for near rhyme (assonance and consonance).

        Determines if two words form a near rhyme through vowel similarity (assonance)
        or consonant similarity (consonance), excluding perfect rhymes.

        Args:
            word1 (str): First word to compare for near rhyming.
            word2 (str): Second word to compare for near rhyming.

        Returns:
            bool: True if words form a near rhyme, False otherwise.

        Note:
            Near rhymes include assonance (vowel similarity) and consonance
            (consonant similarity) but exclude exact phonetic matches.
        """
        if not word1 or not word2 or len(word1) < 2 or len(word2) < 2:
            return False

        # Check for vowel assonance
        vowels1 = [c for c in word1[-3:] if c in "aeiou"]
        vowels2 = [c for c in word2[-3:] if c in "aeiou"]

        if vowels1 and vowels2 and vowels1[-1] == vowels2[-1]:
            return True

        # Check for consonant consonance
        consonants1 = [c for c in word1[-3:] if c not in "aeiou"]
        consonants2 = [c for c in word2[-3:] if c not in "aeiou"]

        return len(set(consonants1) & set(consonants2)) >= 1

    def _phonetic_rhyme_check(self, word1: str, word2: str) -> bool:
        """Perform phonetic rhyme checking using simplified phonetic groups.

        Uses predefined phonetic groups for common sound patterns to determine
        if two words rhyme based on their phonetic similarity rather than exact spelling.

        Args:
            word1 (str): First word to check for phonetic rhyming.
            word2 (str): Second word to check for phonetic rhyming.

        Returns:
            bool: True if words rhyme phonetically according to the groups, False otherwise.

        Note:
            This is a simplified implementation. A full phonetic matching algorithm
            would be more comprehensive but this covers common English sound patterns.
        """
        # Simplified phonetic check
        # In real implementation, there would be a phonetic matching algorithm

        # Check for similar sounds
        phonetic_groups = {
            "k_sounds": ["c", "k", "ck", "q"],
            "s_sounds": ["s", "c", "z"],
            "f_sounds": ["f", "ph", "gh"],
            "long_a": ["a", "ai", "ay", "ei"],
            "long_e": ["e", "ee", "ea", "ie"],
            "long_i": ["i", "ie", "y", "igh"],
            "long_o": ["o", "oa", "ow", "ough"],
            "long_u": ["u", "ue", "ew", "ou"],
        }

        # Check endings for phonetic similarity
        end1 = word1[-2:]
        end2 = word2[-2:]

        for group in phonetic_groups.values():
            if any(end1.endswith(sound) for sound in group) and any(
                end2.endswith(sound) for sound in group
            ):
                return True

        return False

    def _detect_complex_rhyme_scheme(self, endings: list[str]) -> str:
        """Detect complex rhyme scheme pattern in line endings.

        Analyzes the first 16 line endings to identify rhyme scheme patterns
        by grouping rhyming words and assigning letters to rhyme groups.

        Args:
            endings (list[str]): List of line endings to analyze for rhyme scheme.

        Returns:
            str: Rhyme scheme pattern (e.g., "ABAB", "AABB") or "insufficient"
                 if fewer than 4 endings are provided.

        Note:
            Uses AABBCC... pattern for assigning rhyme group letters.
            Only analyzes first 16 lines for performance and relevance.
        """
        if len(endings) < 4:
            return "insufficient"

        # Take first 16 lines for scheme analysis
        sample = endings[:16]

        # Group rhyming words
        rhyme_groups = {}
        scheme = []
        next_letter = "A"

        for ending in sample:
            assigned_letter = None

            # Look for existing group
            for group_word, letter in rhyme_groups.items():
                if self._is_perfect_rhyme(ending, group_word) or self._is_near_rhyme(
                    ending, group_word
                ):
                    assigned_letter = letter
                    break

            # Create new group if not found
            if assigned_letter is None:
                assigned_letter = next_letter
                rhyme_groups[ending] = next_letter
                next_letter = chr(ord(next_letter) + 1)

            scheme.append(assigned_letter)

        return "".join(scheme)

    def _evaluate_scheme_complexity(self, scheme: str) -> float:
        """Evaluate the complexity of a rhyme scheme pattern.

        Assesses how complex a rhyme scheme is based on unique rhymes and
        bonus points for known complex patterns like ABAB, ABCB, ABBA.

        Args:
            scheme (str): Rhyme scheme pattern (e.g., "ABAB", "AABB").

        Returns:
            float: Complexity score between 0.0 and 1.0, where higher values
                   indicate more complex rhyme schemes.

        Note:
            Returns 0.0 for insufficient data. Uses unique rhyme ratio as base
            with bonuses for recognized complex patterns.
        """
        if not scheme or scheme == "insufficient":
            return 0.0

        # Complexity factors
        unique_rhymes = len(set(scheme))
        total_lines = len(scheme)

        # Pattern search
        patterns = {"ABAB": 0.6, "AABB": 0.4, "ABCB": 0.7, "ABBA": 0.8, "AAAA": 0.2}

        complexity_score = unique_rhymes / total_lines

        # Bonus for known complex patterns
        for pattern, bonus in patterns.items():
            if pattern in scheme:
                complexity_score += bonus * 0.1

        return min(complexity_score, 1.0)

    def _calculate_phonetic_similarity(self, endings: list[str]) -> float:
        """Calculate phonetic similarity score across line endings.

        Computes average phonetic similarity between all pairs of line endings
        to assess overall sound-based consistency in the rhyme scheme.

        Args:
            endings (list[str]): List of line endings to analyze for similarity.

        Returns:
            float: Average phonetic similarity score between 0.0 and 1.0.
                   Returns 0.0 if fewer than 2 endings are provided.

        Note:
            Uses _phonetic_similarity_score for individual pair comparisons.
            Higher scores indicate more phonetically consistent endings.
        """
        if len(endings) < 2:
            return 0.0

        similarity_scores = []

        for i in range(len(endings)):
            for j in range(i + 1, len(endings)):
                score = self._phonetic_similarity_score(endings[i], endings[j])
                similarity_scores.append(score)

        return (
            sum(similarity_scores) / len(similarity_scores)
            if similarity_scores
            else 0.0
        )

    def _phonetic_similarity_score(self, word1: str, word2: str) -> float:
        """Calculate phonetic similarity score between two words.

        Compares word endings of different lengths to determine phonetic similarity,
        assigning higher scores for longer matching suffixes.

        Args:
            word1 (str): First word to compare.
            word2 (str): Second word to compare.

        Returns:
            float: Similarity score between 0.0 and 1.0, where higher values
                   indicate greater phonetic similarity.

        Note:
            Uses suffix matching with weighted scoring (longer matches = higher scores).
            Returns 0.0 for empty or identical words.
        """
        if not word1 or not word2:
            return 0.0

        # Compare endings of different lengths
        max_len = min(len(word1), len(word2), 4)
        similarity = 0.0

        for i in range(1, max_len + 1):
            if word1[-i:] == word2[-i:]:
                similarity += i * 0.25

        return min(similarity, 1.0)

    def _calculate_rhyme_density(
        self, endings: list[str], perfect_rhymes: list, near_rhymes: list
    ) -> float:
        """Calculate rhyme density across line endings.

        Measures how densely rhyming occurs by comparing total rhymes found
        against the maximum possible rhymes in the given endings.

        Args:
            endings (list[str]): List of line endings analyzed.
            perfect_rhymes (list): List of perfect rhyme pairs found.
            near_rhymes (list): List of near rhyme pairs found.

        Returns:
            float: Rhyme density score between 0.0 and 1.0, where higher values
                   indicate denser rhyming. Returns 0.0 if fewer than 2 endings.

        Note:
            Near rhymes are weighted at 0.7 compared to perfect rhymes.
            Maximum possible rhymes assumes each ending could rhyme with one other.
        """
        if len(endings) < 2:
            return 0.0

        total_rhymes = (
            len(perfect_rhymes) + len(near_rhymes) * 0.7
        )  # Near rhymes weighted at 0.7
        max_possible_rhymes = len(endings) // 2

        return (
            min(total_rhymes / max_possible_rhymes, 1.0)
            if max_possible_rhymes > 0
            else 0.0
        )

    def _calculate_alliteration(self, lines: list[str]) -> float:
        """Calculate alliteration coefficient (initial consonant repetition).

        Measures the frequency of words starting with the same consonant sound
        within and across lines, which contributes to lyrical flow and rhythm.

        Args:
            lines (list[str]): List of lyric lines to analyze for alliteration.

        Returns:
            float: Alliteration score between 0.0 and 1.0, where higher values
                   indicate more frequent initial consonant repetition.

        Note:
            Considers both adjacent words and words two positions apart.
            Only processes lines with 2+ words and words starting with letters.
        """
        if not lines:
            return 0.0

        alliteration_count = 0
        total_word_pairs = 0

        for line in lines:
            words = [word.lower() for word in re.findall(r"\b[a-zA-Z]{2,}\b", line)]
            if len(words) < 2:
                continue

            for i in range(len(words) - 1):
                total_word_pairs += 1
                if words[i][0] == words[i + 1][0]:
                    alliteration_count += 1

                    # Bonus for alliteration across words
                    if i < len(words) - 2 and words[i][0] == words[i + 2][0]:
                        alliteration_count += 0.5

        return alliteration_count / max(total_word_pairs, 1)

    def _calculate_assonance(self, lines: list[str]) -> float:
        """Calculate assonance coefficient (vowel sound repetition).

        Measures the frequency of repeated vowel sounds within lines,
        which contributes to the musicality and flow of rap lyrics.

        Args:
            lines (list[str]): List of lyric lines to analyze for assonance.

        Returns:
            float: Assonance score between 0.0 and 1.0, where higher values
                   indicate more frequent vowel sound repetition.

        Note:
            Analyzes vowel patterns in word pairs within the same line.
            Only considers words with 3+ characters and meaningful vowel content.
        """
        if not lines:
            return 0.0

        vowels = "aeiou"
        assonance_count = 0
        total_comparisons = 0

        for line in lines:
            words = [word.lower() for word in re.findall(r"\b[a-zA-Z]{3,}\b", line)]

            for i in range(len(words)):
                for j in range(i + 1, min(i + 3, len(words))):
                    vowels_i = [c for c in words[i] if c in vowels]
                    vowels_j = [c for c in words[j] if c in vowels]

                    if vowels_i and vowels_j:
                        total_comparisons += 1
                        # Check vowel matches
                        common_vowels = set(vowels_i) & set(vowels_j)
                        if common_vowels:
                            assonance_count += len(common_vowels) / max(
                                len(vowels_i), len(vowels_j)
                            )

        return assonance_count / max(total_comparisons, 1)

    def _calculate_consonance(self, lines: list[str]) -> float:
        """Calculate consonance coefficient (consonant sound repetition).

        Measures the frequency of repeated consonant sounds within lines,
        contributing to the rhythmic texture and sound-based artistry of rap.

        Args:
            lines (list[str]): List of lyric lines to analyze for consonance.

        Returns:
            float: Consonance score between 0.0 and 1.0, where higher values
                   indicate more frequent consonant sound repetition.

        Note:
            Analyzes consonant patterns in word pairs within the same line.
            Excludes vowels and focuses on consonant clusters and sounds.
        """
        if not lines:
            return 0.0

        vowels = "aeiou"
        consonance_count = 0
        total_comparisons = 0

        for line in lines:
            words = [word.lower() for word in re.findall(r"\b[a-zA-Z]{3,}\b", line)]

            for i in range(len(words)):
                for j in range(i + 1, min(i + 3, len(words))):
                    consonants_i = [
                        c for c in words[i] if c not in vowels and c.isalpha()
                    ]
                    consonants_j = [
                        c for c in words[j] if c not in vowels and c.isalpha()
                    ]

                    if consonants_i and consonants_j:
                        total_comparisons += 1
                        common_consonants = set(consonants_i) & set(consonants_j)
                        if common_consonants:
                            consonance_count += len(common_consonants) / max(
                                len(consonants_i), len(consonants_j)
                            )

        return consonance_count / max(total_comparisons, 1)

    def _empty_rhyme_result(self) -> dict[str, Any]:
        """Return empty rhyme analysis result structure.

        Provides a standardized empty result dictionary for cases where
        rhyme analysis cannot be performed (e.g., insufficient lines).

        Returns:
            dict[str, Any]: Empty rhyme result with all metrics set to
                           default values and scheme marked as "insufficient".

        Note:
            Used when input has fewer than 2 lines or other analysis constraints.
            All numeric values are set to 0.0, scheme to "insufficient".
        """
        return {
            "perfect_rhymes": 0,
            "near_rhymes": 0,
            "internal_rhymes": 0,
            "rhyme_scheme": "insufficient",
            "rhyme_scheme_complexity": 0.0,
            "phonetic_similarity_score": 0.0,
            "rhyme_density": 0.0,
            "alliteration_score": 0.0,
            "assonance_score": 0.0,
            "consonance_score": 0.0,
        }


class ReadabilityAnalyzer:
    """Advanced readability analyzer with multiple readability metrics.

    This analyzer calculates various readability indices including Flesch Reading Ease,
    Flesch-Kincaid Grade Level, SMOG Index, Automated Readability Index, and Coleman-Liau Index.
    It provides comprehensive text complexity assessment for rap lyrics analysis.

    Note:
        Uses standard readability formulas adapted for rap lyrics analysis.
        All indices are calculated based on sentence length, word length, and syllable count.
    """

    def analyze_readability(self, text: str) -> dict[str, Any]:
        """Perform comprehensive readability analysis on text.

        Calculates multiple readability indices including Flesch Reading Ease,
        Flesch-Kincaid Grade Level, SMOG Index, Automated Readability Index,
        and Coleman-Liau Index. Provides consensus readability level assessment.

        Args:
            text (str): The text to analyze for readability.

        Returns:
            dict[str, Any]: Comprehensive readability analysis results containing:
                - flesch_reading_ease (float): Flesch Reading Ease score (0-100)
                - flesch_kincaid_grade (float): U.S. grade level equivalent
                - smog_index (float): SMOG readability grade level
                - automated_readability_index (float): ARI grade level
                - coleman_liau_index (float): Coleman-Liau grade level
                - average_sentence_length (float): Words per sentence
                - average_syllables_per_word (float): Syllables per word
                - readability_consensus (str): Consensus difficulty level

        Note:
            Returns empty result structure if text is empty or contains no sentences/words.
            All indices follow standard readability formula implementations.
        """
        if not text.strip():
            return self._empty_readability_result()

        # Basic metrics
        sentences = self._count_sentences(text)
        words = self._count_words(text)
        syllables = self._count_total_syllables(text)

        if sentences == 0 or words == 0:
            return self._empty_readability_result()

        # Calculate various readability indices
        flesch_score = self._calculate_flesch_reading_ease(sentences, words, syllables)
        flesch_kincaid_grade = self._calculate_flesch_kincaid_grade(
            sentences, words, syllables
        )
        smog_index = self._calculate_smog_index(text, sentences)
        ari_score = self._calculate_ari(sentences, words, text)
        coleman_liau = self._calculate_coleman_liau(text, sentences, words)

        return {
            "flesch_reading_ease": flesch_score,
            "flesch_kincaid_grade": flesch_kincaid_grade,
            "smog_index": smog_index,
            "automated_readability_index": ari_score,
            "coleman_liau_index": coleman_liau,
            "average_sentence_length": words / sentences,
            "average_syllables_per_word": syllables / words,
            "readability_consensus": self._calculate_consensus(
                flesch_score, flesch_kincaid_grade, smog_index
            ),
        }

    def _count_sentences(self, text: str) -> int:
        """Count the number of sentences in the text.

        Uses regular expressions to identify sentence boundaries based on
        punctuation marks (., !, ?) followed by whitespace or end of string.

        Args:
            text (str): The text to count sentences in.

        Returns:
            int: The number of sentences detected in the text.

        Note:
            Handles common sentence-ending punctuation and accounts for
            abbreviations to avoid false sentence breaks.
        """
        # Split by punctuation, filter empty strings
        sentences = re.split(r"[.!?]+", text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return len(sentences)

    def _count_words(self, text: str) -> int:
        """Count the number of words in the text.

        Uses regular expressions to identify word boundaries and count
        alphabetic words, excluding punctuation and numbers.

        Args:
            text (str): The text to count words in.

        Returns:
            int: The number of words detected in the text.

        Note:
            Only counts words containing alphabetic characters, filtering
            out pure punctuation or numeric strings.
        """
        words = re.findall(r"\b[a-zA-Z]+\b", text)
        return len(words)

    def _count_total_syllables(self, text: str) -> int:
        """Count the total number of syllables in the text.

        Extracts alphabetic words from the text and counts syllables
        for each word using the FlowAnalyzer's syllable counting method.

        Args:
            text (str): The text to count syllables in.

        Returns:
            int: The total number of syllables across all words in the text.

        Note:
            Uses FlowAnalyzer._syllables_in_word() for accurate syllable
            counting. Only processes alphabetic words, ignoring punctuation.
        """
        words = re.findall(r"\b[a-zA-Z]+\b", text.lower())
        total_syllables = 0

        flow_analyzer = FlowAnalyzer()
        for word in words:
            total_syllables += flow_analyzer._syllables_in_word(word)

        return total_syllables

    def _calculate_flesch_reading_ease(
        self, sentences: int, words: int, syllables: int
    ) -> float:
        """Calculate Flesch Reading Ease score.

        Computes the Flesch Reading Ease formula: 206.835 - (1.015 Ã— ASL) - (84.6 Ã— ASW)
        where ASL is average sentence length and ASW is average syllables per word.

        Args:
            sentences (int): Number of sentences in the text.
            words (int): Number of words in the text.
            syllables (int): Total number of syllables in the text.

        Returns:
            float: Flesch Reading Ease score (0-100), where higher scores indicate
                   easier text. Returns 0.0 if text contains no sentences or words.

        Note:
            Score interpretation: 90-100 (very easy), 80-89 (easy), 70-79 (fairly easy),
            60-69 (standard), 50-59 (fairly difficult), 30-49 (difficult), 0-29 (very difficult).
        """
        if sentences == 0 or words == 0:
            return 0.0

        asl = words / sentences  # Average Sentence Length
        asw = syllables / words  # Average Syllables per Word

        score = 206.835 - (1.015 * asl) - (84.6 * asw)
        return max(0, min(100, score))

    def _calculate_flesch_kincaid_grade(
        self, sentences: int, words: int, syllables: int
    ) -> float:
        """Calculate Flesch-Kincaid Grade Level.

        Computes the Flesch-Kincaid Grade Level formula: 0.39 Ã— ASL + 11.8 Ã— ASW - 15.59
        where ASL is average sentence length and ASW is average syllables per word.

        Args:
            sentences (int): Number of sentences in the text.
            words (int): Number of words in the text.
            syllables (int): Total number of syllables in the text.

        Returns:
            float: U.S. grade level equivalent (e.g., 8.5 means 8th grade, 5th month).
                   Returns 0.0 if text contains no sentences or words.

        Note:
            Grade interpretation: 0-1 (kindergarten), 2-3 (1st-2nd grade), 4-5 (3rd-5th grade),
            6-8 (6th-8th grade), 9-12 (9th-12th grade), 13+ (college level).
        """
        if sentences == 0 or words == 0:
            return 0.0

        asl = words / sentences
        asw = syllables / words

        grade = (0.39 * asl) + (11.8 * asw) - 15.59
        return max(0, grade)

    def _calculate_smog_index(self, text: str, sentences: int) -> float:
        """Calculate SMOG (Simple Measure of Gobbledygook) readability index.

        Computes SMOG formula: 3 + âˆš(polysyllabic_words Ã— 30 / sentences)
        where polysyllabic words are those with 3 or more syllables.

        Args:
            text (str): The text to analyze for SMOG index.
            sentences (int): Number of sentences in the text.

        Returns:
            float: SMOG grade level. Returns 0.0 if fewer than 3 sentences
                   or no polysyllabic words found.

        Note:
            SMOG is particularly good for assessing health-related texts and
            technical writing. Requires at least 3 sentences for valid results.
            Higher scores indicate more complex text.
        """
        if sentences < 3:
            return 0.0

        # Count words with 3+ syllables
        words = re.findall(r"\b[a-zA-Z]+\b", text.lower())
        complex_words = 0
        flow_analyzer = FlowAnalyzer()

        for word in words:
            if flow_analyzer._syllables_in_word(word) >= 3:
                complex_words += 1

        if complex_words == 0:
            return 0.0

        # SMOG = 3 + âˆš(complex_words * 30 / sentences)
        return 3 + math.sqrt(complex_words * 30 / sentences)

    def _calculate_ari(self, sentences: int, words: int, text: str) -> float:
        """Calculate Automated Readability Index (ARI).

        Computes ARI formula: 4.71 Ã— (characters/words) + 0.5 Ã— (words/sentences) - 21.43
        where characters are alphabetic characters only.

        Args:
            sentences (int): Number of sentences in the text.
            words (int): Number of words in the text.
            text (str): The full text for character counting.

        Returns:
            float: ARI grade level. Returns 0.0 if text contains no sentences,
                   words, or alphabetic characters.

        Note:
            ARI is one of the oldest and most widely used readability indices.
            It correlates highly with other indices and is easy to compute.
            Higher scores indicate more complex text.
        """
        if sentences == 0 or words == 0:
            return 0.0

        characters = len(re.sub(r"[^a-zA-Z]", "", text))

        if characters == 0:
            return 0.0

        ari = (4.71 * (characters / words)) + (0.5 * (words / sentences)) - 21.43
        return max(0, ari)

    def _calculate_coleman_liau(self, text: str, sentences: int, words: int) -> float:
        """Calculate Coleman-Liau Index.

        Computes CLI formula: 0.0588 Ã— L - 0.296 Ã— S - 15.8
        where L is average letters per 100 words and S is average sentences per 100 words.

        Args:
            text (str): The full text for character counting.
            sentences (int): Number of sentences in the text.
            words (int): Number of words in the text.

        Returns:
            float: Coleman-Liau grade level. Returns 0.0 if text contains
                   no words or sentences.

        Note:
            Coleman-Liau relies only on character and sentence counts, making it
            language-independent and useful for texts where syllable counting
            might be unreliable. Higher scores indicate more complex text.
        """
        if words == 0 or sentences == 0:
            return 0.0

        characters = len(re.sub(r"[^a-zA-Z]", "", text))

        letters_per_100 = (characters / words) * 100  # Average letters per 100 words
        sentences_per_100 = (sentences / words) * 100  # Average sentences per 100 words

        cli = (0.0588 * letters_per_100) - (0.296 * sentences_per_100) - 15.8
        return max(0, cli)

    def _calculate_consensus(self, flesch: float, fk_grade: float, smog: float) -> str:
        """Calculate consensus readability level from multiple indices.

        Combines Flesch Reading Ease, Flesch-Kincaid Grade, and SMOG Index
        to determine an overall readability consensus level.

        Args:
            flesch (float): Flesch Reading Ease score (0-100).
            fk_grade (float): Flesch-Kincaid Grade Level.
            smog (float): SMOG Index grade level.

        Returns:
            str: Consensus difficulty level from predefined categories:
                 - "elementary": Basic reading level
                 - "middle_school": Intermediate level
                 - "high_school": Advanced level
                 - "college": University level
                 - "graduate": Postgraduate level

        Note:
            Uses average grade level across indices to determine consensus.
            Flesch score is first converted to approximate grade level for averaging.
        """
        # Convert Flesch to approximate grade level
        if flesch >= 90:
            flesch_grade = 5
        elif flesch >= 80:
            flesch_grade = 6
        elif flesch >= 70:
            flesch_grade = 7
        elif flesch >= 60:
            flesch_grade = 8
        elif flesch >= 50:
            flesch_grade = 9
        elif flesch >= 30:
            flesch_grade = 12
        else:
            flesch_grade = 16

        # Arithmetic mean of levels
        avg_grade = (flesch_grade + fk_grade + smog) / 3

        if avg_grade <= 6:
            return "elementary"
        if avg_grade <= 8:
            return "middle_school"
        if avg_grade <= 12:
            return "high_school"
        if avg_grade <= 16:
            return "college"
        return "graduate"

    def _empty_readability_result(self) -> dict[str, Any]:
        """Return empty readability analysis result structure.

        Provides a standardized empty result dictionary for cases where
        readability analysis cannot be performed (e.g., insufficient text).

        Returns:
            dict[str, Any]: Empty readability result with all metrics set to
                           default values and consensus marked as "insufficient".

        Note:
            Used when text is too short or contains no analyzable content.
            All numeric values are set to 0.0, consensus to "insufficient".
        """
        return {
            "flesch_reading_ease": 0.0,
            "flesch_kincaid_grade": 0.0,
            "smog_index": 0.0,
            "automated_readability_index": 0.0,
            "coleman_liau_index": 0.0,
            "average_sentence_length": 0.0,
            "average_syllables_per_word": 0.0,
            "readability_consensus": "insufficient",
        }


@register_analyzer("advanced_algorithmic")
class AdvancedAlgorithmicAnalyzer(BaseAnalyzer):
    """Advanced algorithmic analyzer with comprehensive rap lyrics analysis.

    This analyzer provides sophisticated algorithmic analysis of rap lyrics without
    using AI models. It combines multiple specialized analyzers to assess technical
    mastery, artistic sophistication, and overall quality through phonetic patterns,
    rhythmic analysis, readability metrics, emotional content, thematic elements,
    and literary devices.

    The analyzer is designed for production use with large-scale datasets (57K+ tracks),
    featuring caching for performance optimization, detailed logging, and PostgreSQL
    integration. It serves as a baseline for comparison with AI-based analyzers.

    Attributes:
        lexicon (AdvancedLexicon): Lexical resources for semantic analysis including
            emotional categories, rap themes, complexity indicators, and literary devices.
        flow_analyzer (FlowAnalyzer): Analyzer for rhythmic patterns, syllable consistency,
            stress patterns, and flow characteristics.
        rhyme_analyzer (RhymeAnalyzer): Advanced rhyme detection with phonetic analysis,
            perfect/near rhymes, internal rhymes, and sound-based literary devices.
        readability_analyzer (ReadabilityAnalyzer): Multiple readability indices including
            Flesch Reading Ease, SMOG, ARI, and Coleman-Liau formulas.
        cache_enabled (bool): Whether result caching is enabled for performance.
        analysis_cache (dict): Cache storage for analyzed results (if enabled).
        detailed_logging (bool): Whether detailed debug logging is enabled.

    Note:
        This analyzer uses algorithmic approaches only - no machine learning models.
        Results are deterministic and reproducible. Designed for batch processing
        with configurable performance optimizations.
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """Initialize the AdvancedAlgorithmicAnalyzer with configuration.

        Sets up all component analyzers and configures caching and logging options.
        Initializes the analysis cache if caching is enabled.

        Args:
            config (dict[str, Any] | None): Configuration dictionary with optional settings:
                - cache_enabled (bool): Enable result caching (default: True)
                - detailed_logging (bool): Enable debug logging (default: False)
                - Other BaseAnalyzer config options

        Note:
            Component analyzers are initialized with default configurations.
            Logging level is set to DEBUG if detailed_logging is enabled.
        """
        super().__init__(config)

        # Initialize component analyzers
        self.lexicon = AdvancedLexicon()
        self.flow_analyzer = FlowAnalyzer()
        self.rhyme_analyzer = RhymeAnalyzer()
        self.readability_analyzer = ReadabilityAnalyzer()

        # Configuration settings
        self.cache_enabled = self.config.get("cache_enabled", True)
        self.analysis_cache = {} if self.cache_enabled else None
        self.detailed_logging = self.config.get("detailed_logging", False)

        if self.detailed_logging:
            logger.setLevel(logging.DEBUG)

    def analyze_song(self, artist: str, title: str, lyrics: str) -> AnalysisResult:
        """Perform comprehensive algorithmic analysis of rap lyrics.

        Conducts thorough analysis using all component analyzers to assess technical
        mastery, artistic sophistication, and overall quality. Includes caching for
        performance optimization and detailed metadata collection.

        Args:
            artist (str): Name of the artist/performer.
            title (str): Title of the song.
            lyrics (str): Full lyrics text to analyze.

        Returns:
            AnalysisResult: Dictionary containing comprehensive analysis results:
                - analysis_type (str): Always "advanced_algorithmic"
                - analysis_data (dict): Raw analysis results from all components
                - confidence (float): Overall confidence score (0.0-1.0)
                - processing_time (float): Time taken for analysis in seconds
                - metadata (dict): Analysis metadata and processing details
                - raw_output (dict): Alias for analysis_data for compatibility
                - timestamp (str): ISO format timestamp of analysis
                - artist (str): Artist name (echoed from input)
                - title (str): Song title (echoed from input)

        Raises:
            ValueError: If input validation fails (invalid artist/title/lyrics).

        Note:
            Results are cached if caching is enabled. Analysis includes rhyme patterns,
            flow metrics, readability indices, emotional content, thematic analysis,
            literary devices, vocabulary sophistication, structural patterns, and
            creativity assessment with composite scoring.
        """
        start_time = time.time()

        # Input validation
        if not self.validate_input(artist, title, lyrics):
            raise ValueError("Invalid input parameters")

        # Check cache for existing results
        cache_key = None
        if self.cache_enabled:
            cache_key = self._generate_cache_key(artist, title, lyrics)
            if cache_key in self.analysis_cache:
                cached_result = self.analysis_cache[cache_key]
                logger.debug(f"Returning cached result for {artist} - {title}")
                return cached_result

        # Preprocessing
        processed_lyrics = self.preprocess_lyrics(lyrics)
        lines = self._split_into_lines(processed_lyrics)
        words = self._extract_meaningful_words(processed_lyrics)

        if self.detailed_logging:
            logger.debug(
                f"Processing {artist} - {title}: {len(lines)} lines, {len(words)} words"
            )

        # Core analysis using all component analyzers
        analysis_results = {
            "advanced_sentiment": self._analyze_advanced_sentiment(words),
            "rhyme_analysis": self.rhyme_analyzer.analyze_rhyme_structure(lines),
            "flow_analysis": self.flow_analyzer.analyze_flow_patterns(lines),
            "readability_metrics": self.readability_analyzer.analyze_readability(
                processed_lyrics
            ),
            "thematic_analysis": self._analyze_themes_advanced(words),
            "literary_devices": self._analyze_literary_devices(processed_lyrics, words),
            "vocabulary_sophistication": self._analyze_vocabulary_sophistication(words),
            "structural_analysis": self._analyze_structure_advanced(
                lines, processed_lyrics
            ),
            "creativity_metrics": self._analyze_creativity_advanced(
                processed_lyrics, words, lines
            ),
        }

        # Calculate composite scores
        composite_scores = self._calculate_advanced_composite_scores(analysis_results)
        analysis_results.update(composite_scores)

        # Calculate overall confidence
        confidence = self._calculate_advanced_confidence(analysis_results, lines, words)

        processing_time = time.time() - start_time

        # Build comprehensive metadata
        metadata = {
            "analyzer_version": "2.0.0",
            "processing_date": datetime.now(tz=timezone.utc).isoformat(),
            "lyrics_length": len(processed_lyrics),
            "word_count": len(words),
            "line_count": len(lines),
            "processing_components": list(analysis_results.keys()),
            "cache_used": False,
            "detailed_logging": self.detailed_logging,
        }

        # Build result dictionary (legacy-compatible format)
        result_dict = {
            "analysis_type": "advanced_algorithmic",
            "analysis_data": analysis_results,
            "confidence": confidence,
            "processing_time": processing_time,
            "metadata": metadata,
            "raw_output": analysis_results,
            "timestamp": datetime.now(tz=timezone.utc).isoformat(),
            "artist": artist,
            "title": title,
        }

        # Cache the result if caching is enabled
        if self.cache_enabled and cache_key:
            self.analysis_cache[cache_key] = result_dict

        if self.detailed_logging:
            logger.debug(
                f"Analysis completed for {artist} - {title} in {processing_time:.3f}s"
            )

        return result_dict

    def _generate_cache_key(self, artist: str, title: str, lyrics: str) -> str:
        """Generate a unique cache key for analysis results.

        Creates an MD5 hash of the artist, title, and first 500 characters of lyrics
        to create a deterministic cache key for result storage and retrieval.

        Args:
            artist (str): Artist name for cache key generation.
            title (str): Song title for cache key generation.
            lyrics (str): Full lyrics text (only first 500 chars used).

        Returns:
            str: MD5 hash string serving as unique cache identifier.

        Note:
            Uses first 500 characters of lyrics to balance uniqueness with performance.
            Same inputs will always generate the same cache key.
        """
        content = (
            f"{artist}|{title}|{lyrics[:500]}"  # First 500 characters for uniqueness
        )
        return hashlib.md5(content.encode()).hexdigest()

    def _split_into_lines(self, lyrics: str) -> list[str]:
        """Split lyrics into individual lines with cleaning.

        Processes lyrics text into clean lines, removing extra whitespace and
        filtering out metadata lines like [Verse], [Chorus], etc.

        Args:
            lyrics (str): Raw lyrics text to process.

        Returns:
            list[str]: List of cleaned lyric lines, excluding metadata markers.

        Note:
            Filters out common rap structure markers and empty lines.
            Preserves meaningful content while removing structural annotations.
        """
        lines = [line.strip() for line in lyrics.split("\n") if line.strip()]

        # Filter out metadata lines
        return [
            line
            for line in lines
            if not re.match(
                r"^\[.*\]$|^\(.*\)$|^(Verse|Chorus|Bridge|Outro|Intro)[\s\d]*:",
                line,
                re.IGNORECASE,
            )
        ]

    def _extract_meaningful_words(self, lyrics: str) -> list[str]:
        """Extract meaningful words from lyrics for analysis.

        Uses regex to extract alphabetic words of 3+ characters, filtering out
        common stop words and short words that don't contribute to semantic analysis.

        Args:
            lyrics (str): Lyrics text to extract words from.

        Returns:
            list[str]: List of meaningful words (3+ chars, alphabetic, not stop words).

        Note:
            Filters out common English stop words and very short words.
            Converts to lowercase for consistent analysis.
        """
        words = re.findall(r"\b[a-zA-Z]{2,}\b", lyrics.lower())

        # Extended stop words list
        stop_words = {
            "the",
            "a",
            "an",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
            "by",
            "from",
            "up",
            "about",
            "into",
            "through",
            "during",
            "before",
            "after",
            "above",
            "below",
            "between",
            "among",
            "this",
            "that",
            "these",
            "those",
            "i",
            "me",
            "my",
            "myself",
            "we",
            "our",
            "ours",
            "ourselves",
            "you",
            "your",
            "yours",
            "he",
            "him",
            "his",
            "himself",
            "she",
            "her",
            "hers",
            "herself",
            "it",
            "its",
            "itself",
            "they",
            "them",
            "their",
            "theirs",
            "themselves",
            "what",
            "which",
            "who",
            "whom",
            "am",
            "is",
            "are",
            "was",
            "were",
            "be",
            "been",
            "being",
            "have",
            "has",
            "had",
            "having",
            "do",
            "does",
            "did",
            "doing",
            "will",
            "would",
            "could",
            "should",
            "yeah",
            "uh",
            "oh",
            "got",
            "get",
            "gotta",
            "wanna",
            "gonna",
            "ain",
            "yall",
            "em",
            "ya",
            "like",
            "just",
            "now",
            "know",
            "see",
            "come",
            "go",
            "say",
            "said",
            "tell",
            "make",
            "way",
            "time",
            "want",
            "need",
            "take",
            "give",
            "put",
            "keep",
            "let",
            "think",
        }

        return [word for word in words if word not in stop_words and len(word) >= 3]

    def _analyze_advanced_sentiment(self, words: list[str]) -> dict[str, Any]:
        """Perform advanced sentiment analysis with emotion gradation.

        Analyzes emotional content using intensity-weighted emotion categories,
        calculates emotional valence, and determines dominant emotions with
        complexity assessment.

        Args:
            words (list[str]): List of meaningful words to analyze for emotions.

        Returns:
            dict[str, Any]: Comprehensive sentiment analysis results containing:
                - emotion_scores (dict): Scores for each emotion category with intensity
                - dominant_emotion (str): Primary emotion detected
                - dominant_emotion_strength (float): Strength of dominant emotion
                - emotional_valence (float): Overall positive/negative balance (-1 to 1)
                - emotional_intensity (float): Overall emotional content (0-1)
                - total_emotional_words (int): Count of emotion-related words
                - emotional_complexity (int): Number of different emotions present

        Note:
            Uses weighted scoring based on emotion intensity levels (weak/medium/strong).
            Emotional valence combines positive and negative emotions for overall tone.
        """
        if not words:
            return self._empty_sentiment_result()

        emotion_scores = {}
        total_emotional_words = 0

        # Analyze by emotion categories with intensity weighting
        for emotion, intensity_levels in self.lexicon.emotions.items():
            emotion_score = 0.0
            emotion_word_count = 0

            for intensity, word_set in intensity_levels.items():
                matches = len(set(words) & word_set)
                if matches > 0:
                    # Intensity weight coefficients
                    intensity_weight = {"weak": 1.0, "medium": 2.0, "strong": 3.0}[
                        intensity
                    ]
                    emotion_score += matches * intensity_weight
                    emotion_word_count += matches
                    total_emotional_words += matches

            emotion_scores[emotion] = {
                "score": emotion_score,
                "word_count": emotion_word_count,
                "normalized_score": emotion_score / len(words) if words else 0,
            }

        # Determine dominant emotion
        if emotion_scores:
            dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1]["score"])
            dominant_emotion_name = dominant_emotion[0]
            dominant_emotion_strength = dominant_emotion[1]["score"]
        else:
            dominant_emotion_name = "neutral"
            dominant_emotion_strength = 0

        # Calculate overall emotional valence
        positive_emotions = emotion_scores.get("joy", {}).get("score", 0)
        negative_emotions = (
            emotion_scores.get("anger", {}).get("score", 0)
            + emotion_scores.get("sadness", {}).get("score", 0)
            + emotion_scores.get("fear", {}).get("score", 0)
        )

        if total_emotional_words > 0:
            valence = (positive_emotions - negative_emotions) / total_emotional_words
        else:
            valence = 0.0

        return {
            "emotion_scores": emotion_scores,
            "dominant_emotion": dominant_emotion_name,
            "dominant_emotion_strength": dominant_emotion_strength,
            "emotional_valence": valence,
            "emotional_intensity": total_emotional_words / len(words) if words else 0,
            "total_emotional_words": total_emotional_words,
            "emotional_complexity": len(
                [e for e in emotion_scores.values() if e["score"] > 0]
            ),
        }

    def _analyze_themes_advanced(self, words: list[str]) -> dict[str, Any]:
        """Perform advanced thematic analysis of rap lyrics.

        Analyzes thematic content across rap-specific categories including money,
        street life, success, relationships, and violence. Calculates thematic
        diversity and identifies dominant themes.

        Args:
            words (list[str]): List of meaningful words to analyze for themes.

        Returns:
            dict[str, Any]: Thematic analysis results containing:
                - theme_scores (dict): Scores for each thematic category
                - dominant_theme (str): Primary theme detected
                - secondary_themes (list[str]): Secondary themes by importance
                - thematic_diversity (float): Diversity of themes present (0-1)
                - total_thematic_words (int): Total words matching theme categories

        Note:
            Uses rap-specific thematic categories relevant to hip-hop culture.
            Thematic diversity measures how many different themes are present.
        """
        if not words:
            return {"theme_scores": {}, "dominant_theme": "neutral"}

        word_set = set(words)
        theme_scores = {}

        # Analyze by thematic categories
        for theme, theme_words in self.lexicon.rap_themes.items():
            matches = len(word_set & theme_words)
            theme_scores[theme] = {
                "absolute_count": matches,
                "relative_score": matches / len(words),
                "theme_coverage": matches / len(theme_words) if theme_words else 0,
            }

        # Determine dominant themes
        sorted_themes = sorted(
            theme_scores.items(), key=lambda x: x[1]["absolute_count"], reverse=True
        )

        dominant_theme = (
            sorted_themes[0][0]
            if sorted_themes and sorted_themes[0][1]["absolute_count"] > 0
            else "neutral"
        )

        # Calculate thematic diversity
        active_themes = [
            theme
            for theme, scores in theme_scores.items()
            if scores["absolute_count"] > 0
        ]
        thematic_diversity = len(active_themes) / len(self.lexicon.rap_themes)

        return {
            "theme_scores": theme_scores,
            "dominant_theme": dominant_theme,
            "secondary_themes": [
                theme[0]
                for theme in sorted_themes[1:4]
                if theme[1]["absolute_count"] > 0
            ],
            "thematic_diversity": thematic_diversity,
            "total_thematic_words": sum(
                score["absolute_count"] for score in theme_scores.values()
            ),
        }

    def _analyze_literary_devices(
        self, lyrics: str, words: list[str]
    ) -> dict[str, Any]:
        """Analyze literary devices and rhetorical techniques in lyrics.

        Detects metaphors, similes, time references, contrasts, repetitions,
        and personification in rap lyrics to assess artistic sophistication.

        Args:
            lyrics (str): Full lyrics text to analyze for literary devices.
            words (list[str]): List of meaningful words for additional analysis.

        Returns:
            dict[str, Any]: Literary devices analysis containing:
                - metaphor_count (int): Number of metaphor indicators found
                - simile_count (int): Number of simile indicators ("like", "as")
                - time_references (int): Count of temporal reference words
                - contrast_usage (int): Number of contrast/concession words
                - repetition_analysis (dict): Analysis of line and phrase repetitions
                - personification_count (int): Count of personification indicators
                - total_literary_devices (int): Sum of all literary device counts

        Note:
            Uses simplified heuristics to detect common literary techniques.
            Personification detection is basic and could be enhanced.
        """
        if not lyrics or not words:
            return self._empty_literary_result()

        # Search for metaphors and similes
        metaphor_count = 0
        simile_count = 0
        lyrics_lower = lyrics.lower()

        for indicator in self.lexicon.literary_devices["metaphor_indicators"]:
            if indicator in ["like", "as"]:
                simile_count += lyrics_lower.count(f" {indicator} ")
            else:
                metaphor_count += lyrics_lower.count(indicator)

        # Analyze time references
        time_references = sum(
            lyrics_lower.count(time_word)
            for time_word in self.lexicon.literary_devices["time_references"]
        )

        # Search for contrasts and oppositions
        contrast_usage = sum(
            lyrics_lower.count(contrast_word)
            for contrast_word in self.lexicon.literary_devices["contrast_words"]
        )

        # Analyze repetitions and refrains
        line_repetitions = self._analyze_repetitions(lyrics)

        # Personification (simplified)
        personification_indicators = [
            "speaks",
            "whispers",
            "calls",
            "cries",
            "laughs",
            "dances",
        ]
        personification_count = sum(
            lyrics_lower.count(indicator) for indicator in personification_indicators
        )

        return {
            "metaphor_count": metaphor_count,
            "simile_count": simile_count,
            "time_references": time_references,
            "contrast_usage": contrast_usage,
            "repetition_analysis": line_repetitions,
            "personification_count": personification_count,
            "total_literary_devices": metaphor_count
            + simile_count
            + time_references
            + contrast_usage
            + personification_count,
        }

    def _analyze_repetitions(self, lyrics: str) -> dict[str, Any]:
        """Analyze repetitions and refrains in lyrics.

        Detects repeated lines, phrases, and patterns that contribute to
        lyrical structure and memorability in rap music.

        Args:
            lyrics (str): Full lyrics text to analyze for repetitions.

        Returns:
            dict[str, Any]: Repetition analysis results containing:
                - repeated_lines (int): Number of lines that appear more than once
                - repeated_phrases (int): Number of 2-4 word phrases repeated
                - repetition_ratio (float): Ratio of repeated to total lines
                - most_repeated_line (str|None): Most frequently repeated line
                - total_line_repetitions (int): Total count of line repetitions

        Note:
            Analyzes both exact line repetitions and repeated phrases of 2-4 words.
            Useful for detecting hooks, choruses, and structural elements.
        """
        lines = [line.strip().lower() for line in lyrics.split("\n") if line.strip()]

        if not lines:
            return {"repeated_lines": 0, "repetition_ratio": 0.0}

        line_counts = Counter(lines)
        repeated_lines = {
            line: count for line, count in line_counts.items() if count > 1
        }

        # Analyze repeated phrases (2-4 words)
        phrase_repetitions = defaultdict(int)
        for line in lines:
            words = line.split()
            for i in range(len(words) - 1):
                for j in range(2, min(5, len(words) - i + 1)):
                    phrase = " ".join(words[i : i + j])
                    if len(phrase) > 5:  # Ignore very short phrases
                        phrase_repetitions[phrase] += 1

        repeated_phrases = {
            phrase: count for phrase, count in phrase_repetitions.items() if count > 1
        }

        return {
            "repeated_lines": len(repeated_lines),
            "repeated_phrases": len(repeated_phrases),
            "repetition_ratio": len(repeated_lines) / len(set(lines)) if lines else 0,
            "most_repeated_line": max(line_counts.items(), key=lambda x: x[1])
            if line_counts
            else None,
            "total_line_repetitions": sum(
                count - 1 for count in line_counts.values() if count > 1
            ),
        }

    def _analyze_vocabulary_sophistication(self, words: list[str]) -> dict[str, Any]:
        """Analyze vocabulary sophistication and lexical diversity.

        Assesses the complexity and richness of vocabulary used, including
        philosophical concepts, abstract thinking, and uncommon word choices.

        Args:
            words (list[str]): List of meaningful words to analyze.

        Returns:
            dict[str, Any]: Vocabulary analysis results containing:
                - complexity_scores (dict): Scores for different complexity categories
                - total_complex_words (int): Total words indicating complex thinking
                - average_word_length (float): Mean length of words in characters
                - long_words_count (int): Number of words with 7+ characters
                - vocabulary_richness (float): Ratio of unique to total words
                - uncommon_words_ratio (float): Ratio of uncommon to total words
                - lexical_diversity (float): Diversity measure (unique/total words)
                - sophisticated_vocabulary_score (float): Overall sophistication score

        Note:
            Uses predefined categories of complex vocabulary and heuristics
            for detecting sophisticated language use in rap lyrics.
        """
        if not words:
            return self._empty_vocabulary_result()

        word_set = set(words)

        # Analyze complexity of thinking
        complexity_scores = {}
        total_complex_words = 0

        for category, category_words in self.lexicon.complexity_indicators.items():
            matches = len(word_set & category_words)
            complexity_scores[category] = matches
            total_complex_words += matches

        # Analyze word length
        word_lengths = [len(word) for word in words]
        avg_word_length = sum(word_lengths) / len(word_lengths)
        long_words = len([w for w in words if len(w) >= 7])

        # Vocabulary richness
        vocabulary_richness = len(word_set) / len(words)

        # Uncommon/rare words (heuristic)
        common_words = {
            "love",
            "money",
            "life",
            "time",
            "good",
            "bad",
            "right",
            "wrong",
            "black",
            "white",
            "red",
            "blue",
            "big",
            "small",
            "old",
            "new",
            "high",
            "low",
            "fast",
            "slow",
            "hot",
            "cold",
            "young",
            "real",
            "hard",
            "easy",
            "free",
            "game",
            "play",
            "work",
            "home",
            "house",
        }

        uncommon_words = len(word_set - common_words)
        uncommon_ratio = uncommon_words / len(words)

        return {
            "complexity_scores": complexity_scores,
            "total_complex_words": total_complex_words,
            "average_word_length": avg_word_length,
            "long_words_count": long_words,
            "vocabulary_richness": vocabulary_richness,
            "uncommon_words_ratio": uncommon_ratio,
            "lexical_diversity": len(word_set) / max(len(words), 1),
            "sophisticated_vocabulary_score": (
                total_complex_words + long_words + uncommon_words
            )
            / len(words),
        }

    def _analyze_structure_advanced(
        self, lines: list[str], full_text: str
    ) -> dict[str, Any]:
        """Perform advanced structural analysis of lyrics.

        Analyzes the structural organization of lyrics including line length
        patterns, punctuation usage, stanza organization, and overall consistency.

        Args:
            lines (list[str]): List of cleaned lyric lines.
            full_text (str): Complete lyrics text for punctuation analysis.

        Returns:
            dict[str, Any]: Structural analysis results containing:
                - total_lines (int): Total number of lines in lyrics
                - average_line_length (float): Mean words per line
                - line_length_variance (float): Variance in line lengths
                - punctuation_analysis (dict): Counts of different punctuation marks
                - structure_patterns (dict): Analysis of repeating structural patterns
                - stanza_analysis (dict): Stanza organization and consistency
                - structural_consistency (float): Overall structural regularity (0-1)

        Note:
            Assesses both micro-structure (line-level patterns) and macro-structure
            (stanza organization) to understand lyrical architecture.
        """
        if not lines:
            return self._empty_structure_result()

        # Basic structural metrics
        total_lines = len(lines)
        line_lengths = [len(line.split()) for line in lines]
        avg_line_length = sum(line_lengths) / len(line_lengths)
        line_length_variance = sum(
            (x - avg_line_length) ** 2 for x in line_lengths
        ) / len(line_lengths)

        # Punctuation and pause analysis
        punctuation_analysis = self._analyze_punctuation(full_text)

        # Structural pattern analysis
        structure_patterns = self._find_structure_patterns(lines)

        # Stanza organization analysis
        stanzas = self._identify_stanzas(full_text)

        return {
            "total_lines": total_lines,
            "average_line_length": avg_line_length,
            "line_length_variance": line_length_variance,
            "punctuation_analysis": punctuation_analysis,
            "structure_patterns": structure_patterns,
            "stanza_analysis": stanzas,
            "structural_consistency": 1.0
            - (line_length_variance / max(avg_line_length, 1)),
        }

    def _analyze_punctuation(self, text: str) -> dict[str, int]:
        """Analyze punctuation usage in text.

        Counts various punctuation marks and symbols that can affect
        the rhythm and delivery of rap lyrics.

        Args:
            text (str): The text to analyze for punctuation usage.

        Returns:
            dict[str, int]: Dictionary containing counts of different
                           punctuation marks including periods, commas,
                           exclamations, questions, etc.

        Note:
            Includes total count and individual punctuation categories.
            Useful for understanding text structure and delivery patterns.
        """
        punctuation_counts = {
            "periods": text.count("."),
            "commas": text.count(","),
            "exclamations": text.count("!"),
            "questions": text.count("?"),
            "semicolons": text.count(";"),
            "colons": text.count(":"),
            "dashes": text.count("-") + text.count("--"),
            "ellipses": text.count("..."),
            "parentheses": text.count("(") + text.count(")"),
            "quotations": text.count('"') + text.count("'"),
        }

        total_punctuation = sum(punctuation_counts.values())
        punctuation_counts["total"] = total_punctuation

        return punctuation_counts

    def _find_structure_patterns(self, lines: list[str]) -> dict[str, Any]:
        """Find structural patterns in lyric lines.

        Analyzes line length patterns to identify repeating structural
        elements that contribute to the song's organization and flow.

        Args:
            lines (list[str]): List of lyric lines to analyze for patterns.

        Returns:
            dict[str, Any]: Dictionary containing pattern analysis results
                           including whether patterns were found and their
                           consistency metrics.

        Note:
            Looks for repeating sequences in line lengths that indicate
            structural organization in the lyrics.
        """
        if len(lines) < 4:
            return {"pattern_found": False}

        # Analysis of line length patterns
        line_lengths = [len(line.split()) for line in lines]

        # Search for repeating length patterns (e.g., ABAB by length)
        pattern_length = 4
        patterns_found = []

        for i in range(len(line_lengths) - pattern_length + 1):
            pattern = line_lengths[i : i + pattern_length]
            # Look for repetition of this pattern
            for j in range(i + pattern_length, len(line_lengths) - pattern_length + 1):
                if line_lengths[j : j + pattern_length] == pattern:
                    patterns_found.append(pattern)
                    break

        return {
            "pattern_found": len(patterns_found) > 0,
            "patterns": patterns_found,
            "pattern_consistency": len(patterns_found)
            / max(len(line_lengths) // pattern_length, 1),
        }

    def _identify_stanzas(self, text: str) -> dict[str, Any]:
        """Analyze stanza structure in lyrics.

        Identifies stanza divisions and analyzes their consistency
        and organization within the complete lyrics text.

        Args:
            text (str): Complete lyrics text to analyze for stanza structure.

        Returns:
            dict[str, Any]: Dictionary containing stanza analysis results
                           including count, average length, and consistency metrics.

        Note:
            Stanzas are identified by double line breaks and other structural
            indicators common in rap lyrics formatting.
        """
        # Split text by double line breaks
        stanzas = [
            stanza.strip() for stanza in re.split(r"\n\s*\n", text) if stanza.strip()
        ]

        if not stanzas:
            stanzas = [text]

        stanza_lengths = [len(stanza.split("\n")) for stanza in stanzas]

        return {
            "stanza_count": len(stanzas),
            "average_stanza_length": sum(stanza_lengths) / len(stanza_lengths)
            if stanza_lengths
            else 0,
            "stanza_length_consistency": self._calculate_consistency_score(
                stanza_lengths
            ),
            "stanza_lengths": stanza_lengths,
        }

    def _analyze_creativity_advanced(
        self, lyrics: str, words: list[str], lines: list[str]
    ) -> dict[str, Any]:
        """Perform advanced creativity analysis of rap lyrics.

        Analyzes creative elements including neologisms, unique phrases,
        wordplay techniques, and innovative rhyme patterns to assess
        artistic originality and linguistic innovation.

        Args:
            lyrics (str): Full lyrics text to analyze for creative elements.
            words (list[str]): List of meaningful words for neologism detection.
            lines (list[str]): List of lyric lines for phrase analysis.

        Returns:
            dict[str, Any]: Creativity analysis results containing:
                - neologisms (list[str]): Detected potential neologisms and invented words
                - unique_phrases (list[str]): Unique and original phrase constructions
                - wordplay_analysis (dict): Analysis of wordplay techniques and puns
                - innovative_rhymes (dict): Assessment of rhyme innovation and complexity
                - creativity_factors (list[float]): Individual creativity component scores
                - overall_creativity_score (float): Composite creativity assessment (0-1)

        Note:
            Combines multiple creativity indicators including linguistic innovation,
            wordplay sophistication, and structural originality to provide
            comprehensive assessment of artistic creativity.
        """
        if not words or not lines:
            return self._empty_creativity_result()

        # Neologisms and unusual word forms
        neologisms = self._detect_neologisms(words)

        # Original phrase constructions
        unique_phrases = self._find_unique_phrases(lines)

        # Semantic shifts and wordplay
        wordplay_analysis = self._analyze_advanced_wordplay(lyrics, words)

        # Innovation in rhymes
        innovative_rhymes = self._analyze_rhyme_innovation(lines)

        # Overall creativity score
        creativity_factors = [
            len(neologisms) / max(len(words), 1),
            len(unique_phrases) / max(len(lines), 1),
            wordplay_analysis["total_score"],
            innovative_rhymes["innovation_score"],
        ]

        overall_creativity = sum(creativity_factors) / len(creativity_factors)

        return {
            "neologisms": neologisms,
            "unique_phrases": unique_phrases,
            "wordplay_analysis": wordplay_analysis,
            "innovative_rhymes": innovative_rhymes,
            "creativity_factors": creativity_factors,
            "overall_creativity_score": overall_creativity,
        }

    def _detect_neologisms(self, words: list[str]) -> list[str]:
        """Detect neologisms and unusual words in vocabulary.

        Identifies potentially invented or uncommon words that may indicate
        creative language use or unique artistic expression in rap lyrics.

        Args:
            words (list[str]): List of meaningful words to analyze for neologisms.

        Returns:
            list[str]: List of detected potential neologisms, limited to top 10
                      for performance reasons.

        Note:
            Uses heuristics based on word length, unusual suffixes/prefixes,
            and repeated character patterns to identify potential neologisms.
        """
        # Simple heuristic for detecting possible neologisms
        potential_neologisms = []

        for word in words:
            # Words with unusual suffixes or prefixes
            if len(word) > 6 and (
                word.endswith(("ness", "tion", "ism"))
                or word.startswith(("un", "pre", "over"))
            ):
                potential_neologisms.append(word)

            # Words with repeating parts
            if len(word) > 4:
                mid = len(word) // 2
                if word[:mid] == word[mid:] or word[:mid] in word[mid:]:
                    potential_neologisms.append(word)

        return potential_neologisms[:10]

    def _find_unique_phrases(self, lines: list[str]) -> list[str]:
        """Find unique phrase constructions in lyrics.

        Identifies unusual or creative phrase structures that demonstrate
        artistic originality and innovative language use in rap lyrics.

        Args:
            lines (list[str]): List of lyric lines to analyze for unique phrases.

        Returns:
            list[str]: List of detected unique phrases, limited to top 5
                      for performance reasons.

        Note:
            Looks for inverted sentence structures, alliterative patterns,
            and other creative linguistic constructions.
        """
        unique_phrases = []

        # Look for phrases with unusual structure
        for line in lines:
            words = line.split()
            if len(words) >= 3:
                # Search for inverted constructions
                if len(words) >= 4 and words[0].lower() in [
                    "when",
                    "where",
                    "how",
                    "why",
                ]:
                    unique_phrases.append(line)

                # Search for alliterations in phrases
                if (
                    len(
                        [
                            w
                            for w in words[:3]
                            if w and words[0] and w[0].lower() == words[0][0].lower()
                        ]
                    )
                    >= 2
                ):
                    unique_phrases.append(line)

        return unique_phrases[:5]

    def _analyze_advanced_wordplay(
        self, lyrics: str, words: list[str]
    ) -> dict[str, Any]:
        """Analyze advanced wordplay techniques in lyrics.

        Detects sophisticated linguistic techniques including double meanings,
        onomatopoeia, and potential puns that demonstrate artistic mastery.

        Args:
            lyrics (str): Full lyrics text to analyze for wordplay.
            words (list[str]): List of meaningful words for analysis.

        Returns:
            dict[str, Any]: Dictionary containing wordplay analysis results
                           including detected techniques and overall score.

        Note:
            Identifies multiple forms of wordplay that contribute to the
            artistic complexity and entertainment value of rap lyrics.
        """
        wordplay_score = 0
        techniques_found = []

        # Double meanings (simplified)
        double_meanings = [
            word
            for word in set(words)
            if len(word) > 4
            and any(
                other in word for other in words if other != word and len(other) > 2
            )
        ]

        if double_meanings:
            wordplay_score += len(double_meanings) * 0.1
            techniques_found.append("double_meanings")

        # Onomatopoeia
        onomatopoeia = [
            "bang",
            "boom",
            "crash",
            "pop",
            "snap",
            "crack",
            "splash",
            "whoosh",
        ]
        onomatopoeia_count = sum(lyrics.lower().count(sound) for sound in onomatopoeia)

        if onomatopoeia_count > 0:
            wordplay_score += onomatopoeia_count * 0.05
            techniques_found.append("onomatopoeia")

        # Puns (simple heuristic)
        puns_detected = 0
        for i, word in enumerate(words[:-1]):
            next_word = words[i + 1]
            if (
                len(word) > 3
                and len(next_word) > 3
                and word[:-1] == next_word[:-1]
                and word != next_word
            ):
                puns_detected += 1

        if puns_detected > 0:
            wordplay_score += puns_detected * 0.15
            techniques_found.append("potential_puns")

        return {
            "total_score": min(wordplay_score, 1.0),
            "techniques_found": techniques_found,
            "double_meanings": double_meanings,
            "onomatopoeia_count": onomatopoeia_count,
            "potential_puns": puns_detected,
        }

    def _analyze_rhyme_innovation(self, lines: list[str]) -> dict[str, Any]:
        """Analyze innovative rhyme techniques and patterns.

        Evaluates creative rhyme usage including multisyllabic rhymes,
        internal rhyme density, and complex rhyme schemes that demonstrate
        lyrical innovation and technical skill.

        Args:
            lines (list[str]): List of lyric lines to analyze for rhyme innovation.

        Returns:
            dict[str, Any]: Dictionary with innovation metrics including
                           multisyllabic rhyme ratio, internal rhyme density,
                           and overall innovation score.

        Note:
            Focuses on advanced rhyme techniques that go beyond basic
            end-rhyme patterns to assess artistic sophistication.
        """
        if len(lines) < 4:
            return {"innovation_score": 0.0}

        # Extract endings
        endings = []
        for line in lines[:12]:
            words = line.split()
            if words:
                ending = re.sub(r"[^\w]", "", words[-1].lower())
                if len(ending) >= 2:
                    endings.append(ending)

        innovation_factors = []

        # Multi-syllable rhymes
        multisyllabic_rhymes = 0
        flow_analyzer = FlowAnalyzer()
        for ending in endings:
            if flow_analyzer._syllables_in_word(ending) >= 3:
                multisyllabic_rhymes += 1

        if endings:
            multisyllabic_ratio = multisyllabic_rhymes / len(endings)
            innovation_factors.append(multisyllabic_ratio)

        # Internal rhymes
        internal_rhyme_count = 0
        for line in lines:
            words = line.split()
            for i in range(len(words) - 1):
                for j in range(i + 1, len(words)):
                    if self._simple_rhyme_check(words[i], words[j]):
                        internal_rhyme_count += 1

        internal_rhyme_ratio = internal_rhyme_count / max(len(lines), 1)
        innovation_factors.append(min(internal_rhyme_ratio, 1.0))

        # Unusual rhymes (long endings)
        long_rhymes = len([e for e in endings if len(e) >= 6])
        long_rhyme_ratio = long_rhymes / max(len(endings), 1)
        innovation_factors.append(long_rhyme_ratio)

        overall_innovation = (
            sum(innovation_factors) / len(innovation_factors)
            if innovation_factors
            else 0
        )

        return {
            "innovation_score": overall_innovation,
            "multisyllabic_rhymes": multisyllabic_rhymes,
            "internal_rhymes": internal_rhyme_count,
            "long_rhymes": long_rhymes,
            "innovation_factors": innovation_factors,
        }

    def _simple_rhyme_check(self, word1: str, word2: str) -> bool:
        """Perform a simple rhyme check based on ending similarity.

        Checks if two words rhyme by comparing their last two characters,
        providing a basic rhyme detection mechanism for internal rhyme analysis.

        Args:
            word1 (str): First word to check for rhyming.
            word2 (str): Second word to check for rhyming.

        Returns:
            bool: True if words rhyme (last 2 characters match), False otherwise.

        Note:
            This is a basic implementation that doesn't account for phonetic
            nuances or complex rhyme patterns. Used for internal rhyme detection
            within lines where more sophisticated analysis may be overkill.
        """
        if len(word1) < 2 or len(word2) < 2 or word1 == word2:
            return False
        return word1[-2:].lower() == word2[-2:].lower()

    def _calculate_advanced_composite_scores(
        self, analysis_results: dict[str, Any]
    ) -> dict[str, Any]:
        """Calculate advanced composite scores from individual analysis metrics.

        Combines multiple analysis components into comprehensive quality assessments
        including technical mastery, artistic sophistication, overall quality,
        and innovation scores using weighted algorithms.

        Args:
            analysis_results (dict[str, Any]): Dictionary containing all individual
                analysis results (rhyme_analysis, flow_analysis, vocabulary_sophistication,
                creativity_metrics, readability_metrics).

        Returns:
            dict[str, Any]: Composite scoring results containing:
                - composite_scores (dict): Weighted composite metrics including:
                    - technical_mastery (float): Technical skill assessment (rhyme + flow + vocab)
                    - artistic_sophistication (float): Artistic complexity score
                    - overall_quality (float): Combined quality assessment
                    - innovation_score (float): Creativity and novelty assessment
                    - complexity_balance (float): Balance between vocabulary richness and readability

        Note:
            Uses domain-specific weighting to combine technical precision (rhymes, flow)
            with artistic elements (creativity, vocabulary) for comprehensive evaluation.
            Readability is inversely weighted as more complex text indicates higher artistry.
        """
        # Extract key metrics
        rhyme_density = analysis_results.get("rhyme_analysis", {}).get(
            "rhyme_density", 0
        )
        flow_consistency = analysis_results.get("flow_analysis", {}).get(
            "syllable_consistency", 0
        )
        vocabulary_richness = analysis_results.get("vocabulary_sophistication", {}).get(
            "vocabulary_richness", 0
        )
        creativity_score = analysis_results.get("creativity_metrics", {}).get(
            "overall_creativity_score", 0
        )
        readability = analysis_results.get("readability_metrics", {}).get(
            "flesch_reading_ease", 0
        )

        # Normalize readability (Flesch scale: 0-100, higher = easier)
        normalized_readability = readability / 100

        # Composite metrics
        technical_mastery = (
            rhyme_density * 0.4 + flow_consistency * 0.4 + vocabulary_richness * 0.2
        )

        artistic_sophistication = (
            creativity_score * 0.5
            + vocabulary_richness * 0.3
            + (1 - normalized_readability) * 0.2
        )  # More complex text = higher artistry

        overall_quality = (
            technical_mastery * 0.4
            + artistic_sophistication * 0.4
            + creativity_score * 0.2
        )

        # Innovation
        innovation_score = creativity_score * 0.6 + rhyme_density * 0.4

        return {
            "composite_scores": {
                "technical_mastery": technical_mastery,
                "artistic_sophistication": artistic_sophistication,
                "overall_quality": overall_quality,
                "innovation_score": innovation_score,
                "complexity_balance": (
                    vocabulary_richness + (1 - normalized_readability)
                )
                / 2,
            }
        }

    def _calculate_advanced_confidence(
        self, analysis_results: dict[str, Any], lines: list[str], words: list[str]
    ) -> float:
        """Calculate advanced confidence score for analysis reliability assessment.

        Computes a comprehensive confidence metric based on multiple quality factors
        including text volume, analysis completeness, data quality, and vocabulary
        diversity to determine the reliability of the analysis results.

        Args:
            analysis_results (dict[str, Any]): Dictionary of completed analysis results
                to assess completeness and quality.
            lines (list[str]): List of lyric lines for quality assessment.
            words (list[str]): List of meaningful words for diversity analysis.

        Returns:
            float: Confidence score between 0.0 and 1.0, where higher values
                indicate more reliable analysis results.

        Note:
            Uses weighted combination of text volume, analysis completeness,
            data quality, and vocabulary diversity factors. Returns 0.5 as
            default when no confidence factors can be calculated.
        """
        confidence_factors = []

        # Text volume factor
        text_volume_factor = min(len(words) / 100, 1.0) * min(len(lines) / 10, 1.0)
        confidence_factors.append(text_volume_factor)

        # Analysis completeness factor
        expected_analyses = [
            "advanced_sentiment",
            "rhyme_analysis",
            "flow_analysis",
            "readability_metrics",
        ]
        completed_analyses = sum(
            1 for analysis in expected_analyses if analysis in analysis_results
        )
        completeness_factor = completed_analyses / len(expected_analyses)
        confidence_factors.append(completeness_factor)

        # Data quality factor
        if words and lines:
            avg_line_length = sum(len(line.split()) for line in lines) / len(lines)
            quality_factor = min(
                avg_line_length / 8, 1.0
            )  # Optimal line length ~ 8 words
            confidence_factors.append(quality_factor)

        # Vocabulary diversity factor
        if words:
            vocab_diversity = len(set(words)) / len(words)
            diversity_factor = min(vocab_diversity * 2, 1.0)
            confidence_factors.append(diversity_factor)

        return (
            sum(confidence_factors) / len(confidence_factors)
            if confidence_factors
            else 0.5
        )

    def _calculate_consistency_score(self, values: list[float]) -> float:
        """Calculate the consistency score of a list of values using coefficient of variation.

        This method measures how consistent a set of values are by calculating the
        coefficient of variation (standard deviation divided by mean) and converting
        it to a consistency score where higher values indicate more consistent data.

        Args:
            values: List of float values to analyze for consistency.

        Returns:
            Float between 0.0 and 1.0 representing consistency score, where:
            - 1.0 = perfectly consistent (all values identical)
            - 0.0 = highly inconsistent (very high variation)
            - Values closer to 1.0 indicate higher consistency

        Note:
            Returns 1.0 for lists with fewer than 2 values (no variation possible).
            Returns 1.0 when mean is 0 (avoids division by zero).
            Uses coefficient of variation formula: CV = Ïƒ/Î¼.
            Consistency score = 1 / (1 + CV) to normalize between 0 and 1.
        """
        if len(values) < 2:
            return 1.0

        mean = sum(values) / len(values)
        if mean == 0:
            return 1.0

        variance = sum((x - mean) ** 2 for x in values) / len(values)
        coefficient_of_variation = (variance**0.5) / mean

        # Consistency: the smaller the coefficient of variation, the higher the consistency
        consistency = 1 / (1 + coefficient_of_variation)
        return min(consistency, 1.0)

    # Methods for empty results
    def _empty_sentiment_result(self) -> dict[str, Any]:
        """Return an empty sentiment analysis result structure.

        This method provides a standardized empty result dictionary for sentiment
        analysis when no meaningful sentiment data can be extracted from the lyrics.

        Returns:
            Dictionary containing empty/default sentiment analysis results with keys:
            - emotion_scores: Empty dict for emotion intensity scores
            - dominant_emotion: Default "neutral" emotion
            - dominant_emotion_strength: Default strength of 0
            - emotional_valence: Default valence of 0.0 (neutral)
            - emotional_intensity: Default intensity of 0.0
            - total_emotional_words: Default count of 0
            - emotional_complexity: Default complexity of 0

        Note:
            Used when sentiment analysis cannot be performed due to insufficient
            text data or processing errors. All values are set to neutral/default states.
        """
        return {
            "emotion_scores": {},
            "dominant_emotion": "neutral",
            "dominant_emotion_strength": 0,
            "emotional_valence": 0.0,
            "emotional_intensity": 0.0,
            "total_emotional_words": 0,
            "emotional_complexity": 0,
        }

    def _empty_literary_result(self) -> dict[str, Any]:
        """Return an empty literary devices analysis result structure.

        This method provides a standardized empty result dictionary for literary
        devices analysis when no literary elements can be detected in the lyrics.

        Returns:
            Dictionary containing empty/default literary analysis results with keys:
            - metaphor_count: Default count of 0 metaphors
            - simile_count: Default count of 0 similes
            - time_references: Default count of 0 temporal references
            - contrast_usage: Default count of 0 contrast elements
            - repetition_analysis: Dict with repeated_lines count and repetition_ratio
            - personification_count: Default count of 0 personification instances
            - total_literary_devices: Default total count of 0

        Note:
            Used when literary devices analysis cannot be performed due to insufficient
            text data or processing errors. All literary device counts are set to zero.
        """
        return {
            "metaphor_count": 0,
            "simile_count": 0,
            "time_references": 0,
            "contrast_usage": 0,
            "repetition_analysis": {"repeated_lines": 0, "repetition_ratio": 0.0},
            "personification_count": 0,
            "total_literary_devices": 0,
        }

    def _empty_vocabulary_result(self) -> dict[str, Any]:
        """Return an empty vocabulary sophistication analysis result structure.

        This method provides a standardized empty result dictionary for vocabulary
        analysis when no meaningful vocabulary metrics can be calculated from the lyrics.

        Returns:
            Dictionary containing empty/default vocabulary analysis results with keys:
            - complexity_scores: Empty dict for word complexity scores
            - total_complex_words: Default count of 0 complex words
            - average_word_length: Default average length of 0.0
            - long_words_count: Default count of 0 long words
            - vocabulary_richness: Default richness score of 0.0
            - uncommon_words_ratio: Default ratio of 0.0
            - lexical_diversity: Default diversity score of 0.0
            - sophisticated_vocabulary_score: Default sophistication score of 0.0

        Note:
            Used when vocabulary analysis cannot be performed due to insufficient
            text data or processing errors. All vocabulary metrics are set to zero/neutral values.
        """
        return {
            "complexity_scores": {},
            "total_complex_words": 0,
            "average_word_length": 0.0,
            "long_words_count": 0,
            "vocabulary_richness": 0.0,
            "uncommon_words_ratio": 0.0,
            "lexical_diversity": 0.0,
            "sophisticated_vocabulary_score": 0.0,
        }

    def _empty_structure_result(self) -> dict[str, Any]:
        """Return an empty structural pattern analysis result structure.

        This method provides a standardized empty result dictionary for structural
        analysis when no meaningful structural patterns can be detected in the lyrics.

        Returns:
            Dictionary containing empty/default structural analysis results with keys:
            - total_lines: Default count of 0 lines
            - average_line_length: Default average length of 0.0
            - line_length_variance: Default variance of 0.0
            - punctuation_analysis: Empty dict for punctuation patterns
            - structure_patterns: Dict with pattern_found=False
            - stanza_analysis: Dict with stanza_count=0
            - structural_consistency: Default consistency score of 0.0

        Note:
            Used when structural analysis cannot be performed due to insufficient
            text data or processing errors. All structural metrics are set to zero/neutral values.
        """
        return {
            "total_lines": 0,
            "average_line_length": 0.0,
            "line_length_variance": 0.0,
            "punctuation_analysis": {},
            "structure_patterns": {"pattern_found": False},
            "stanza_analysis": {"stanza_count": 0},
            "structural_consistency": 0.0,
        }

    def _empty_creativity_result(self) -> dict[str, Any]:
        """Return an empty creativity and innovation analysis result structure.

        This method provides a standardized empty result dictionary for creativity
        analysis when no creative elements can be detected in the lyrics.

        Returns:
            Dictionary containing empty/default creativity analysis results with keys:
            - neologisms: Empty list for newly coined words
            - unique_phrases: Empty list for distinctive phrases
            - wordplay_analysis: Dict with total_score=0.0
            - innovative_rhymes: Dict with innovation_score=0.0
            - creativity_factors: List of 4 default creativity factor scores (all 0.0)
            - overall_creativity_score: Default overall score of 0.0

        Note:
            Used when creativity analysis cannot be performed due to insufficient
            text data or processing errors. All creativity metrics are set to zero values.
        """
        return {
            "neologisms": [],
            "unique_phrases": [],
            "wordplay_analysis": {"total_score": 0.0},
            "innovative_rhymes": {"innovation_score": 0.0},
            "creativity_factors": [0.0, 0.0, 0.0, 0.0],
            "overall_creativity_score": 0.0,
        }

    def get_analyzer_info(self) -> dict[str, Any]:
        """Get comprehensive information about this analyzer.

        Returns detailed metadata about the AdvancedAlgorithmicAnalyzer including
        its capabilities, configuration options, and performance characteristics.

        Returns:
            Dictionary containing analyzer information with keys:
            - name: Analyzer name ("AdvancedAlgorithmicAnalyzer")
            - version: Current version ("2.0.0")
            - description: Detailed description of analysis capabilities
            - author: Author information
            - type: Analyzer type identifier
            - supported_features: List of supported analysis features
            - components: List of internal analyzer components
            - config_options: Dictionary of configurable options with descriptions
            - performance: Dictionary with performance metrics and expectations

        Note:
            This method provides comprehensive metadata for integration with
            analysis frameworks and user interfaces. All information is static
            and describes the analyzer's capabilities and configuration options.
        """
        return {
            "name": "AdvancedAlgorithmicAnalyzer",
            "version": "2.0.0",
            "description": "Advanced algorithmic lyrics analysis with phonetic rhyme analysis, flow metrics, readability indices, and semantic sophistication scoring",
            "author": "Rap Scraper Project - Advanced Edition",
            "type": self.analyzer_type,
            "supported_features": self.supported_features,
            "components": [
                "AdvancedLexicon",
                "FlowAnalyzer",
                "RhymeAnalyzer",
                "ReadabilityAnalyzer",
                "LiteraryDevicesAnalyzer",
            ],
            "config_options": {
                "cache_enabled": "Enable result caching for performance (default: True)",
                "detailed_logging": "Enable detailed debug logging (default: False)",
                "min_word_length": "Minimum word length for analysis (default: 3)",
                "max_cache_size": "Maximum cache entries (default: 1000)",
            },
            "performance": {
                "typical_processing_time": "50-200ms per song",
                "memory_usage": "~5-10MB for cache",
                "scalability": "Excellent for batch processing",
            },
        }

    @property
    def analyzer_type(self) -> str:
        """Get the type identifier for this analyzer.

        Returns:
            String identifier for the analyzer type ("advanced_algorithmic").

        Note:
            This property provides a standardized way to identify the analyzer
            type for integration with analysis frameworks and result categorization.
        """
        return "advanced_algorithmic"

    @property
    def supported_features(self) -> list[str]:
        """Get the list of analysis features supported by this analyzer.

        Returns:
            List of strings representing all supported analysis features:
            - "phonetic_rhyme_analysis": Advanced rhyme detection using phonetics
            - "advanced_flow_metrics": Syllable counting and rhythmic analysis
            - "readability_indices": Multiple readability scoring algorithms
            - "emotional_gradient_analysis": Sentiment and emotional content analysis
            - "thematic_categorization": Theme identification and classification
            - "literary_devices_detection": Metaphor, simile, and literary element detection
            - "vocabulary_sophistication": Lexical complexity and sophistication metrics
            - "structural_pattern_analysis": Line structure and stanza pattern analysis
            - "creativity_assessment": Innovation and creative element evaluation
            - "composite_scoring": Combined multi-dimensional quality assessment
            - "performance_caching": Result caching for improved performance

        Note:
            This comprehensive feature set enables detailed algorithmic analysis
            of rap lyrics without requiring external AI models or language processing services.
        """
        return [
            "phonetic_rhyme_analysis",
            "advanced_flow_metrics",
            "readability_indices",
            "emotional_gradient_analysis",
            "thematic_categorization",
            "literary_devices_detection",
            "vocabulary_sophistication",
            "structural_pattern_analysis",
            "creativity_assessment",
            "composite_scoring",
            "performance_caching",
        ]

    def clear_cache(self):
        """Clear the analysis results cache.

        This method removes all cached analysis results from memory to free up
        resources or ensure fresh analysis on subsequent calls. The cache is used
        to improve performance by avoiding redundant calculations for identical inputs.

        Note:
            - Only clears the cache if it exists and is not None
            - Logs the cache clearing operation for debugging purposes
            - Does not affect the analyzer's configuration or functionality
            - Useful for memory management or forcing fresh analysis results
        """
        if self.analysis_cache:
            self.analysis_cache.clear()
            logger.info("Analysis cache cleared")


# Demonstration function
async def demo_advanced_analysis():
    """Demonstrate the capabilities of the advanced algorithmic analyzer.

    Runs a comprehensive demonstration of the AdvancedAlgorithmicAnalyzer
    using sample rap lyrics to showcase all analysis features including
    rhyme analysis, flow metrics, readability indices, and composite scoring.
    Uses predefined sample lyrics to demonstrate analysis features and prints
    detailed results to console with emojis and formatting. Serves as both
    a test function and user demonstration tool.

    This function creates an analyzer instance, processes sample lyrics,
    and displays formatted results showing the full range of algorithmic
    analysis capabilities without requiring external AI models.

    Returns:
        None: Prints demonstration results directly to stdout.

    Raises:
        Exception: If analyzer initialization or analysis fails.
            Error message is printed to stderr.
    """

    sample_lyrics = """
    Metaphors cascade like waterfalls in my mind
    Each syllable calculated, rhythmically designed
    Philosophy meets poetry in this verbal shrine
    Where consciousness and consonance perfectly align

    The lexicon I wield cuts deeper than a blade
    Multisyllabic mastery in every word I've made
    Assonance and alliteration, my lyrical trade
    While lesser wordsmiths stumble, my foundation's never swayed

    In the labyrinth of language, I navigate with ease
    Semantic sophistication brings critics to their knees
    Innovation flows through every line like autumn leaves
    This artistic architecture is what true genius achieves
    """

    print("ðŸš€ Ð”Ð•ÐœÐžÐÐ¡Ð¢Ð ÐÐ¦Ð˜Ð¯ ÐŸÐ ÐžÐ”Ð’Ð˜ÐÐ£Ð¢ÐžÐ“Ðž ÐÐ›Ð“ÐžÐ Ð˜Ð¢ÐœÐ˜Ð§Ð•Ð¡ÐšÐžÐ“Ðž ÐÐÐÐ›Ð˜Ð—ÐÐ¢ÐžÐ Ð")
    print("=" * 70)

    analyzer = AdvancedAlgorithmicAnalyzer(
        {"cache_enabled": True, "detailed_logging": True}
    )

    try:
        result = analyzer.analyze_song(
            "Demo Artist", "Advanced Analysis Demo", sample_lyrics
        )

        print("\nðŸ“Š Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð« ÐÐÐÐ›Ð˜Ð—Ð:")
        print(f"ðŸŽ¯ Ð£Ð²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ: {result['confidence']:.3f}")
        print(f"âš¡ Ð’Ñ€ÐµÐ¼Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸: {result['processing_time']:.3f}s")

        # Rhymes and sound
        rhyme_analysis = result["raw_output"].get("rhyme_analysis", {})
        print("\nðŸŽµ Ð Ð˜Ð¤ÐœÐ« Ð˜ Ð—Ð’Ð£Ð§ÐÐÐ˜Ð•:")
        print(f"  Ð¡Ñ…ÐµÐ¼Ð° Ñ€Ð¸Ñ„Ð¼Ð¾Ð²ÐºÐ¸: {rhyme_analysis.get('rhyme_scheme', 'N/A')}")
        print(f"  ÐŸÐ»Ð¾Ñ‚Ð½Ð¾ÑÑ‚ÑŒ Ñ€Ð¸Ñ„Ð¼: {rhyme_analysis.get('rhyme_density', 0):.3f}")
        print(f"  ÐÐ»Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ñ: {rhyme_analysis.get('alliteration_score', 0):.3f}")
        print(f"  Ð’Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ðµ Ñ€Ð¸Ñ„Ð¼Ñ‹: {rhyme_analysis.get('internal_rhymes', 0)}")

        # Flow analysis
        flow_analysis = result["raw_output"].get("flow_analysis", {})
        print("\nðŸŒŠ FLOW Ð˜ Ð Ð˜Ð¢Ðœ:")
        print(
            f"  ÐšÐ¾Ð½ÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ ÑÐ»Ð¾Ð³Ð¾Ð²: {flow_analysis.get('syllable_consistency', 0):.3f}"
        )
        print(
            f"  Ð¡Ñ€. ÑÐ»Ð¾Ð³Ð¾Ð² Ð½Ð° ÑÑ‚Ñ€Ð¾ÐºÑƒ: {flow_analysis.get('average_syllables_per_line', 0):.1f}"
        )
        print(
            f"  Ð Ð¸Ñ‚Ð¼Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¿Ð»Ð¾Ñ‚Ð½Ð¾ÑÑ‚ÑŒ: {flow_analysis.get('rhythmic_density', 0):.3f}"
        )

        # Readability
        readability = result["raw_output"].get("readability_metrics", {})
        print("\nðŸ“š Ð§Ð˜Ð¢ÐÐ‘Ð•Ð›Ð¬ÐÐžÐ¡Ð¢Ð¬:")
        print(f"  Flesch Reading Ease: {readability.get('flesch_reading_ease', 0):.1f}")
        print(f"  SMOG Index: {readability.get('smog_index', 0):.1f}")
        print(f"  ÐšÐ¾Ð½ÑÐµÐ½ÑÑƒÑ: {readability.get('readability_consensus', 'N/A')}")

        # Composite scores
        composite = result["raw_output"].get("composite_scores", {})
        print("\nðŸ† ÐšÐžÐœÐŸÐžÐ—Ð˜Ð¢ÐÐ«Ð• ÐžÐ¦Ð•ÐÐšÐ˜:")
        print(f"  Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¼Ð°ÑÑ‚ÐµÑ€ÑÑ‚Ð²Ð¾: {composite.get('technical_mastery', 0):.3f}")
        print(
            f"  ÐÑ€Ñ‚Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÑƒÑ‚Ð¾Ð½Ñ‡Ñ‘Ð½Ð½Ð¾ÑÑ‚ÑŒ: {composite.get('artistic_sophistication', 0):.3f}"
        )
        print(f"  ÐžÐ±Ñ‰ÐµÐµ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾: {composite.get('overall_quality', 0):.3f}")
        print(f"  Ð˜Ð½Ð½Ð¾Ð²Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾ÑÑ‚ÑŒ: {composite.get('innovation_score', 0):.3f}")

        print("=" * 70)
        print("âœ… Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾!")

    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸: {e}")
        import traceback

        traceback.print_exc()


# Class for working with PostgreSQL
class PostgreSQLAnalyzer:
    """PostgreSQL database analyzer for large-scale rap lyrics analysis.

    This class provides database integration for the AdvancedAlgorithmicAnalyzer,
    enabling batch processing and statistical analysis of large rap lyrics datasets
    stored in PostgreSQL. It handles database connections, query execution, and
    result aggregation for production-scale analysis workflows. The analyzer supports
    database statistics and metadata collection, batch processing of songs with
    configurable batch sizes, individual track analysis by ID, progress tracking
    and error handling, and summary statistics calculation across analysis results.
    Requires PostgreSQL with asyncpg driver and proper database configuration,
    and is designed for production use with 50K+ track datasets and concurrent processing.

    Attributes:
        analyzer: Instance of AdvancedAlgorithmicAnalyzer for core analysis logic.
        db_config: Database configuration loaded from config.yaml.
    """

    def __init__(self):
        """Initialize the PostgreSQL analyzer with database configuration.

        Sets up the analyzer instance and loads database configuration from
        the project's config.yaml file. Falls back to default localhost settings
        if configuration file is not found. Database configuration is loaded
        from config.yaml in the project root, and if the config file is missing,
        default PostgreSQL connection parameters are used.

        Returns:
            None
        """
        self.analyzer = AdvancedAlgorithmicAnalyzer(
            {"cache_enabled": True, "detailed_logging": False}
        )

        # PostgreSQL configuration (will be loaded from config.yaml)
        self.db_config = self._load_db_config()

    def _load_db_config(self) -> dict[str, Any]:
        """Load database configuration from the project's config.yaml file.

        Attempts to read database configuration from config.yaml in the project root.
        If the file doesn't exist or cannot be read, falls back to default localhost
        PostgreSQL connection parameters.

        Returns:
            Dictionary containing database configuration with keys:
            - host: Database server hostname (default: "localhost")
            - port: Database server port (default: 5432)
            - name: Database name (default: "rap_lyrics_db")
            - user: Database username (default: "postgres")
            - password: Database password (default: "password")

        Note:
            Configuration is loaded from config.yaml in the project root directory.
            If config file is missing, default values are used for local development.
            Errors during config loading are logged but don't prevent initialization.
        """
        try:
            import yaml

            config_path = Path(__file__).parent.parent.parent / "config.yaml"

            if config_path.exists():
                with open(config_path, encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                    return config.get("database", {})
            else:
                print("âš ï¸ config.yaml file not found, using default values")
                return {
                    "host": "localhost",
                    "port": 5432,
                    "name": "rap_lyrics_db",
                    "user": "postgres",
                    "password": "password",
                }
        except Exception as e:
            print(f"âš ï¸ Configuration loading error: {e}")
            return {}

    async def get_database_stats(self) -> dict[str, Any]:
        """Retrieve comprehensive statistics about the rap lyrics database.

        Connects to the PostgreSQL database and gathers detailed statistics about
        the tracks table, including total counts, artist diversity, lyrics availability,
        and text length distributions. Provides both raw numbers and percentage breakdowns.

        Returns:
            Dictionary containing database statistics with keys:
            - total_songs: Total number of tracks in the database
            - unique_artists: Number of distinct artists
            - songs_with_lyrics: Tracks that have lyrics text
            - analyzable_songs: Tracks with sufficient lyrics for analysis (>100 chars)
            - non_analyzable_songs: Tracks without adequate lyrics
            - avg_lyrics_length: Average length of lyrics in characters
            - min_lyrics_length: Minimum lyrics length
            - max_lyrics_length: Maximum lyrics length
            - median_lyrics_length: Median lyrics length

        Note:
            Requires active PostgreSQL connection and tracks table.
            Prints formatted statistics to console for user feedback.
            Handles connection errors gracefully and returns empty dict on failure.
        """
        try:
            import asyncpg

            conn = await asyncpg.connect(
                host=self.db_config.get("host", "localhost"),
                port=self.db_config.get("port", 5432),
                database=self.db_config.get("name", "rap_lyrics_db"),
                user=self.db_config.get("user", "postgres"),
                password=self.db_config.get("password", "password"),
            )

            try:
                # Query for total statistics
                total_stats_query = """
                SELECT
                    COUNT(*) as total_songs,
                    COUNT(DISTINCT artist) as unique_artists,
                    COUNT(CASE WHEN lyrics IS NOT NULL THEN 1 END) as songs_with_lyrics,
                    COUNT(CASE WHEN lyrics IS NOT NULL AND LENGTH(lyrics) > 100 THEN 1 END) as analyzable_songs,
                    COUNT(CASE WHEN lyrics IS NULL OR LENGTH(lyrics) <= 100 THEN 1 END) as non_analyzable_songs
                FROM tracks;
                """

                # Query statistics by text length (only for songs with lyrics)
                lyrics_stats_query = """
                SELECT
                    AVG(LENGTH(lyrics)) as avg_lyrics_length,
                    MIN(LENGTH(lyrics)) as min_lyrics_length,
                    MAX(LENGTH(lyrics)) as max_lyrics_length,
                    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY LENGTH(lyrics)) as median_lyrics_length
                FROM tracks
                WHERE lyrics IS NOT NULL;
                """

                total_result = await conn.fetchrow(total_stats_query)
                lyrics_result = await conn.fetchrow(lyrics_stats_query)

                stats = {
                    "total_songs": total_result["total_songs"],
                    "unique_artists": total_result["unique_artists"],
                    "songs_with_lyrics": total_result["songs_with_lyrics"],
                    "analyzable_songs": total_result["analyzable_songs"],
                    "non_analyzable_songs": total_result["non_analyzable_songs"],
                    "avg_lyrics_length": float(lyrics_result["avg_lyrics_length"])
                    if lyrics_result["avg_lyrics_length"]
                    else 0,
                    "min_lyrics_length": lyrics_result["min_lyrics_length"],
                    "max_lyrics_length": lyrics_result["max_lyrics_length"],
                    "median_lyrics_length": float(lyrics_result["median_lyrics_length"])
                    if lyrics_result["median_lyrics_length"]
                    else 0,
                }

                print("ðŸ“Š COMPLETE DATABASE STATISTICS:")
                print("=" * 50)
                print(f"  ðŸ“€ Total records in DB: {stats['total_songs']:,}")
                print(f"  ðŸŽ¤ Unique artists: {stats['unique_artists']:,}")
                print(f"  ðŸ“ Songs with lyrics: {stats['songs_with_lyrics']:,}")
                print(
                    f"  âœ… Songs for analysis (>100 chars): {stats['analyzable_songs']:,}"
                )
                print(
                    f"  âŒ Not suitable for analysis: {stats['non_analyzable_songs']:,}"
                )
                print("")
                print("ðŸ“ TEXT LENGTH STATISTICS:")
                print(f"  Average length: {stats['avg_lyrics_length']:.0f} characters")
                print(
                    f"  Median length: {stats['median_lyrics_length']:.0f} characters"
                )
                print(
                    f"  Range: {stats['min_lyrics_length']:,} - {stats['max_lyrics_length']:,} characters"
                )

                # Calculate percentage ratios
                if stats["total_songs"] > 0:
                    lyrics_percent = (
                        stats["songs_with_lyrics"] / stats["total_songs"]
                    ) * 100
                    analyzable_percent = (
                        stats["analyzable_songs"] / stats["total_songs"]
                    ) * 100
                    print("")
                    print("ðŸ“Š PERCENTAGE RATIOS:")
                    print(f"  ÐŸÐµÑÐµÐ½ Ñ Ñ‚ÐµÐºÑÑ‚Ð°Ð¼Ð¸: {lyrics_percent:.1f}%")
                    print(f"  Suitable for analysis: {analyzable_percent:.1f}%")

                return stats

            finally:
                await conn.close()

        except Exception as e:
            print(f"âŒ Error retrieving database statistics: {e}")
            return {}

    async def analyze_all_songs(
        self, limit: int | None = None, batch_size: int = 100
    ) -> dict[str, Any]:
        """Analyze all songs in the database with batch processing and progress tracking.

        Performs comprehensive algorithmic analysis on all tracks in the database that
        have sufficient lyrics (>100 characters). Processes songs in configurable batches
        with progress reporting and error handling for individual tracks.

        Args:
            limit: Optional maximum number of songs to analyze. If None, analyzes all eligible songs.
            batch_size: Number of songs to process in each batch (default: 100).

        Returns:
            Dictionary containing analysis results with keys:
            - total_processed: Number of songs successfully analyzed
            - results: List of individual analysis results with song metadata
            - summary_stats: Aggregated statistics across all analyzed songs

        Note:
            - Only analyzes tracks with lyrics longer than 100 characters
            - Processes songs in batches to manage memory and provide progress feedback
            - Individual song analysis errors don't stop the overall process
            - Returns comprehensive summary statistics for quality assessment
        """
        try:
            import asyncpg

            conn = await asyncpg.connect(
                host=self.db_config.get("host", "localhost"),
                port=self.db_config.get("port", 5432),
                database=self.db_config.get("name", "rap_lyrics_db"),
                user=self.db_config.get("user", "postgres"),
                password=self.db_config.get("password", "password"),
            )

            try:
                # Query songs for analysis
                limit_clause = f"LIMIT {limit}" if limit else ""
                query = f"""
                SELECT id, artist, title, lyrics
                FROM tracks
                WHERE lyrics IS NOT NULL AND LENGTH(lyrics) > 100
                ORDER BY id
                {limit_clause};
                """

                tracks = await conn.fetch(query)
                total_songs = len(tracks)

                print(
                    f"ðŸš€ Starting analysis of {total_songs:,} songs (batches of {batch_size})"
                )

                processed = 0
                results = []

                # Process in batches
                for i in range(0, total_songs, batch_size):
                    batch = tracks[i : i + batch_size]
                    batch_results = []

                    for song in batch:
                        try:
                            # Analyze song
                            result = self.analyzer.analyze_song(
                                artist=song["artist"],
                                title=song["title"],
                                lyrics=song["lyrics"],
                            )

                            batch_results.append(
                                {
                                    "song_id": song["id"],
                                    "artist": song["artist"],
                                    "title": song["title"],
                                    "analysis": result["raw_output"],
                                    "confidence": result["confidence"],
                                    "processing_time": result["processing_time"],
                                }
                            )

                            processed += 1

                        except Exception as e:
                            print(f"âš ï¸ Error analyzing song {song['id']}: {e}")

                    results.extend(batch_results)

                    # Progress update
                    progress = (processed / total_songs) * 100
                    print(
                        f"ðŸ“ˆ Progress: {processed:,}/{total_songs:,} ({progress:.1f}%)"
                    )

                print(f"âœ… Analysis completed! Processed {processed:,} songs")

                return {
                    "total_processed": processed,
                    "results": results,
                    "summary_stats": self._calculate_summary_stats(results),
                }

            finally:
                await conn.close()

        except Exception as e:
            print(f"âŒ Error analyzing songs: {e}")
            return {}

    async def analyze_single_track(self, track_id: int) -> dict[str, Any]:
        """Analyze a single track by its database ID.

        Retrieves a specific track from the database by ID and performs comprehensive
        algorithmic analysis if the track has sufficient lyrics content.

        Args:
            track_id: The unique identifier of the track in the database.

        Returns:
            Dictionary containing analysis results with keys:
            - song_id: The track's database ID
            - artist: Artist name
            - title: Song title
            - analysis: Complete algorithmic analysis results
            - confidence: Analysis confidence score (0.0-1.0)
            - processing_time: Time taken for analysis in seconds

        Note:
            - Returns empty dict if track is not found or has insufficient lyrics
            - Requires lyrics longer than 100 characters for meaningful analysis
            - Prints formatted analysis results to console for user feedback
            - Handles database connection errors gracefully
        """
        try:
            import asyncpg

            conn = await asyncpg.connect(
                host=self.db_config.get("host", "localhost"),
                port=self.db_config.get("port", 5432),
                database=self.db_config.get("name", "rap_lyrics_db"),
                user=self.db_config.get("user", "postgres"),
                password=self.db_config.get("password", "password"),
            )

            try:
                # Retrieve the song
                query = "SELECT id, artist, title, lyrics FROM tracks WHERE id = $1"
                song = await conn.fetchrow(query, track_id)

                if not song:
                    print(f"âŒ Track with ID {track_id} not found")
                    return {}

                if not song["lyrics"] or len(song["lyrics"]) < 100:
                    print(f"âŒ Insufficient text for analysis (ID: {track_id})")
                    return {}

                print(f"ðŸŽµ Analyzing: {song['artist']} - {song['title']}")

                # Perform analysis
                result = self.analyzer.analyze_song(
                    artist=song["artist"], title=song["title"], lyrics=song["lyrics"]
                )

                # Display formatted results
                self._print_analysis_results(result)

                return {
                    "song_id": song["id"],
                    "artist": song["artist"],
                    "title": song["title"],
                    "analysis": result["raw_output"],
                    "confidence": result["confidence"],
                    "processing_time": result["processing_time"],
                }

            finally:
                await conn.close()

        except Exception as e:
            print(f"âŒ Error analyzing track {track_id}: {e}")
            return {}

    def _calculate_summary_stats(self, results: list[dict[str, Any]]) -> dict[str, Any]:
        """Calculate summary statistics across a collection of analysis results.

        Aggregates key metrics from multiple song analyses to provide insights
        into the overall quality and characteristics of the analyzed dataset.

        Args:
            results: List of individual analysis result dictionaries.

        Returns:
            Dictionary containing aggregated statistics with keys:
            - avg_confidence: Average confidence score across all results
            - avg_processing_time: Average processing time in seconds
            - avg_technical_mastery: Average technical mastery score
            - avg_artistic_sophistication: Average artistic sophistication score
            - total_results: Total number of results processed

        Note:
            - Returns empty dict if no results are provided
            - Safely handles missing composite score data
            - Provides comprehensive overview of analysis quality and performance
        """
        if not results:
            return {}

        # Extract metrics
        confidences = [r["confidence"] for r in results]
        processing_times = [r["processing_time"] for r in results]

        # Composite scores
        technical_scores = []
        artistic_scores = []

        for result in results:
            composite = result.get("analysis", {}).get("composite_scores", {})
            if composite:
                technical_scores.append(composite.get("technical_mastery", 0))
                artistic_scores.append(composite.get("artistic_sophistication", 0))

        return {
            "avg_confidence": sum(confidences) / len(confidences),
            "avg_processing_time": sum(processing_times) / len(processing_times),
            "avg_technical_mastery": sum(technical_scores) / len(technical_scores)
            if technical_scores
            else 0,
            "avg_artistic_sophistication": sum(artistic_scores) / len(artistic_scores)
            if artistic_scores
            else 0,
            "total_results": len(results),
        }

    def _print_analysis_results(self, result: dict[str, Any]):
        """Print formatted analysis results to the console.

        Displays comprehensive algorithmic analysis results in a user-friendly
        format with emojis and clear section headers for easy interpretation.
        The output includes rhyme analysis, flow metrics, and composite scores
        with enhanced readability through emojis and formatting. Safely handles
        missing analysis components for robust CLI output.

        Args:
            result (dict[str, Any]): Analysis result dictionary with the following keys:
                - 'confidence' (float): Analysis confidence score
                - 'processing_time' (float): Processing time in seconds
                - 'raw_output' (dict): Contains 'rhyme_analysis', 'flow_analysis',
                  and 'composite_scores' sub-dictionaries

        Returns:
            None: Prints directly to stdout.

        Examples:
            >>> result = {
            ...     'confidence': 0.95,
            ...     'processing_time': 1.23,
            ...     'raw_output': {'rhyme_analysis': {...}}
            ... }
            >>> analyzer._print_analysis_results(result)
            ðŸ“Š ANALYSIS RESULTS:
            ðŸŽ¯ Confidence: 0.950
            ...
        """
        print("\nðŸ“Š ANALYSIS RESULTS:")
        print(f"ðŸŽ¯ Confidence: {result['confidence']:.3f}")
        print(f"âš¡ Processing time: {result['processing_time']:.3f}s")

        # Rhymes and sound
        rhyme_analysis = result["raw_output"].get("rhyme_analysis", {})
        if rhyme_analysis:
            print("\nðŸŽµ RHYMES AND SOUND:")
            print(f"  Rhyme scheme: {rhyme_analysis.get('rhyme_scheme', 'N/A')}")
            print(f"  Rhyme density: {rhyme_analysis.get('rhyme_density', 0):.3f}")
            print(f"  Alliteration: {rhyme_analysis.get('alliteration_score', 0):.3f}")
            print(f"  Internal rhymes: {rhyme_analysis.get('internal_rhymes', 0)}")

        # Flow analysis
        flow_analysis = result["raw_output"].get("flow_analysis", {})
        if flow_analysis:
            print("\nðŸŒŠ FLOW AND RHYTHM:")
            print(
                f"  Syllable consistency: {flow_analysis.get('syllable_consistency', 0):.3f}"
            )
            print(
                f"  Avg syllables per line: {flow_analysis.get('average_syllables_per_line', 0):.1f}"
            )
            print(f"  Rhythmic density: {flow_analysis.get('rhythmic_density', 0):.3f}")

        # Composite scores
        composite = result["raw_output"].get("composite_scores", {})
        if composite:
            print("\nðŸ† COMPOSITE SCORES:")
            print(f"  Technical mastery: {composite.get('technical_mastery', 0):.3f}")
            print(
                f"  Artistic sophistication: {composite.get('artistic_sophistication', 0):.3f}"
            )
            print(f"  Overall quality: {composite.get('overall_quality', 0):.3f}")
            print(f"  Innovation score: {composite.get('innovation_score', 0):.3f}")


async def main():
    """Main function for PostgreSQL-based rap lyrics analysis.

    Provides a command-line interface for the AdvancedAlgorithmicAnalyzer
    with PostgreSQL integration. Supports database statistics, batch analysis,
    individual track analysis, and demonstration modes. Requires PostgreSQL
    database connection and proper configuration. Designed for production use
    with large-scale rap lyrics datasets and provides comprehensive CLI interface
    with progress tracking and error handling.

    The CLI accepts several command-line arguments: --stats displays database
    statistics, --analyze-all analyzes all tracks in the database, --analyze-track ID
    analyzes a specific track by its ID, --limit N limits the number of tracks
    for analysis, --batch-size N sets the batch size for processing (default: 100),
    and --demo runs the analyzer demonstration with sample lyrics.

    Returns:
        None: Outputs results directly to stdout.

    Raises:
        Exception: If database connection fails or analysis errors occur.
            Traceback is printed to stderr for debugging.

    Examples:
        Run from command line:

        $ python src/analyzers/algorithmic_analyzer.py --stats
        $ python src/analyzers/algorithmic_analyzer.py --analyze-all --limit 100
        $ python src/analyzers/algorithmic_analyzer.py --analyze-track 123
    """
    import argparse

    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description="Advanced algorithmic analyzer for PostgreSQL"
    )

    parser.add_argument("--stats", action="store_true", help="Show database statistics")
    parser.add_argument(
        "--analyze-all", action="store_true", help="Analyze all tracks in database"
    )
    parser.add_argument(
        "--analyze-track",
        type=int,
        metavar="ID",
        help="Analyze specific track by ID",
    )
    parser.add_argument(
        "--limit",
        type=int,
        metavar="N",
        help="Limit number of tracks for analysis",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        metavar="N",
        help="Batch size for processing (default: 100)",
    )
    parser.add_argument(
        "--demo", action="store_true", help="Run analyzer demonstration"
    )

    args = parser.parse_args()

    # If no arguments provided, show beautiful menu
    action_args = [args.stats, args.analyze_all, args.analyze_track, args.demo]
    if not any(action_args):
        print()
        print("ðŸ§® ADVANCED ALGORITHMIC ANALYZER FOR POSTGRESQL")
        print("=" * 65)
        print("ðŸŽ¯ Professional text analysis without AI models")
        print("âš¡ Working with 57K+ tracks in PostgreSQL database")
        print("ðŸ“Š Detailed metrics: rhymes, flow, readability, emotions")
        print()
        print("ðŸ–¥ï¸ CLI INTERFACE:")
        print("  --stats              ðŸ“Š Show database statistics")
        print("  --analyze-all        ðŸš€ Analyze all tracks")
        print("  --analyze-track ID   ðŸŽµ Analyze specific track")
        print("  --limit N            ðŸ”¢ Limit number of tracks")
        print("  --batch-size N       ðŸ“¦ Batch size for processing")
        print("  --demo               ðŸŽ­ Run analyzer demonstration")
        print("  --help               â“ Full help")
        print()
        print("ðŸ’¡ USAGE EXAMPLES:")
        print("  python src/analyzers/algorithmic_analyzer.py --stats")
        print(
            "  python src/analyzers/algorithmic_analyzer.py --analyze-all --limit 100"
        )
        print("  python src/analyzers/algorithmic_analyzer.py --analyze-track 123")
        print("  python src/analyzers/algorithmic_analyzer.py --demo")
        print()
        print("ðŸ“ˆ ANALYSIS METRICS:")
        print("  ðŸŽµ Rhymes: scheme, density, phonetic similarity")
        print("  ðŸŒŠ Flow: syllable consistency, rhythmic density")
        print("  ðŸ“š Readability: Flesch, SMOG, ARI indices")
        print("  ðŸ’­ Emotions: valence, intensity, complexity")
        print("  ðŸŽ¨ Themes: money, street, success, relationships")
        print("=" * 65)
        return

    # Create analyzer only when actually needed
    try:
        if args.demo:
            print("ðŸš€ Starting demonstration...")
            await demo_advanced_analysis()

        else:
            # Initialize analyzer for database work
            print("âœ… PostgreSQL analyzer initialized")
            analyzer = PostgreSQLAnalyzer()

            if args.stats:
                print("ðŸ“Š Retrieving database statistics...")
                await analyzer.get_database_stats()

            elif args.analyze_track:
                print(f"ðŸŽµ Analyzing track ID: {args.analyze_track}")
                await analyzer.analyze_single_track(args.analyze_track)

            elif args.analyze_all:
                print("ðŸš€ Mass analysis of all tracks...")
                results = await analyzer.analyze_all_songs(
                    limit=args.limit, batch_size=args.batch_size
                )

                if results:
                    summary = results.get("summary_stats", {})
                    print("\nðŸ“ˆ SUMMARY STATISTICS:")
                    print(f"  Processed tracks: {summary.get('total_results', 0):,}")
                    print(
                        f"  Average confidence: {summary.get('avg_confidence', 0):.3f}"
                    )
                    print(
                        f"  Average processing time: {summary.get('avg_processing_time', 0):.3f}s"
                    )
                    print(
                        f"  Average technical mastery: {summary.get('avg_technical_mastery', 0):.3f}"
                    )
                    print(
                        f"  Average artistic sophistication: {summary.get('avg_artistic_sophistication', 0):.3f}"
                    )

    except Exception as e:
        print(f"âŒ Execution error: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
