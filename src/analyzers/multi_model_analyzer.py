"""Multi-model AI analyzer for rap lyrics with safety validation and interpretability.

This module provides comprehensive AI-powered analysis of rap song lyrics using
multiple provider models with automatic fallback, safety validation, and hallucination
detection. It supports local models (Ollama), cloud APIs (Google Gemma), and mock
providers for testing.

Key Features:
    - Multi-provider AI analysis with automatic fallback (Ollama -> Gemma -> Mock)
    - Safety validation and hallucination detection to ensure reliable results
    - Interpretable analysis with decision explanations and confidence scores
    - Batch processing with PostgreSQL storage and async support
    - Cost optimization (prioritizes free local models)
    - Comprehensive quality metrics and authenticity scoring

Architecture:
    - ModelProvider: Base class for AI providers (Ollama, Gemma, Mock)
    - MultiModelAnalyzer: Main analyzer with fallback logic
    - SafetyValidator: Validates analysis reliability and detects hallucinations
    - InterpretableAnalyzer: Generates explanations for AI decisions
    - PostgreSQLManager: Async database connection management

Typical Usage:
    Basic analysis:
        analyzer = MultiModelAnalyzer()
        await analyzer.initialize()
        result = analyzer.analyze_song("Kendrick Lamar", "HUMBLE.", lyrics)
        await analyzer.close()

    Analysis with safety validation:
        result = analyzer.analyze_song_with_safety("Drake", "Hotline Bling", lyrics)
        if result['is_safe']:
            print(f"Reliable: {result['summary']}")

    Explainable analysis:
        explainable = analyzer.analyze_with_explanations("Artist", "Title", lyrics)
        print(f"Confidence: {explainable.confidence:.2f}")
        print(f"Explanations: {explainable.explanation}")

    Batch processing:
        await analyzer.batch_analyze_from_db(limit=100)

Dependencies:
    - asyncpg, psycopg2-binary: PostgreSQL connectivity
    - ollama: Local model inference (optional)
    - google-generativeai: Gemma API access (optional)
    - pydantic: Data validation and models
    - requests: HTTP requests for Ollama API

Environment Variables:
    - POSTGRES_HOST: PostgreSQL server (default: localhost)
    - POSTGRES_PORT: PostgreSQL port (default: 5432)
    - POSTGRES_DATABASE: Database name (default: rap_lyrics)
    - POSTGRES_USERNAME: Database user (default: rap_user)
    - POSTGRES_PASSWORD: Database password (required)
    - GOOGLE_API_KEY: Google Gemma API key (optional)

Author:
    AI Assistant

Version:
    2.0.0 - Multi-model with safety validation
"""

# TODO(code_review): [HIGH] Move imports to follow Google Python Style Guide order:
# 1. Standard library imports
# 2. Third-party imports
# 3. Local application imports
# Currently mixing all import types without clear separation
import asyncio
import json
import logging
import os
import re
from datetime import datetime

import asyncpg
import psycopg2
import requests
from dotenv import load_dotenv
from psycopg2.extras import RealDictCursor
from pydantic import BaseModel, Field

# TODO(code_review): [CRITICAL] Avoid module-level side effects (load_dotenv())
# Move to main() or create explicit initialization function
# This breaks testability and causes issues with import order
# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# TODO(code_review): [HIGH] Avoid module-level logging configuration
# This affects global logging state and breaks when imported as library
# Move to main() or use __name__ == "__main__" guard
# Consider using logging.getLogger(__name__).setLevel() instead
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("ai_analysis.log", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


# ===== PostgreSQL Configuration =====
# TODO(code_review): [HIGH] Convert to dataclass or use __init__ for proper instance attributes
# Current implementation uses class attributes which are shared across all instances
# This can lead to unexpected behavior and testing issues
# TODO(code_review): [CRITICAL] NEVER hardcode credentials, even as defaults
# Remove "securepassword123" default - fail fast if password not provided
# Use required environment variables or raise ConfigurationError
class DatabaseConfig:
    """PostgreSQL database connection configuration.

    Configuration class for PostgreSQL connection parameters loaded from
    environment variables with sensible defaults.

    Attributes:
        host: PostgreSQL server hostname (default: localhost).
        port: PostgreSQL server port (default: 5432).
        database: Target database name (default: rap_lyrics).
        username: Database username for authentication (default: rap_user).
        password: Database password for authentication (default: securepassword123).
        max_connections: Maximum connection pool size (default: 20).
        min_connections: Minimum connection pool size (default: 5).

    Note:
        All attributes are loaded from environment variables with POSTGRES_ prefix.
        Connection pooling parameters should be tuned based on expected load.
    """
    # TODO(code_review): [MEDIUM] Extract magic numbers to named constants at module level
    # DEFAULT_POSTGRES_HOST = "localhost"
    # DEFAULT_POSTGRES_PORT = 5432
    # etc.

    host: str = os.getenv("POSTGRES_HOST", "localhost")
    port: int = int(os.getenv("POSTGRES_PORT", "5432"))
    database: str = os.getenv("POSTGRES_DATABASE", "rap_lyrics")
    username: str = os.getenv("POSTGRES_USERNAME", "rap_user")
    password: str = os.getenv("POSTGRES_PASSWORD", "securepassword123")  # TODO(code_review): [CRITICAL] SECURITY: Remove hardcoded password!
    max_connections: int = int(os.getenv("POSTGRES_MAX_CONNECTIONS", "20"))
    min_connections: int = int(os.getenv("POSTGRES_MIN_CONNECTIONS", "5"))


# ===== Data Models =====
class SongMetadata(BaseModel):
    """Song metadata and high-level characteristics.

    Pydantic model for storing basic song metadata including genre classification,
    emotional characteristics, and content warnings.

    Attributes:
        genre: Music genre classification (e.g., "rap", "trap", "drill", "old_school").
            Default: "rap".
        mood: Emotional mood/tone (e.g., "aggressive", "melancholic", "energetic", "neutral").
            Default: "neutral".
        energy_level: Energy intensity level ("low", "medium", "high").
            Default: "medium".
        explicit_content: Whether song contains explicit language or mature themes.
            Default: False.
    """

    genre: str = Field(default="rap")
    mood: str = Field(default="neutral")
    energy_level: str = Field(default="medium")
    explicit_content: bool = Field(default=False)


class LyricsAnalysis(BaseModel):
    """Detailed lyrics structure and literary analysis.

    Pydantic model for in-depth analysis of lyrical content, structure,
    themes, and artistic techniques.

    Attributes:
        structure: Song structure pattern (e.g., "verse-chorus-verse", "freestyle", "hook").
            Default: "verse".
        rhyme_scheme: Rhyme pattern (e.g., "AABB", "ABAB", "complex", "simple").
            Default: "unknown".
        complexity_level: Lyrical complexity rating ("beginner", "intermediate", "advanced").
            Default: "intermediate".
        main_themes: List of identified thematic elements (e.g., ["money", "street_life"]).
            Default: empty list.
        emotional_tone: Overall emotional tone ("positive", "negative", "neutral", "mixed").
            Default: "neutral".
        storytelling_type: Narrative style ("narrative", "abstract", "conversational").
            Default: "conversational".
        wordplay_quality: Quality of wordplay and linguistic creativity ("basic", "good", "excellent").
            Default: "basic".
    """

    structure: str = Field(default="verse")
    rhyme_scheme: str = Field(default="unknown")
    complexity_level: str = Field(default="intermediate")
    main_themes: list[str] = Field(default_factory=list)
    emotional_tone: str = Field(default="neutral")
    storytelling_type: str = Field(default="conversational")
    wordplay_quality: str = Field(default="basic")


class QualityMetrics(BaseModel):
    """Quality and authenticity metrics for song analysis.

    Pydantic model for quantitative quality assessments across multiple dimensions
    including authenticity, creativity, commercial viability, and AI detection.

    Attributes:
        authenticity_score: Perceived authenticity and genuineness (0.0-1.0).
            Higher values indicate more authentic street/real expression.
            Default: 0.5.
        lyrical_creativity: Creative and linguistic innovation level (0.0-1.0).
            Measures wordplay, metaphors, and unique expression.
            Default: 0.5.
        commercial_appeal: Mainstream commercial potential (0.0-1.0).
            Likelihood of broad audience appeal and radio play.
            Default: 0.5.
        uniqueness: Originality and distinctiveness (0.0-1.0).
            How unique the style and content are.
            Default: 0.5.
        overall_quality: Aggregate quality rating ("poor", "fair", "good", "excellent").
            Default: "fair".
        ai_likelihood: Probability lyrics are AI-generated (0.0-1.0).
            Higher values suggest potential AI authorship.
            Default: 0.5.

    Note:
        All float metrics are constrained to [0.0, 1.0] range via Pydantic validation.
    """

    authenticity_score: float = Field(default=0.5, ge=0.0, le=1.0)
    lyrical_creativity: float = Field(default=0.5, ge=0.0, le=1.0)
    commercial_appeal: float = Field(default=0.5, ge=0.0, le=1.0)
    uniqueness: float = Field(default=0.5, ge=0.0, le=1.0)
    overall_quality: str = Field(default="fair")
    ai_likelihood: float = Field(default=0.5, ge=0.0, le=1.0)


class EnhancedSongData(BaseModel):
    """Complete AI analysis results for a song.

    Comprehensive analysis result combining metadata, lyrical analysis,
    quality metrics, and analysis metadata.

    Attributes:
        artist: Artist/performer name.
        title: Song title.
        metadata: High-level metadata (genre, mood, energy, explicit).
        lyrics_analysis: Detailed lyrical analysis (structure, themes, complexity).
        quality_metrics: Quality scores (authenticity, creativity, commercial appeal).
        model_used: Name of AI model/provider used (e.g., "ollama", "gemma-2-27b-it").
        analysis_date: ISO 8601 timestamp of analysis completion.

    Example:
        >>> data = EnhancedSongData(
        ...     artist="Kendrick Lamar",
        ...     title="HUMBLE.",
        ...     metadata=SongMetadata(genre="trap", mood="aggressive"),
        ...     lyrics_analysis=LyricsAnalysis(complexity_level="advanced"),
        ...     quality_metrics=QualityMetrics(authenticity_score=0.92),
        ...     model_used="ollama-llama3.2",
        ...     analysis_date="2025-11-02T10:30:00"
        ... )
    """

    artist: str
    title: str
    metadata: SongMetadata
    lyrics_analysis: LyricsAnalysis
    quality_metrics: QualityMetrics
    model_used: str
    analysis_date: str


class ExplainableAnalysisResult(BaseModel):
    """Analysis result with AI decision explanations and interpretability.

    Extended analysis result that includes base analysis plus interpretability
    features: explanations, confidence scores, decision factors, and influential phrases.

    Attributes:
        analysis: Base EnhancedSongData analysis result.
        explanation: Category-keyed explanations for AI decisions.
            Keys: "genre_indicators", "mood_triggers", "authenticity_markers", "quality_indicators".
            Values: List of human-readable explanation strings.
        confidence: Overall confidence score in analysis (0.0-1.0).
            Based on text length, genre evidence, metric consistency, and detail presence.
        decision_factors: Dictionary of factor names to importance scores (0.0-1.0).
            E.g., {"trap_keywords": 0.85, "authenticity": 0.73, "word_diversity": 0.67}.
        influential_phrases: Category-keyed lists of influential lyrics phrases.
            Keys: "genre_phrases", "mood_phrases", "authenticity_phrases", "quality_phrases".
            Values: Lists of actual lyrics lines that influenced the decision.

    Example:
        >>> result = ExplainableAnalysisResult(
        ...     analysis=enhanced_data,
        ...     explanation={"genre_indicators": ["Genre 'trap' detected: –º–æ–ª–ª–∏, lean, —Å–∫—Ä—Ä"]},
        ...     confidence=0.87,
        ...     decision_factors={"trap_keywords": 0.92, "authenticity": 0.78},
        ...     influential_phrases={"genre_phrases": ["–ú–æ–ª–ª–∏ –≤ –º–æ–µ–π —á–∞—à–∫–µ, —è lean –ø—å—é"]}
        ... )
    """

    analysis: EnhancedSongData
    explanation: dict[str, list[str]]
    confidence: float
    decision_factors: dict[str, float]
    influential_phrases: dict[str, list[str]]


# ===== PostgreSQL Database Manager =====
class PostgreSQLManager:
    """PostgreSQL connection manager with async connection pooling.

    Manages asyncpg connection pool for efficient async database operations.
    Supports both async and synchronous connection modes with automatic
    connection lifecycle management.

    Attributes:
        config: DatabaseConfig instance with connection parameters.
        pool: asyncpg connection pool (None until initialized).
        logger: Logger instance for database operations.

    Example:
        >>> db = PostgreSQLManager()
        >>> await db.initialize()
        >>> async with db.get_connection() as conn:
        ...     result = await conn.fetch("SELECT * FROM tracks LIMIT 10")
        >>> await db.close()
    """

    def __init__(self, config: DatabaseConfig = None):
        """Initialize PostgreSQL manager with configuration.

        Args:
            config: DatabaseConfig instance. If None, creates default config
                from environment variables.

        Note:
            Connection pool is not created until initialize() is called.
        """
        self.config = config or DatabaseConfig()
        self.pool = None
        self.logger = logging.getLogger(f"{__name__}.PostgreSQLManager")

    # TODO(code_review): [MEDIUM] Add type hints for async context manager protocol
    # Consider implementing __aenter__ and __aexit__ for proper async context manager
    async def initialize(self) -> bool:
        """Initialize asyncpg connection pool and test connectivity.

        Creates connection pool with configured min/max size and tests
        database connectivity by executing a simple query.

        Returns:
            True if pool initialized and test query succeeds, False otherwise.

        Side Effects:
            - Creates self.pool asyncpg connection pool
            - Logs initialization status

        Note:
            This method is idempotent - calling multiple times recreates the pool.
            Connection timeout is set to 60 seconds for all queries.
        """
        try:
            self.logger.info("Initializing PostgreSQL connection pool")
            # TODO(code_review): [HIGH] Avoid string interpolation for DSN with credentials
            # Use asyncpg.create_pool() parameters directly for better security
            # Current approach logs credentials if dsn variable is printed
            dsn = f"postgresql://{self.config.username}:{self.config.password}@{self.config.host}:{self.config.port}/{self.config.database}"
            # TODO(code_review): [MEDIUM] Extract magic number 60 to named constant
            # COMMAND_TIMEOUT_SECONDS = 60
            self.pool = await asyncpg.create_pool(
                dsn,
                min_size=self.config.min_connections,
                max_size=self.config.max_connections,
                command_timeout=60,  # TODO(code_review): [MEDIUM] Magic number - extract to constant
                server_settings={
                    "application_name": "multi_model_analyzer",
                    "timezone": "UTC",
                },
            )

            # Test connection
            async with self.pool.acquire() as conn:
                await conn.execute("SELECT 1")
            # TODO(code_review): [LOW] Avoid emoji in production logs - breaks parsing/monitoring tools
            # Use structured logging with severity levels instead
            self.logger.info("‚úÖ PostgreSQL connection pool initialized successfully")
            return True

        except Exception as e:
            # TODO(code_review): [HIGH] Catch specific exceptions (asyncpg.PostgresError, etc.)
            # Generic Exception catching hides bugs and makes debugging harder
            # TODO(code_review): [MEDIUM] Add exception context: logger.error(..., exc_info=True)
            self.logger.error(f"‚ùå Failed to initialize PostgreSQL: {e}")
            return False

    async def get_connection(self):
        """Get connection from pool, initializing if necessary.

        Returns:
            asyncpg.pool.PoolAcquireContext: Connection context manager.
                Use with async context manager pattern.

        Side Effects:
            If pool not initialized, calls initialize() automatically.

        Example:
            >>> async with db.get_connection() as conn:
            ...     rows = await conn.fetch("SELECT * FROM tracks")
        """
        if not self.pool:
            await self.initialize()
        return self.pool.acquire()

    async def close(self):
        """Close connection pool and release all connections.

        Gracefully closes all pooled connections and resets pool to None.
        Safe to call multiple times (idempotent).

        Side Effects:
            - Closes all active connections in pool
            - Sets self.pool to None
        """
        if self.pool:
            await self.pool.close()
            self.pool = None

    def get_sync_connection(self):
        """Get synchronous psycopg2 connection for non-async operations.

        Creates a new synchronous connection using psycopg2 with RealDictCursor
        for dict-style row access. Connection is NOT pooled.

        Returns:
            psycopg2.extensions.connection: Synchronous database connection
                with RealDictCursor factory.

        Warning:
            Caller is responsible for closing the connection.
            Prefer async methods when possible for better performance.

        Example:
            >>> conn = db.get_sync_connection()
            >>> try:
            ...     cursor = conn.cursor()
            ...     cursor.execute("SELECT * FROM tracks LIMIT 1")
            ...     row = cursor.fetchone()
            ... finally:
            ...     conn.close()
        """
        return psycopg2.connect(
            host=self.config.host,
            port=self.config.port,
            database=self.config.database,
            user=self.config.username,
            password=self.config.password,
            cursor_factory=RealDictCursor,
        )


# TODO(code_review): [HIGH] Large class (>600 lines) - violates Single Responsibility Principle
# Split into smaller focused classes:
# - ThemeValidator, MoodValidator, ConsistencyChecker, HallucinationDetector
class SafetyValidator:
    """Validator for AI analysis reliability and hallucination detection.

    Comprehensive validation system that checks AI-generated analysis results
    for internal consistency, factual accuracy, hallucinations, and text-analysis
    alignment. Uses keyword-based validation with English and Russian support.

    Key Validation Checks:
        - Internal consistency: Logical coherence of predictions
        - Factual accuracy: Claims match actual lyrics content
        - Hallucination detection: Identifies fabricated themes/attributes
        - Text alignment: Analysis matches lyrics characteristics
        - Warning flags: Identifies suspicious patterns

    Attributes:
        theme_keywords: Dict mapping themes to English keyword lists.
        mood_indicators: Dict mapping moods to English keyword lists.
        consistency_threshold: Minimum score for consistency (default: 0.6).
        hallucination_threshold: Maximum acceptable hallucination risk (default: 0.4).

    Example:
        >>> validator = SafetyValidator()
        >>> result = validator.validate_analysis(lyrics, analysis_dict)
        >>> if result['is_reliable']:
        ...     print(f"‚úÖ {result['validation_summary']}")
        ... else:
        ...     print(f"‚ö†Ô∏è Warnings: {result['warning_flags']}")
    """
    # TODO(code_review): [MEDIUM] Extract keyword dictionaries to external config file (JSON/YAML)
    # Hardcoded dictionaries make internationalization and updates difficult
    # Consider using external keyword database or ML-based theme detection

    def __init__(self):
        """Initialize SafetyValidator with keyword dictionaries and thresholds.

        Sets up theme and mood keyword dictionaries for validation, primarily
        focused on English keywords with some Russian support.

        Note:
            Thresholds can be adjusted after initialization if needed:
            - consistency_threshold: Lower = more permissive (default 0.6)
            - hallucination_threshold: Higher = stricter (default 0.4)
        """
        # TODO(code_review): [LOW] Mixed language comments (Russian) - use English for consistency
        # Follow Google Style Guide: use English for all code/comments
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–µ–º–∞—Ç–∏–∫ (English-focused)
        # TODO(code_review): [MEDIUM] Large hardcoded data structure - extract to constants or config
        # This makes the __init__ method hard to read and test
        self.theme_keywords = {
            "money": [
                "cash",
                "money",
                "dollars",
                "bands",
                "racks",
                "bread",
                "paper",
                "coins",
                "wealth",
                "riches",
                "bank",
                "rich",
                "fortune",
            ],
            "relationships": [
                "love",
                "girl",
                "boy",
                "girlfriend",
                "boyfriend",
                "wife",
                "husband",
                "family",
                "bae",
                "baby",
                "relationship",
                "romance",
            ],
            "street_life": [
                "street",
                "block",
                "neighborhood",
                "ghetto",
                "projects",
                "corners",
                "trap",
                "streets",
                "hood",
                "city",
                "urban",
            ],
            "success": [
                "success",
                "famous",
                "star",
                "career",
                "achievement",
                "winning",
                "made it",
                "top",
                "win",
                "champion",
                "glory",
            ],
            "struggle": [
                "struggle",
                "pain",
                "problems",
                "hardship",
                "suffering",
                "tough",
                "hard times",
                "grind",
                "difficult",
                "rough",
            ],
            "drugs": [
                "drugs",
                "molly",
                "xanax",
                "percs",
                "pills",
                "cocaine",
                "heroin",
                "marijuana",
                "cannabis",
                "weed",
                "lean",
                "high",
            ],
            "violence": [
                "war",
                "fight",
                "murder",
                "blood",
                "gun",
                "knife",
                "shoot",
                "kill",
                "weapon",
                "violence",
                "battle",
                "beef",
            ],
            "party": [
                "party",
                "club",
                "dance",
                "fun",
                "alcohol",
                "beer",
                "drunk",
                "drinking",
                "turn up",
                "lit",
                "celebration",
            ],
            "depression": [
                "depression",
                "sad",
                "suicide",
                "death",
                "lonely",
                "sorrow",
                "depressed",
                "dark",
                "pain",
                "hurt",
                "broken",
            ],
            "social_issues": [
                "politics",
                "society",
                "system",
                "power",
                "protest",
                "revolution",
                "government",
                "social",
                "justice",
                "change",
            ],
        }

        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π (English-focused)
        self.mood_indicators = {
            "aggressive": [
                "hate",
                "angry",
                "mad",
                "kill",
                "war",
                "blood",
                "fight",
                "rage",
                "fury",
                "violence",
                "beef",
                "pissed",
            ],
            "melancholic": [
                "sad",
                "sadness",
                "tears",
                "depression",
                "lonely",
                "pain",
                "hurt",
                "broken",
                "crying",
                "sorrow",
            ],
            "energetic": [
                "party",
                "club",
                "dance",
                "hype",
                "lit",
                "turn up",
                "wild",
                "crazy",
                "bounce",
                "jump",
                "energy",
            ],
            "neutral": [
                "talking",
                "telling",
                "thinking",
                "know",
                "remember",
                "see",
                "saying",
                "speaking",
                "telling",
            ],
        }

        # TODO(code_review): [HIGH] Extract magic numbers to module-level constants
        # CONSISTENCY_THRESHOLD = 0.6
        # HALLUCINATION_THRESHOLD = 0.4
        # Consider making these configurable via constructor parameters
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
        self.consistency_threshold = 0.6  # –ü–æ–Ω–∏–∂–µ–Ω –¥–ª—è –±–æ–ª–µ–µ –≥–∏–±–∫–æ–π –æ—Ü–µ–Ω–∫–∏  # TODO(code_review): [MEDIUM] Magic number
        self.hallucination_threshold = 0.4  # –ü–æ–≤—ã—à–µ–Ω –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π  # TODO(code_review): [MEDIUM] Magic number

    # TODO(code_review): [HIGH] Return TypedDict instead of plain dict for type safety
    # Define ValidationResult(TypedDict) with all expected fields
    # Current return type is untyped dict which breaks IDE autocomplete and type checking
    def validate_analysis(self, lyrics: str, ai_analysis: dict) -> dict:  # TODO(code_review): [HIGH] Add proper return type hint
        """Perform comprehensive reliability validation of AI analysis results.

        Validates AI-generated analysis through multiple checks including internal
        consistency, factual accuracy, hallucination detection, and text alignment.
        Returns detailed validation metrics and overall reliability verdict.

        Args:
            lyrics: Original song lyrics text (any language).
            ai_analysis: Dictionary containing AI analysis results with expected keys:
                - metadata: dict with genre, mood, energy_level, explicit_content
                - lyrics_analysis: dict with structure, main_themes, complexity_level
                - quality_metrics: dict with authenticity_score, commercial_appeal, etc.
                (Keys may vary; missing keys are handled gracefully)

        Returns:
            Dictionary with validation results:
                - is_reliable (bool): Overall reliability verdict based on all checks
                - reliability_score (float): Aggregate reliability 0.0-1.0
                - consistency_score (float): Internal consistency 0.0-1.0
                - factual_accuracy (float): Factual claims accuracy 0.0-1.0
                - hallucination_risk (float): Risk of hallucinations 0.0-1.0
                - text_alignment (float): Text-analysis alignment 0.0-1.0
                - warning_flags (list[str]): List of warning flag identifiers
                - validation_summary (str): Human-readable summary message

        Example:
            >>> validator = SafetyValidator()
            >>> analysis = {
            ...     "genre": "trap", "mood": "aggressive",
            ...     "main_themes": ["money", "street_life"],
            ...     "authenticity_score": 0.85
            ... }
            >>> result = validator.validate_analysis(lyrics, analysis)
            >>> print(f"Reliable: {result['is_reliable']}")
            >>> print(f"Hallucination risk: {result['hallucination_risk']:.2f}")

        Note:
            Result is considered reliable if:
            - hallucination_risk < 0.4
            - consistency_score > 0.6
            - factual_accuracy > 0.5
            - text_alignment > 0.4
            - No critical warning flags
        """

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        consistency_score = self.check_internal_consistency(ai_analysis)

        # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
        factual_accuracy = self.validate_factual_claims(lyrics, ai_analysis)

        # 3. –î–µ—Ç–µ–∫—Ü–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
        hallucination_risk = self.detect_hallucinations(lyrics, ai_analysis)

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞
        text_alignment = self.check_text_analysis_alignment(lyrics, ai_analysis)

        # 5. –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞—é—â–∏—Ö —Ñ–ª–∞–≥–æ–≤
        warning_flags = self.get_warning_flags(ai_analysis, lyrics)

        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        is_reliable = (
            hallucination_risk < self.hallucination_threshold
            and consistency_score > self.consistency_threshold
            and factual_accuracy > 0.5  # –ü–æ–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥
            and text_alignment > 0.4  # –ü–æ–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥
            and len(warning_flags) == 0  # –ù–∏–∫–∞–∫–∏—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
        )

        return {
            "is_reliable": is_reliable,
            "reliability_score": (consistency_score + factual_accuracy + text_alignment)
            / 3,
            "consistency_score": consistency_score,
            "factual_accuracy": factual_accuracy,
            "hallucination_risk": hallucination_risk,
            "text_alignment": text_alignment,
            "warning_flags": warning_flags,
            "validation_summary": self._generate_validation_summary(
                is_reliable, hallucination_risk, consistency_score, warning_flags
            ),
        }

    # TODO(code_review): [MEDIUM] Method too long (70+ lines) - violates SRP
    # Split into smaller focused methods: check_theme_hallucinations(), check_mood_hallucinations(), etc.
    def detect_hallucinations(self, lyrics: str, analysis: dict) -> float:
        """Detect potential hallucinations in AI analysis results.

        Checks if AI-claimed themes, moods, genre, and explicit content are actually
        supported by evidence in the lyrics. Accumulates penalty scores for
        unsupported claims.

        Args:
            lyrics: Original song lyrics text.
            analysis: Dict with analysis results (genre, mood, main_themes, etc.).

        Returns:
            Hallucination risk score 0.0-1.0, where:
                - 0.0 = No hallucinations detected
                - 0.4+ = High risk (threshold for unreliable)
                - 1.0 = Maximum risk (capped)

        Note:
            Penalties are accumulated:
            - Theme not found: +0.15 per theme
            - Mood unsupported: +0.2
            - Inappropriate genre: +0.3
            - Explicit content mismatch: +0.1
            - Unrealistic quality scores: +0.1
        """
        # TODO(code_review): [MEDIUM] Extract penalty values to named constants
        # THEME_PENALTY = 0.15
        # MOOD_PENALTY = 0.2
        # GENRE_PENALTY = 0.3, etc.
        hallucination_score = 0.0
        lyrics_lower = lyrics.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞—è–≤–ª–µ–Ω–Ω—ã–µ —Ç–µ–º—ã
        if "main_themes" in analysis:
            claimed_themes = analysis["main_themes"]
            if isinstance(claimed_themes, list):
                for theme in claimed_themes:
                    if not self.theme_present_in_lyrics(theme, lyrics_lower):
                        hallucination_score += 0.15  # TODO(code_review): [MEDIUM] Magic number - extract to constant
                        logger.warning(
                            f"üö® Possible hallucination: theme '{theme}' not found in lyrics"
                        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ
        if "mood" in analysis:
            claimed_mood = analysis["mood"].lower()
            if not self.mood_supported_by_lyrics(claimed_mood, lyrics_lower):
                hallucination_score += 0.2
                logger.warning(
                    f"üö® Possible hallucination: mood '{claimed_mood}' not supported by lyrics"
                )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∂–∞–Ω—Ä (–º–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–æ, —Ç–∞–∫ –∫–∞–∫ –∂–∞–Ω—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å –º—É–∑—ã–∫–∞–ª—å–Ω—ã–º)
        if "genre" in analysis:
            claimed_genre = analysis["genre"].lower()
            if (
                claimed_genre in ["classical", "jazz", "country"]
                and "rap" not in lyrics_lower
            ):
                hallucination_score += 0.3  # –Ø–≤–Ω–æ –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π –∂–∞–Ω—Ä

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º explicit content
        if "explicit_content" in analysis:
            claimed_explicit = analysis["explicit_content"]
            actual_explicit = self.detect_explicit_content(lyrics_lower)
            if claimed_explicit != actual_explicit:
                hallucination_score += 0.1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å
        if "authenticity_score" in analysis:
            auth_score = analysis["authenticity_score"]
            if isinstance(auth_score, (int, float)):
                if (
                    auth_score > 0.9 and len(lyrics.split()) < 50
                ):  # –í—ã—Å–æ–∫–∞—è –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –∫–æ—Ä–æ—Ç–∫–æ–º —Ç–µ–∫—Å—Ç–µ
                    hallucination_score += 0.1

        return min(hallucination_score, 1.0)

    def theme_present_in_lyrics(self, theme: str, lyrics_lower: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–µ–º–∞ –≤ —Ç–µ–∫—Å—Ç–µ –ø–µ—Å–Ω–∏"""
        theme_lower = theme.lower().replace("_", " ")

        # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if theme_lower in lyrics_lower:
            return True

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if theme_lower in self.theme_keywords:
            keywords = self.theme_keywords[theme_lower]
            found_keywords = sum(1 for keyword in keywords if keyword in lyrics_lower)
            return found_keywords >= 1  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–¥–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞

        # –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö —Ç–µ–º (English-focused)
        if "street" in theme_lower and any(
            word in lyrics_lower
            for word in ["street", "block", "hood", "neighborhood", "ghetto"]
        ):
            return True
        if "money" in theme_lower and any(
            word in lyrics_lower
            for word in ["cash", "money", "dollars", "bands", "racks", "bread"]
        ):
            return True
        if "love" in theme_lower and any(
            word in lyrics_lower
            for word in ["love", "girl", "relationship", "girlfriend", "romance"]
        ):
            return True

        return False

    def mood_supported_by_lyrics(self, mood: str, lyrics_lower: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –∑–∞—è–≤–ª–µ–Ω–Ω–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç—É"""
        if mood in self.mood_indicators:
            indicators = self.mood_indicators[mood]
            found_indicators = sum(
                1 for indicator in indicators if indicator in lyrics_lower
            )
            return found_indicators >= 1

        # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –≤–æ–∑–≤—Ä–∞—â–∞–µ–º True (–Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å)
        return True

    # TODO(code_review): [HIGH] Hardcoded profanity list is incomplete and unmaintainable
    # Use external profanity filter library (e.g., better-profanity, profanity-check)
    # Current approach:
    # 1. Misses common profanity variations (f**k, sh!t, etc.)
    # 2. Doesn't support multiple languages properly
    # 3. No context awareness (Scunthorpe problem)
    # 4. Hardcoded list is difficult to update/customize
    def detect_explicit_content(self, lyrics_lower: str) -> bool:
        """–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç explicit –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ç–µ–∫—Å—Ç–µ (English-focused)"""
        # TODO(code_review): [MEDIUM] Extract to module-level constant or config file
        # TODO(code_review): [HIGH] Consider using set instead of list for O(1) lookup
        explicit_words = [
            "fuck",
            "shit",
            "bitch",
            "asshole",
            "damn",
            "hell",
            "pussy",
            "dick",
            "cock",
            "motherfucker",
            "nigga",
            "nigger",
            "whore",
            "slut",
            "cunt",
            "bastard",
            "piss",
        ]
        # TODO(code_review): [MEDIUM] Inefficient O(n*m) algorithm
        # Convert explicit_words to set for O(n) performance
        # Or use regex compilation for better performance
        return any(word in lyrics_lower for word in explicit_words)

    def check_internal_consistency(self, analysis: dict) -> float:
        """Check internal logical consistency of analysis results.

        Validates that different analysis dimensions are logically coherent
        (e.g., aggressive mood with low energy is suspicious).

        Args:
            analysis: Dict with analysis results (mood, energy_level, quality metrics).

        Returns:
            Consistency score 0.0-1.0, where:
                - 1.0 = Perfectly consistent
                - 0.6+ = Acceptable consistency (threshold)
                - 0.0 = Highly inconsistent

        Note:
            Penalties for logical contradictions:
            - Melancholic mood + high energy: -0.2
            - Aggressive mood + low energy: -0.3
            - Very high authenticity + very high commercial: -0.2
            - Advanced complexity + poor quality: -0.2
            - Beginner complexity + excellent quality: -0.1
        """
        consistency_score = 1.0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ —ç–Ω–µ—Ä–≥–∏–∏
        mood = analysis.get("mood", "").lower()
        energy = analysis.get("energy_level", "").lower()

        # –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
        if mood == "melancholic" and energy == "high":
            consistency_score -= 0.2  # –ì—Ä—É—Å—Ç–Ω–∞—è, –Ω–æ —ç–Ω–µ—Ä–≥–∏—á–Ω–∞—è - –≤–æ–∑–º–æ–∂–Ω–æ
        if mood == "aggressive" and energy == "low":
            consistency_score -= 0.3  # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è, –Ω–æ –Ω–∏–∑–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è - —Å—Ç—Ä–∞–Ω–Ω–æ

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if "authenticity_score" in analysis and "commercial_appeal" in analysis:
            auth = analysis["authenticity_score"]
            commercial = analysis["commercial_appeal"]
            if isinstance(auth, (int, float)) and isinstance(commercial, (int, float)):
                # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –ò –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –∞–ø–ø–µ–∞–ª - —Ä–µ–¥–∫–æ
                if auth > 0.9 and commercial > 0.9:
                    consistency_score -= 0.2

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
        complexity = analysis.get("complexity_level", "").lower()
        overall_quality = analysis.get("overall_quality", "").lower()

        if complexity == "advanced" and overall_quality == "poor":
            consistency_score -= 0.2
        if complexity == "beginner" and overall_quality == "excellent":
            consistency_score -= 0.1

        return max(consistency_score, 0.0)

    def validate_factual_claims(self, lyrics: str, analysis: dict) -> float:
        """Validate factual claims in analysis against actual lyrics.

        Checks if structural and complexity claims are reasonable given
        the actual lyrics length, structure, and characteristics.

        Args:
            lyrics: Original song lyrics text.
            analysis: Dict with structure, rhyme_scheme, complexity_level claims.

        Returns:
            Factual accuracy score 0.0-1.0, where:
                - 1.0 = All claims validated
                - 0.5+ = Acceptable accuracy (threshold)
                - 0.0 = Multiple invalid claims

        Note:
            Penalties for unrealistic claims:
            - Complex structure claimed but too few lines: -0.2
            - Hook structure but too many lines: -0.1
            - Complex rhyme scheme but simple repetition: -0.1
            - Advanced complexity but < 100 words: -0.2
            - Beginner complexity but > 500 words: -0.1
        """
        factual_score = 1.0
        lyrics_lower = lyrics.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        claimed_structure = analysis.get("structure", "").lower()
        if claimed_structure:
            # –ü–æ–¥—Å—á–µ—Ç —Å—Ç—Ä–æ–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            lines = [line for line in lyrics.split("\n") if line.strip()]

            if "verse-chorus-verse" in claimed_structure and len(lines) < 8:
                factual_score -= 0.2  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è —Ç–∞–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if "hook" in claimed_structure and len(lines) > 20:
                factual_score -= 0.1  # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –¥–ª—è hook

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–µ–º—É —Ä–∏—Ñ–º
        rhyme_scheme = analysis.get("rhyme_scheme", "").lower()
        if rhyme_scheme and rhyme_scheme != "unknown":
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∏—Ñ–º
            lines = [line.strip() for line in lyrics.split("\n") if line.strip()]
            if len(lines) >= 4:
                # –ï—Å–ª–∏ –∑–∞—è–≤–ª–µ–Ω–∞ —Å–ª–æ–∂–Ω–∞—è —Å—Ö–µ–º–∞, –Ω–æ —Ç–µ–∫—Å—Ç –ø—Ä–æ—Å—Ç–æ–π
                if (
                    "complex" in rhyme_scheme
                    and len(set(line.split()[-1] for line in lines[:4] if line.split()))
                    == 1
                ):
                    factual_score -= 0.1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ vs —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        word_count = len(lyrics.split())
        complexity = analysis.get("complexity_level", "").lower()

        if complexity == "advanced" and word_count < 100:
            factual_score -= 0.2
        if complexity == "beginner" and word_count > 500:
            factual_score -= 0.1

        return max(factual_score, 0.0)

    def check_text_analysis_alignment(self, lyrics: str, analysis: dict) -> float:
        """Check alignment between lyrics characteristics and analysis.

        Validates that analysis matches observable text characteristics like
        length, explicit content, energy indicators (punctuation, caps).

        Args:
            lyrics: Original song lyrics text.
            analysis: Dict with analysis results.

        Returns:
            Alignment score 0.0-1.0, where:
                - 1.0 = Perfect alignment
                - 0.4+ = Acceptable alignment (threshold)
                - 0.0 = Poor alignment

        Note:
            Penalties for misalignment:
            - Short text but detailed analysis: -0.2
            - Explicit content mismatch: -0.3
            - High energy but no indicators: -0.2
            - Low energy but many indicators: -0.2
        """
        alignment_score = 1.0
        lyrics_lower = lyrics.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ—Ç–∞–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞
        word_count = len(lyrics.split())

        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π, –Ω–æ –∞–Ω–∞–ª–∏–∑ –æ—á–µ–Ω—å –¥–µ—Ç–∞–ª—å–Ω—ã–π - –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ
        if word_count < 50:
            detailed_fields = sum(
                1
                for key in ["main_themes", "structure", "rhyme_scheme"]
                if analysis.get(key)
            )
            if detailed_fields > 2:
                alignment_score -= 0.2

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º explicit content alignment
        actual_explicit = self.detect_explicit_content(lyrics_lower)
        claimed_explicit = analysis.get("explicit_content", False)

        if actual_explicit != claimed_explicit:
            alignment_score -= 0.3

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º energy level alignment
        energy = analysis.get("energy_level", "").lower()
        exclamation_count = lyrics.count("!")
        caps_ratio = sum(1 for c in lyrics if c.isupper()) / max(len(lyrics), 1)

        if energy == "high" and exclamation_count == 0 and caps_ratio < 0.05:
            alignment_score -= 0.2
        if energy == "low" and exclamation_count > 5:
            alignment_score -= 0.2

        return max(alignment_score, 0.0)

    def get_warning_flags(self, analysis: dict, lyrics: str) -> list:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞—é—â–∏—Ö —Ñ–ª–∞–≥–æ–≤"""
        flags = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏–µ –æ—Ü–µ–Ω–∫–∏
        if analysis.get("authenticity_score", 0) > 0.95:
            flags.append("SUSPICIOUSLY_HIGH_AUTHENTICITY")

        if analysis.get("uniqueness", 0) > 0.95:
            flags.append("SUSPICIOUSLY_HIGH_UNIQUENESS")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª–∏–Ω—ã –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        word_count = len(lyrics.split())
        complexity = analysis.get("complexity_level", "").lower()

        if word_count < 50 and complexity == "advanced":
            flags.append("SHORT_TEXT_HIGH_COMPLEXITY")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ç–µ–º –≤ –∫–æ—Ä–æ—Ç–∫–æ–º —Ç–µ–∫—Å—Ç–µ
        themes = analysis.get("main_themes", [])
        if word_count < 100 and len(themes) > 4:
            flags.append("SHORT_TEXT_MANY_THEMES")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        mood = analysis.get("mood", "").lower()
        commercial = analysis.get("commercial_appeal", 0)

        if mood == "melancholic" and commercial > 0.8:
            flags.append("SAD_MOOD_HIGH_COMMERCIAL")

        return flags

    def _generate_validation_summary(
        self,
        is_reliable: bool,
        hallucination_risk: float,
        consistency_score: float,
        warning_flags: list,
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        if is_reliable:
            return f"‚úÖ –ê–Ω–∞–ª–∏–∑ –Ω–∞–¥–µ–∂–µ–Ω (—Ä–∏—Å–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: {hallucination_risk:.2f})"
        issues = []
        if hallucination_risk > 0.4:  # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥
            issues.append(f"–≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π ({hallucination_risk:.2f})")
        if consistency_score < 0.6:  # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥
            issues.append(f"–Ω–∏–∑–∫–∞—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å ({consistency_score:.2f})")
        if warning_flags:
            issues.append(f"–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {len(warning_flags)}")

        return f"‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ –Ω–µ–Ω–∞–¥–µ–∂–µ–Ω: {', '.join(issues)}"


# TODO(code_review): [HIGH] Large class (375 lines) - violates SRP
# Split into smaller classes: ExplanationGenerator, ConfidenceCalculator, FactorExtractor
# TODO(code_review): [MEDIUM] Duplicate code with SafetyValidator keyword dictionaries
# Extract shared keyword dictionaries to separate KeywordRegistry class
class InterpretableAnalyzer:
    """Analyzer with AI decision explanations and interpretability features.

    Wraps base analyzer to provide interpretability by explaining classification
    decisions, calculating confidence scores, identifying key decision factors,
    and extracting influential phrases from lyrics.

    Uses keyword-based feature extraction with Russian and English support
    to explain genre, mood, and authenticity classifications.

    Attributes:
        base_analyzer: Base analyzer instance (e.g., MultiModelAnalyzer).
        genre_keywords: Dict mapping genres to keyword lists.
        mood_keywords: Dict mapping moods to keyword lists.
        authenticity_keywords: Dict mapping authenticity types to keywords.

    Example:
        >>> base = MultiModelAnalyzer()
        >>> interpreter = InterpretableAnalyzer(base)
        >>> result = interpreter.analyze_with_explanation("Artist", "Title", lyrics)
        >>> print(f"Confidence: {result.confidence:.2f}")
        >>> for category, explanations in result.explanation.items():
        ...     print(f"{category}: {explanations}")
    """

    def __init__(self, base_analyzer):
        """Initialize InterpretableAnalyzer with base analyzer.

        Args:
            base_analyzer: Base analyzer instance that provides analyze_song() method.
                Typically MultiModelAnalyzer.

        Note:
            Initializes genre, mood, and authenticity keyword dictionaries
            for decision explanation generation.
        """
        self.base_analyzer = base_analyzer

        # –°–ª–æ–≤–∞—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        self.genre_keywords = {
            "trap": ["trap", "–º–æ–ª–ª–∏", "lean", "xanax", "—Å–∫—Ä—Ä", "–π–∞", "bando", "plug"],
            "drill": ["drill", "smoke", "opps", "block", "gang", "sliding", "packed"],
            "old_school": [
                "boom bap",
                "real hip hop",
                "90s",
                "golden era",
                "conscious",
            ],
            "gangsta": ["glock", "ak", "blood", "crip", "hood", "street", "thug"],
            "emo_rap": [
                "–¥–µ–ø—Ä–µ—Å—Å–∏—è",
                "—Å—É–∏—Ü–∏–¥",
                "–±–æ–ª—å",
                "–≥—Ä—É—Å—Ç—å",
                "–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ",
                "—Å–ª–µ–∑—ã",
            ],
        }

        self.mood_keywords = {
            "aggressive": ["—É–±—å—é", "–≤–æ–π–Ω–∞", "–∫—Ä–æ–≤—å", "–¥—Ä–∞–∫–∞", "hate", "angry", "mad"],
            "melancholic": ["–≥—Ä—É—Å—Ç—å", "–ø–µ—á–∞–ª—å", "—Å–ª–µ–∑—ã", "–¥–µ–ø—Ä–µ—Å—Å–∏—è", "–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ"],
            "energetic": ["party", "club", "dance", "energy", "–≤–ø–µ—Ä–µ–¥", "–¥–≤–∏–∂–µ–Ω–∏–µ"],
            "chill": ["—Ä–∞—Å—Å–ª–∞–±–æ–Ω", "—Å–ø–æ–∫–æ–π–Ω–æ", "–º–µ–¥–ª–µ–Ω–Ω–æ", "vibe", "–∞—Ç–º–æ—Å—Ñ–µ—Ä–∞"],
        }

        self.authenticity_keywords = {
            "real": ["–ø—Ä–∞–≤–¥–∞", "—Ä–µ–∞–ª—å–Ω–æ", "—á–µ—Å—Ç–Ω–æ", "–±–µ–∑ —Ñ–∞–ª—å—à–∏", "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É"],
            "fake": ["–ø–æ–Ω—Ç", "—Ñ–µ–π–∫", "–ø–∏–∂–æ–Ω", "–ø–æ–∫–∞–∑—É—Ö–∞", "–ø—Ä–∏—Ç–≤–æ—Ä—Å—Ç–≤–æ"],
            "street": ["—É–ª–∏—Ü–∞", "—Ä–∞–π–æ–Ω", "–¥–≤–æ—Ä", "–ø–æ–¥—ä–µ–∑–¥", "–∫–≤–∞—Ä—Ç–∞–ª", "–≥–µ—Ç—Ç–æ"],
            "commercial": ["money", "brand", "–∫–æ–º–º–µ—Ä—Ü–∏—è", "–ø—Ä–æ–¥–∞–∂–∏", "mainstream"],
        }

    def analyze_with_explanation(
        self, artist: str, title: str, lyrics: str
    ) -> ExplainableAnalysisResult | None:
        """Analyze song with AI decision explanations and confidence scores.

        Performs base analysis and augments it with interpretability features:
        explanations of classification decisions, confidence score, key decision
        factors, and influential lyrics phrases.

        Args:
            artist: Artist/performer name.
            title: Song title.
            lyrics: Complete song lyrics text.

        Returns:
            ExplainableAnalysisResult containing:
                - analysis: Base EnhancedSongData with full analysis
                - explanation: Dict of category to list of explanation strings
                - confidence: Overall confidence score 0.0-1.0
                - decision_factors: Dict of factor names to importance scores
                - influential_phrases: Dict of category to influential lyrics
            Returns None if base analysis fails.

        Example:
            >>> result = analyzer.analyze_with_explanation("Kendrick", "DNA.", lyrics)
            >>> if result:
            ...     print(f"Genre: {result.analysis.metadata.genre}")
            ...     print(f"Confidence: {result.confidence:.2f}")
            ...     for expl in result.explanation['genre_indicators']:
            ...         print(f"  - {expl}")

        Note:
            Confidence is based on text length, genre evidence strength,
            quality metric consistency, and detail presence.
        """
        try:
            # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
            base_result = self.base_analyzer.analyze_song(artist, title, lyrics)
            if not base_result:
                return None

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
            explanation = self.explain_decision(lyrics, base_result)
            confidence = self.calculate_confidence(base_result, lyrics)
            decision_factors = self.extract_key_factors(lyrics, base_result)
            influential_phrases = self.find_influential_phrases(lyrics, base_result)

            return ExplainableAnalysisResult(
                analysis=base_result,
                explanation=explanation,
                confidence=confidence,
                decision_factors=decision_factors,
                influential_phrases=influential_phrases,
            )

        except Exception as e:
            logging.error(f"‚åõ –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}")
            return None

    def explain_decision(
        self, lyrics: str, result: EnhancedSongData
    ) -> dict[str, list[str]]:
        """–û–±—ä—è—Å–Ω—è–µ—Ç, –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–µ–≥–æ –º–æ–¥–µ–ª—å –ø—Ä–∏–Ω—è–ª–∞ —Ä–µ—à–µ–Ω–∏–µ"""
        lyrics_lower = lyrics.lower()
        explanations = {
            "genre_indicators": [],
            "mood_triggers": [],
            "authenticity_markers": [],
            "quality_indicators": [],
        }

        # –ê–Ω–∞–ª–∏–∑ –∂–∞–Ω—Ä–æ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        detected_genre = result.metadata.genre.lower()
        for genre, keywords in self.genre_keywords.items():
            if genre in detected_genre:
                found_keywords = [kw for kw in keywords if kw in lyrics_lower]
                if found_keywords:
                    explanations["genre_indicators"].extend(
                        [
                            f"–ñ–∞–Ω—Ä '{genre}' –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ —Å–ª–æ–≤–∞–º: {', '.join(found_keywords[:3])}"
                        ]
                    )

        # –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        detected_mood = result.metadata.mood.lower()
        for mood, keywords in self.mood_keywords.items():
            if mood in detected_mood:
                found_keywords = [kw for kw in keywords if kw in lyrics_lower]
                if found_keywords:
                    explanations["mood_triggers"].extend(
                        [
                            f"–ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ '{mood}' –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –ø–æ —Å–ª–æ–≤–∞–º: {', '.join(found_keywords[:3])}"
                        ]
                    )

        # –ê–Ω–∞–ª–∏–∑ –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
        auth_score = result.quality_metrics.authenticity_score
        if auth_score > 0.7:
            real_words = [
                kw for kw in self.authenticity_keywords["real"] if kw in lyrics_lower
            ]
            street_words = [
                kw for kw in self.authenticity_keywords["street"] if kw in lyrics_lower
            ]
            if real_words or street_words:
                explanations["authenticity_markers"].append(
                    f"–í—ã—Å–æ–∫–∞—è –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å ({auth_score:.2f}) –±–ª–∞–≥–æ–¥–∞—Ä—è: {', '.join((real_words + street_words)[:3])}"
                )
        elif auth_score < 0.4:
            fake_words = [
                kw for kw in self.authenticity_keywords["fake"] if kw in lyrics_lower
            ]
            commercial_words = [
                kw
                for kw in self.authenticity_keywords["commercial"]
                if kw in lyrics_lower
            ]
            if fake_words or commercial_words:
                explanations["authenticity_markers"].append(
                    f"–ù–∏–∑–∫–∞—è –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å ({auth_score:.2f}) –∏–∑-–∑–∞: {', '.join((fake_words + commercial_words)[:3])}"
                )

        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
        creativity = result.quality_metrics.lyrical_creativity
        wordplay = result.lyrics_analysis.wordplay_quality
        explanations["quality_indicators"].append(
            f"–ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å: {creativity:.2f}, Wordplay: {wordplay}"
        )

        return explanations

    def calculate_confidence(self, result: EnhancedSongData, lyrics: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∞–Ω–∞–ª–∏–∑–µ"""
        confidence_factors = []

        # –§–∞–∫—Ç–æ—Ä 1: –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (–±–æ–ª—å—à–µ —Ç–µ–∫—Å—Ç–∞ = –±–æ–ª—å—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏)
        text_length_factor = min(len(lyrics) / 1000, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 1.0
        confidence_factors.append(text_length_factor * 0.2)

        # –§–∞–∫—Ç–æ—Ä 2: –ù–∞–ª–∏—á–∏–µ —è–≤–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∂–∞–Ω—Ä–∞
        genre_confidence = self._calculate_genre_confidence(
            lyrics, result.metadata.genre
        )
        confidence_factors.append(genre_confidence * 0.3)

        # –§–∞–∫—Ç–æ—Ä 3: –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_consistency = self._calculate_quality_consistency(
            result.quality_metrics
        )
        confidence_factors.append(quality_consistency * 0.3)

        # –§–∞–∫—Ç–æ—Ä 4: –ù–∞–ª–∏—á–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π (–∏–º–µ–Ω–∞, –º–µ—Å—Ç–∞, —Å–æ–±—ã—Ç–∏—è)
        detail_factor = self._calculate_detail_factor(lyrics)
        confidence_factors.append(detail_factor * 0.2)

        return min(sum(confidence_factors), 1.0)

    def _calculate_genre_confidence(self, lyrics: str, genre: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∂–∞–Ω—Ä–∞"""
        lyrics_lower = lyrics.lower()
        genre_lower = genre.lower()

        matching_keywords = 0
        total_keywords = 0

        for g, keywords in self.genre_keywords.items():
            if g in genre_lower:
                total_keywords = len(keywords)
                matching_keywords = sum(1 for kw in keywords if kw in lyrics_lower)
                break

        if total_keywords == 0:
            return 0.5  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∂–∞–Ω—Ä–æ–≤

        return matching_keywords / total_keywords

    def _calculate_quality_consistency(self, metrics: QualityMetrics) -> float:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
        scores = [
            metrics.authenticity_score,
            metrics.lyrical_creativity,
            metrics.commercial_appeal,
            metrics.uniqueness,
        ]

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
        mean_score = sum(scores) / len(scores)
        variance = sum((x - mean_score) ** 2 for x in scores) / len(scores)
        std_dev = variance**0.5

        # –ù–∏–∑–∫–æ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ = –≤—ã—Å–æ–∫–∞—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
        consistency = max(0, 1 - (std_dev * 2))  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        return consistency

    def _calculate_detail_factor(self, lyrics: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π"""
        detail_indicators = [
            r"\b[A-Z][a-z]+\b",  # –ò–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ
            r"\b\d{4}\b",  # –ì–æ–¥—ã
            r"\b\d+[–∫–º]\b",  # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è
            r"\$\d+",  # –î–µ–Ω—å–≥–∏
            r"\b[–ê-–Ø–Å][–∞-—è—ë]+\b",  # –†—É—Å—Å–∫–∏–µ –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ
        ]

        total_details = 0
        for pattern in detail_indicators:
            matches = re.findall(pattern, lyrics)
            total_details += len(matches)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
        detail_density = total_details / max(len(lyrics.split()), 1)
        return min(detail_density * 10, 1.0)  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º

    def extract_key_factors(
        self, lyrics: str, result: EnhancedSongData
    ) -> dict[str, float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ –∞–Ω–∞–ª–∏–∑"""
        factors = {}
        lyrics_lower = lyrics.lower()

        # –ß–∞—Å—Ç–æ—Ç–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, keywords in {**self.genre_keywords, **self.mood_keywords}.items():
            keyword_count = sum(1 for kw in keywords if kw in lyrics_lower)
            factors[f"{category}_keywords"] = keyword_count / len(keywords)

        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        factors["text_length"] = min(len(lyrics) / 2000, 1.0)
        factors["line_count"] = min(len(lyrics.split("\n")) / 50, 1.0)
        factors["word_diversity"] = len(set(lyrics.lower().split())) / max(
            len(lyrics.split()), 1
        )

        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞–∫ —Ñ–∞–∫—Ç–æ—Ä—ã
        factors["authenticity"] = result.quality_metrics.authenticity_score
        factors["creativity"] = result.quality_metrics.lyrical_creativity
        factors["commercial_appeal"] = result.quality_metrics.commercial_appeal
        factors["uniqueness"] = result.quality_metrics.uniqueness

        return factors

    def find_influential_phrases(
        self, lyrics: str, result: EnhancedSongData
    ) -> dict[str, list[str]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–≤–ª–∏—è–ª–∏ –Ω–∞ –æ—Ü–µ–Ω–∫—É"""
        influential = {
            "genre_phrases": [],
            "mood_phrases": [],
            "authenticity_phrases": [],
            "quality_phrases": [],
        }

        lines = lyrics.split("\n")

        # –ü–æ–∏—Å–∫ –≤–ª–∏—è—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑ –¥–ª—è –∂–∞–Ω—Ä–∞
        genre_lower = result.metadata.genre.lower()
        for genre, keywords in self.genre_keywords.items():
            if genre in genre_lower:
                for line in lines:
                    if any(kw in line.lower() for kw in keywords):
                        influential["genre_phrases"].append(line.strip())
                        if len(influential["genre_phrases"]) >= 3:
                            break

        # –ü–æ–∏—Å–∫ —Ñ—Ä–∞–∑ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        mood_lower = result.metadata.mood.lower()
        for mood, keywords in self.mood_keywords.items():
            if mood in mood_lower:
                for line in lines:
                    if any(kw in line.lower() for kw in keywords):
                        influential["mood_phrases"].append(line.strip())
                        if len(influential["mood_phrases"]) >= 3:
                            break

        # –ü–æ–∏—Å–∫ —Ñ—Ä–∞–∑ –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
        auth_score = result.quality_metrics.authenticity_score
        auth_keywords = (
            self.authenticity_keywords["real"] + self.authenticity_keywords["street"]
        )
        if auth_score > 0.7:
            for line in lines:
                if any(kw in line.lower() for kw in auth_keywords):
                    influential["authenticity_phrases"].append(line.strip())
                    if len(influential["authenticity_phrases"]) >= 2:
                        break

        # –ü–æ–∏—Å–∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö wordplay —Ñ—Ä–∞–∑
        if result.lyrics_analysis.wordplay_quality == "excellent":
            # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —Ä–∏—Ñ–º–∞–º–∏ –∏–ª–∏ –∞–ª–ª–∏—Ç–µ—Ä–∞—Ü–∏–µ–π
            for line in lines:
                words = line.lower().split()
                if len(words) >= 4:
                    # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∏—Ñ–º—É (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è)
                    endings = [word[-2:] for word in words if len(word) > 3]
                    if (
                        len(set(endings)) < len(endings) * 0.8
                    ):  # –ú–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –æ–∫–æ–Ω—á–∞–Ω–∏–π
                        influential["quality_phrases"].append(line.strip())
                        if len(influential["quality_phrases"]) >= 2:
                            break

        return influential


class ModelProvider:
    """Base class for AI provider implementations.

    Abstract base class defining interface for AI model providers.
    Concrete implementations must provide availability checking and
    song analysis functionality.

    Attributes:
        name: Provider name (e.g., "Ollama", "Gemma", "Mock").
        available: Whether provider is available/initialized (bool).
        cost_per_1k_tokens: Cost per 1000 tokens in USD (float).

    Note:
        Subclasses must implement check_availability() and analyze_song().
    """

    def __init__(self, name: str):
        """Initialize provider with name.

        Args:
            name: Provider identifier string.
        """
        self.name = name
        self.available = False
        self.cost_per_1k_tokens = 0.0

    def check_availability(self) -> bool:
        """Check if provider is available and operational.

        Returns:
            True if provider can be used, False otherwise.

        Raises:
            NotImplementedError: Must be implemented by subclass.
        """
        raise NotImplementedError

    def analyze_song(
        self, artist: str, title: str, lyrics: str
    ) -> EnhancedSongData | None:
        """Analyze song lyrics and return structured results.

        Args:
            artist: Artist/performer name.
            title: Song title.
            lyrics: Complete song lyrics text.

        Returns:
            EnhancedSongData with analysis results, or None on failure.

        Raises:
            NotImplementedError: Must be implemented by subclass.
        """
        raise NotImplementedError


class OllamaProvider(ModelProvider):
    """Provider for local Ollama models.

    Connects to locally-running Ollama server for free, offline AI inference.
    Automatically checks availability and attempts to pull missing models.

    Attributes:
        model_name: Ollama model identifier (e.g., "llama3.2:3b").
        base_url: Ollama API base URL (default: http://localhost:11434).
        cost_per_1k_tokens: Always 0.0 (free local inference).

    Example:
        >>> provider = OllamaProvider(model_name="llama3.2:3b")
        >>> if provider.available:
        ...     result = provider.analyze_song("Artist", "Title", lyrics)

    Note:
        Requires Ollama server running: `ollama serve`
        Timeout is 60 seconds for analysis requests.
    """

    def __init__(
        self, model_name: str = "llama3.2:3b", base_url: str = "http://localhost:11434"
    ):
        """Initialize Ollama provider with model and URL.

        Args:
            model_name: Ollama model to use (default: "llama3.2:3b").
            base_url: Ollama API endpoint (default: "http://localhost:11434").

        Note:
            Automatically calls check_availability() during initialization.
            If model not found, attempts to pull it automatically.
        """
        super().__init__("Ollama")
        self.model_name = model_name
        self.base_url = base_url
        self.cost_per_1k_tokens = 0.0  # –ë–µ—Å–ø–ª–∞—Ç–Ω–æ!
        self.available = self.check_availability()

    def check_availability(self) -> bool:
        """Check if Ollama server is running and model is available.

        Makes HTTP request to Ollama API /api/tags to verify:
        1. Server is running and responsive
        2. Configured model exists locally
        3. If model missing, attempts automatic pull

        Returns:
            True if Ollama accessible and model available/downloaded,
            False if server unreachable or model pull fails.

        Side Effects:
            - Logs availability status and model list
            - May trigger model download via _pull_model()

        Note:
            Uses 5 second timeout for API request.
            Disables proxies for local connection.
        """
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=5,
                proxies={"http": "", "https": ""},
            )
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                logger.info(f"ü¶ô Ollama –¥–æ—Å—Ç—É–ø–µ–Ω. –ú–æ–¥–µ–ª–∏: {available_models}")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω–æ–π –º–æ–¥–µ–ª–∏
                if any(self.model_name in model for model in available_models):
                    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.model_name} –Ω–∞–π–¥–µ–Ω–∞")
                    return True
                logger.warning(
                    f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏..."
                )
                return self._pull_model()
            return False
        except requests.exceptions.RequestException as e:
            logger.warning(f"‚åõ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            return False

    def _pull_model(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
        try:
            logger.info(f"üî• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {self.model_name}...")
            response = requests.post(
                f"{self.base_url}/api/pull",
                json={"name": self.model_name},
                timeout=300,  # 5 –º–∏–Ω—É—Ç –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É
                proxies={"http": "", "https": ""},
            )
            if response.status_code == 200:
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return True
            logger.error(f"‚åõ –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {response.text}")
            return False
        except Exception as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    def analyze_song(
        self, artist: str, title: str, lyrics: str
    ) -> EnhancedSongData | None:
        """Analyze song using local Ollama model.

        Sends lyrics to Ollama with structured prompt requesting JSON analysis.
        Parses response and constructs EnhancedSongData.

        Args:
            artist: Artist/performer name.
            title: Song title.
            lyrics: Complete song lyrics (truncated to 2000 chars in prompt).

        Returns:
            EnhancedSongData with analysis results, or None if:
                - Provider not available
                - API request fails
                - JSON parsing fails

        Note:
            Uses temperature=0.1 for consistent results.
            60 second timeout for analysis.
        """
        if not self.available:
            return None

        try:
            prompt = self._create_analysis_prompt(artist, title, lyrics)

            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                        "top_p": 0.9,
                        "max_tokens": 1500,
                    },
                },
                timeout=60,
                proxies={"http": "", "https": ""},
            )

            if response.status_code == 200:
                result = response.json()
                analysis_text = result.get("response", "")
                return self._parse_analysis(analysis_text, artist, title)
            logger.error(f"‚åõ Ollama –æ—à–∏–±–∫–∞: {response.status_code} - {response.text}")
            return None

        except Exception as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ Ollama: {e}")
            return None

    # TODO(code_review): [HIGH] Method returns large multiline string - extract to template file
    # Use jinja2 or similar template engine for better maintainability
    # Current approach makes prompt versioning and A/B testing difficult
    def _create_analysis_prompt(self, artist: str, title: str, lyrics: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        # TODO(code_review): [MEDIUM] Magic number 2000 - extract to constant
        # LYRICS_MAX_LENGTH = 2000
        # TODO(code_review): [HIGH] Truncating lyrics at 2000 chars may cut mid-word/sentence
        # Use proper text truncation that respects word boundaries
        return f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä—ç–ø-–ø–µ—Å–Ω—é –∏ –≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ.

–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å: {artist}
–ù–∞–∑–≤–∞–Ω–∏–µ: {title}
–¢–µ–∫—Å—Ç: {lyrics[:2000]}...

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:

{{
    "metadata": {{
        "genre": "rap",
        "mood": "aggressive",
        "energy_level": "high",
        "explicit_content": true
    }},
    "lyrics_analysis": {{
        "structure": "verse-chorus-verse",
        "rhyme_scheme": "ABAB",
        "complexity_level": "advanced",
        "main_themes": ["street_life", "success", "relationships"],
        "emotional_tone": "mixed",
        "storytelling_type": "narrative",
        "wordplay_quality": "excellent"
    }},
    "quality_metrics": {{
        "authenticity_score": 0.8,
        "lyrical_creativity": 0.9,
        "commercial_appeal": 0.7,
        "uniqueness": 0.6,
        "overall_quality": "excellent",
        "ai_likelihood": 0.1
    }}
}}

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ü–û–õ–Ø:
- emotional_tone: positive/negative/neutral/mixed
- storytelling_type: narrative/abstract/conversational
- wordplay_quality: basic/good/excellent

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤!
"""

    # TODO(code_review): [CRITICAL] Code duplication - identical method in GemmaProvider
    # Extract to shared utility function or base class method
    # DRY principle violation - same logic duplicated 80+ lines
    def _parse_analysis(
        self, analysis_text: str, artist: str, title: str
    ) -> EnhancedSongData | None:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # TODO(code_review): [MEDIUM] Naive JSON extraction - fragile parsing logic
            # Use regex or proper parsing library to handle edge cases
            # Current approach fails if JSON contains nested braces
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            json_start = analysis_text.find("{")
            json_end = analysis_text.rfind("}") + 1

            if json_start == -1 or json_end <= json_start:
                logger.error("‚åõ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ")
                return None

            json_str = analysis_text[json_start:json_end]
            data = json.loads(json_str)  # TODO(code_review): [MEDIUM] Add JSONDecodeError handling separately

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –¥–æ–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è
            metadata_data = data.get("metadata", {})
            lyrics_data = data.get("lyrics_analysis", {})
            quality_data = data.get("quality_metrics", {})

            # –î–æ–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è –≤ lyrics_analysis
            if "emotional_tone" not in lyrics_data:
                lyrics_data["emotional_tone"] = "neutral"
                logger.warning("‚ö†Ô∏è –î–æ–±–∞–≤–ª–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è emotional_tone")

            if "storytelling_type" not in lyrics_data:
                lyrics_data["storytelling_type"] = "conversational"
                logger.warning(
                    "‚ö†Ô∏è –î–æ–±–∞–≤–ª–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è storytelling_type"
                )

            if "wordplay_quality" not in lyrics_data:
                lyrics_data["wordplay_quality"] = "basic"
                logger.warning("‚ö†Ô∏è –î–æ–±–∞–≤–ª–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è wordplay_quality")

            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            metadata = SongMetadata(**metadata_data)
            lyrics_analysis = LyricsAnalysis(**lyrics_data)
            quality_metrics = QualityMetrics(**quality_data)

            return EnhancedSongData(
                artist=artist,
                title=title,
                metadata=metadata,
                lyrics_analysis=lyrics_analysis,
                quality_metrics=quality_metrics,
                model_used="gemma-2-27b-it",
                analysis_date=datetime.now().isoformat(),
            )

        except json.JSONDecodeError as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON Gemma: {e}")
            logger.debug(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {analysis_text[:500]}")
            return None
        except Exception as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ Gemma: {e}")
            return None


# TODO(code_review): [HIGH] MockProvider.analyze_song() is 187 lines - violates SRP
# Extract rule-based analysis logic to separate analyzers:
# - GenreClassifier, MoodDetector, QualityEstimator
# Use strategy pattern or composition instead of single monolithic method
class MockProvider(ModelProvider):
    """Mock provider for testing and demonstration.

    Provides rule-based analysis without external AI models. Always available
    and free, serves as fallback when other providers fail. Uses keyword matching
    and heuristics for genre, mood, and quality estimation.

    Attributes:
        cost_per_1k_tokens: Always 0.0 (no cost for mock analysis).
        available: Always True (no dependencies).

    Example:
        >>> provider = MockProvider()
        >>> result = provider.analyze_song("Test", "Song", lyrics)
        >>> print(f"Genre: {result.metadata.genre}")

    Note:
        Provides reasonable estimates but not true AI analysis.
        Useful for testing, demos, and fallback scenarios.
    """

    def __init__(self):
        """Initialize MockProvider (always available).

        No external dependencies required. Sets available=True immediately.
        """
        super().__init__("Mock")
        self.cost_per_1k_tokens = 0.0  # –ë–µ—Å–ø–ª–∞—Ç–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
        self.available = True  # –í—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω

    def check_availability(self) -> bool:
        """Check availability (always returns True).

        Returns:
            True (MockProvider has no dependencies and is always available).
        """
        logger.info("‚úÖ Mock –ø—Ä–æ–≤–∞–π–¥–µ—Ä –≥–æ—Ç–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏")
        return True

    def analyze_song(
        self, artist: str, title: str, lyrics: str
    ) -> EnhancedSongData | None:
        """Analyze song using rule-based heuristics.

        Performs keyword-based analysis for genre, mood, themes, and quality
        without external AI. Uses pattern matching and statistical features.

        Args:
            artist: Artist/performer name.
            title: Song title.
            lyrics: Complete song lyrics text.

        Returns:
            EnhancedSongData with heuristic analysis, or None on error.

        Note:
            Analysis logic:
            - Genre: Keyword matching (trap, drill, emo_rap, etc.)
            - Mood: Sentiment keywords (aggressive, sad, energetic)
            - Energy: Punctuation and caps ratio
            - Explicit: Profanity detection
            - Quality: Word diversity and length heuristics
        """
        try:
            lyrics_lower = lyrics.lower()

            # –£–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∂–∞–Ω—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            genre = "rap"
            if any(word in lyrics_lower for word in ["trap", "–º–æ–ª–ª–∏", "lean", "—Å–∫—Ä—Ä"]):
                genre = "trap"
            elif any(
                word in lyrics_lower for word in ["drill", "smoke", "opps", "gang"]
            ):
                genre = "drill"
            elif any(
                word in lyrics_lower for word in ["—É–ª–∏—Ü–∞", "—Ä–∞–π–æ–Ω", "–¥–≤–æ—Ä", "–ø–æ–¥—ä–µ–∑–¥"]
            ):
                genre = "gangsta_rap"
            elif any(word in lyrics_lower for word in ["–¥–µ–ø—Ä–µ—Å—Å–∏—è", "–≥—Ä—É—Å—Ç—å", "—Å–ª–µ–∑—ã"]):
                genre = "emo_rap"

            # –£–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
            mood = "neutral"
            aggressive_words = ["—É–±—å—é", "–≤–æ–π–Ω–∞", "–¥—Ä–∞–∫–∞", "hate", "angry"]
            sad_words = ["–≥—Ä—É—Å—Ç—å", "–ø–µ—á–∞–ª—å", "—Å–ª–µ–∑—ã", "–¥–µ–ø—Ä–µ—Å—Å–∏—è", "–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ"]
            positive_words = ["party", "—Å—á–∞—Å—Ç—å–µ", "—Ä–∞–¥–æ—Å—Ç—å", "love", "—É—Å–ø–µ—Ö"]

            if any(word in lyrics_lower for word in aggressive_words):
                mood = "aggressive"
            elif any(word in lyrics_lower for word in sad_words):
                mood = "melancholic"
            elif any(word in lyrics_lower for word in positive_words):
                mood = "energetic"

            # –ê–Ω–∞–ª–∏–∑ —ç–Ω–µ—Ä–≥–∏–∏
            energy = "medium"
            if (
                len(lyrics.split("!")) > 3
                or "–π–∞" in lyrics_lower
                or "—Å–∫—Ä—Ä" in lyrics_lower
            ):
                energy = "high"
            elif any(word in lyrics_lower for word in ["–º–µ–¥–ª–µ–Ω–Ω–æ", "—Å–ø–æ–∫–æ–π–Ω–æ", "—Ç–∏—Ö–æ"]):
                energy = "low"

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ explicit content
            explicit_words = [
                "—Å—É–∫–∞",
                "–±–ª—è—Ç—å",
                "—Ö—É–π",
                "–ø–∏–∑–¥–∞",
                "–µ–±–∞—Ç—å",
                "fuck",
                "shit",
                "bitch",
            ]
            explicit_content = any(word in lyrics_lower for word in explicit_words)

            # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            lines = lyrics.strip().split("\n")
            non_empty_lines = [line for line in lines if line.strip()]

            structure = "verse"
            if len(non_empty_lines) > 16:
                structure = "verse-chorus-verse"
            elif len(non_empty_lines) < 8:
                structure = "hook"

            # –ê–Ω–∞–ª–∏–∑ —Ä–∏—Ñ–º—ã (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            rhyme_scheme = "ABAB"
            if len(non_empty_lines) >= 4:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞ —Å—Ç—Ä–æ–∫
                last_words = [
                    line.strip().split()[-1].lower()
                    for line in non_empty_lines[:4]
                    if line.strip().split()
                ]
                if len(set(last_words)) == 1:
                    rhyme_scheme = "AAAA"
                elif len(set(last_words)) == 2:
                    rhyme_scheme = "AABB"

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            complexity = "intermediate"
            word_count = len(lyrics.split())
            unique_words = len(set(lyrics.lower().split()))
            diversity = unique_words / max(word_count, 1)

            if diversity > 0.7 and word_count > 200:
                complexity = "advanced"
            elif diversity < 0.5 or word_count < 100:
                complexity = "beginner"

            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã
            themes = []
            theme_keywords = {
                "street_life": ["—É–ª–∏—Ü–∞", "—Ä–∞–π–æ–Ω", "–¥–≤–æ—Ä", "–ø–æ–¥—ä–µ–∑–¥"],
                "money": ["–¥–µ–Ω—å–≥–∏", "cash", "money", "–±–∞–±–∫–∏", "–ª–∞–≤—ç"],
                "relationships": ["–ª—é–±–æ–≤—å", "–¥–µ–≤–æ—á–∫–∞", "–æ—Ç–Ω–æ—à–µ–Ω–∏—è", "—Å–µ–º—å—è"],
                "success": ["—É—Å–ø–µ—Ö", "fame", "—Å–ª–∞–≤–∞", "—Ç–æ–ø"],
                "struggle": ["–±–æ—Ä—å–±–∞", "struggle", "–ø—Ä–æ–±–ª–µ–º—ã", "—Ç—Ä—É–¥–Ω–æ—Å—Ç–∏"],
            }

            for theme, keywords in theme_keywords.items():
                if any(keyword in lyrics_lower for keyword in keywords):
                    themes.append(theme)

            if not themes:
                themes = ["life"]

            # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            authenticity_score = 0.5
            street_words = ["—É–ª–∏—Ü–∞", "—Ä–∞–π–æ–Ω", "–¥–≤–æ—Ä", "–ø–æ–¥—ä–µ–∑–¥", "–ø—Ä–∞–≤–¥–∞", "—Ä–µ–∞–ª—å–Ω–æ"]
            fake_words = ["–ø–æ–Ω—Ç", "—Ñ—ç–π–∫", "–ø–æ–∫–∞–∑—É—Ö–∞"]

            street_count = sum(1 for word in street_words if word in lyrics_lower)
            fake_count = sum(1 for word in fake_words if word in lyrics_lower)

            authenticity_score = min(
                0.3 + (street_count * 0.15) - (fake_count * 0.1), 1.0
            )

            creativity = min(0.4 + (diversity * 0.6), 1.0)
            commercial_appeal = (
                0.5
                + (0.1 if explicit_content else 0.2)
                + (0.1 if energy == "high" else 0)
            )
            uniqueness = diversity * 0.8 + 0.2

            # –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
            avg_quality = (
                authenticity_score + creativity + commercial_appeal + uniqueness
            ) / 4
            if avg_quality > 0.8:
                overall_quality = "excellent"
            elif avg_quality > 0.6:
                overall_quality = "good"
            elif avg_quality > 0.4:
                overall_quality = "fair"
            else:
                overall_quality = "poor"

            # AI likelihood (–æ–±—Ä–∞—Ç–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏)
            ai_likelihood = max(0.1, 1.0 - authenticity_score)

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            metadata = SongMetadata(
                genre=genre,
                mood=mood,
                energy_level=energy,
                explicit_content=explicit_content,
            )

            lyrics_analysis = LyricsAnalysis(
                structure=structure,
                rhyme_scheme=rhyme_scheme,
                complexity_level=complexity,
                main_themes=themes,
                emotional_tone=mood,
                storytelling_type="narrative"
                if "–∏—Å—Ç–æ—Ä–∏—è" in lyrics_lower or len(non_empty_lines) > 12
                else "conversational",
                wordplay_quality="excellent"
                if creativity > 0.8
                else ("good" if creativity > 0.6 else "basic"),
            )

            quality_metrics = QualityMetrics(
                authenticity_score=authenticity_score,
                lyrical_creativity=creativity,
                commercial_appeal=commercial_appeal,
                uniqueness=uniqueness,
                overall_quality=overall_quality,
                ai_likelihood=ai_likelihood,
            )

            return EnhancedSongData(
                artist=artist,
                title=title,
                metadata=metadata,
                lyrics_analysis=lyrics_analysis,
                quality_metrics=quality_metrics,
                model_used="mock_analyzer_v1",
                analysis_date=datetime.now().isoformat(),
            )

        except Exception as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ Mock –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return None


# TODO(code_review): [HIGH] GemmaProvider duplicates 100+ lines from OllamaProvider
# Extract shared logic to base class or mixin:
# - _create_analysis_prompt() is identical
# - _parse_analysis() is identical
# Use template method pattern or composition to eliminate duplication
class GemmaProvider(ModelProvider):
    """Provider for Google Gemma API.

    Connects to Google's Gemma model API for cloud-based AI analysis.
    Requires GOOGLE_API_KEY environment variable.

    Attributes:
        api_key: Google API key from environment (str or None).
        cost_per_1k_tokens: 0.0 within free tier limits.

    Example:
        >>> os.environ['GOOGLE_API_KEY'] = 'your_key_here'
        >>> provider = GemmaProvider()
        >>> if provider.available:
        ...     result = provider.analyze_song("Artist", "Title", lyrics)

    Note:
        Uses gemma-2-27b-it model.
        Requires google-generativeai package installed.
    """

    def __init__(self):
        """Initialize GemmaProvider with API key from environment.

        Reads GOOGLE_API_KEY from environment and checks availability.
        """
        super().__init__("Gemma")
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.available = self.check_availability()
        self.cost_per_1k_tokens = 0.0  # Free tier –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–æ–≤

    def check_availability(self) -> bool:
        """Check if Google API key is valid and library is installed.

        Returns:
            True if API key present and google-generativeai importable,
            False otherwise.

        Note:
            Configures API with key if available.
            Logs warnings if key missing or import fails.
        """
        if not self.api_key:
            logger.warning("‚åõ GOOGLE_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
            return False

        try:
            import google.generativeai as genai
            from google.generativeai.client import configure
            from google.generativeai.generative_models import GenerativeModel

            configure(api_key=self.api_key)
            logger.info("‚úÖ Google Gemma API –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            return True
        except ImportError:
            logger.warning("‚åõ google-generativeai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            return False
        except Exception as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Gemma API: {e}")
            return False

    def analyze_song(
        self, artist: str, title: str, lyrics: str
    ) -> EnhancedSongData | None:
        """Analyze song using Google Gemma API.

        Sends structured prompt to Gemma requesting JSON analysis.
        Parses response and constructs EnhancedSongData.

        Args:
            artist: Artist/performer name.
            title: Song title.
            lyrics: Complete song lyrics (truncated to 2000 chars in prompt).

        Returns:
            EnhancedSongData with analysis results, or None if:
                - Provider not available
                - API request fails
                - JSON parsing fails

        Note:
            Uses temperature=0.1 and max 1500 output tokens.
            Model: gemma-2-27b-it
        """
        if not self.available:
            return None

        try:
            from google.generativeai.generative_models import GenerativeModel

            model = GenerativeModel("gemma-2-27b-it")
            prompt = self._create_analysis_prompt(artist, title, lyrics)

            response = model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.1,
                    "max_output_tokens": 1500,
                },
            )

            if response.text:
                return self._parse_analysis(response.text, artist, title)
            logger.error("‚åõ Gemma: –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
            return None

        except Exception as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ Gemma: {e}")
            return None

    def _create_analysis_prompt(self, artist: str, title: str, lyrics: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è Gemma"""
        return f"""
Analyze this rap song and return results in STRICT JSON format:

Artist: {artist}
Title: {title}
Lyrics: {lyrics[:2000]}...

Return ONLY valid JSON with these exact fields:
{{
    "metadata": {{
        "genre": "rap/trap/drill/old-school/gangsta/emo-rap",
        "mood": "aggressive/melancholic/energetic/neutral",
        "energy_level": "low/medium/high",
        "explicit_content": true/false
    }},
    "lyrics_analysis": {{
        "structure": "verse-chorus-verse/freestyle/storytelling",
        "rhyme_scheme": "AABA/ABAB/complex/simple",
        "complexity_level": "beginner/intermediate/advanced",
        "main_themes": ["money", "relationships", "street_life", "success"],
        "emotional_tone": "positive/negative/neutral/mixed",
        "storytelling_type": "narrative/abstract/conversational",
        "wordplay_quality": "basic/good/excellent"
    }},
    "quality_metrics": {{
        "authenticity_score": 0.0-1.0,
        "lyrical_creativity": 0.0-1.0,
        "commercial_appeal": 0.0-1.0,
        "uniqueness": 0.0-1.0,
        "overall_quality": "poor/fair/good/excellent",
        "ai_likelihood": 0.0-1.0
    }}
}}

Return ONLY JSON, no additional text!
"""

    def _parse_analysis(
        self, analysis_text: str, artist: str, title: str
    ) -> EnhancedSongData | None:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            json_start = analysis_text.find("{")
            json_end = analysis_text.rfind("}") + 1

            if json_start == -1 or json_end <= json_start:
                logger.error("‚åõ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ Gemma")
                return None

            json_str = analysis_text[json_start:json_end]
            data = json.loads(json_str)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –¥–æ–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è
            metadata_data = data.get("metadata", {})
            lyrics_data = data.get("lyrics_analysis", {})
            quality_data = data.get("quality_metrics", {})

            # –î–æ–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è –≤ lyrics_analysis
            if "emotional_tone" not in lyrics_data:
                lyrics_data["emotional_tone"] = "neutral"
                logger.warning("‚ö†Ô∏è –î–æ–±–∞–≤–ª–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è emotional_tone")

            if "storytelling_type" not in lyrics_data:
                lyrics_data["storytelling_type"] = "conversational"
                logger.warning(
                    "‚ö†Ô∏è –î–æ–±–∞–≤–ª–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è storytelling_type"
                )

            if "wordplay_quality" not in lyrics_data:
                lyrics_data["wordplay_quality"] = "basic"
                logger.warning("‚ö†Ô∏è –î–æ–±–∞–≤–ª–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è wordplay_quality")

            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            metadata = SongMetadata(**metadata_data)
            lyrics_analysis = LyricsAnalysis(**lyrics_data)
            quality_metrics = QualityMetrics(**quality_data)

            return EnhancedSongData(
                artist=artist,
                title=title,
                metadata=metadata,
                lyrics_analysis=lyrics_analysis,
                quality_metrics=quality_metrics,
                model_used="gemma-2-27b-it",
                analysis_date=datetime.now().isoformat(),
            )

        except json.JSONDecodeError as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON Gemma: {e}")
            logger.debug(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {analysis_text[:500]}")
            return None
        except Exception as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ Gemma: {e}")
            return None


# TODO(code_review): [HIGH] God class - 562 lines with too many responsibilities
# Violates SRP - handles:
# 1. Provider management
# 2. Database operations
# 3. Batch processing
# 4. Statistics tracking
# 5. Safety validation orchestration
# Split into: ProviderManager, AnalysisOrchestrator, BatchProcessor, StatsCollector
# TODO(code_review): [MEDIUM] No unit tests - only integration test in main()
# Add proper unit tests with mocked dependencies
class MultiModelAnalyzer:
    """Multi-provider AI analyzer with fallback, safety validation, and interpretability.

    Main analyzer class that coordinates multiple AI providers with automatic fallback,
    provides safety validation, hallucination detection, and interpretable analysis
    with explanations.

    Architecture:
        - Provider priority: Ollama (free local) -> Gemma (cloud) -> Mock (fallback)
        - Safety validation via SafetyValidator
        - Interpretability via InterpretableAnalyzer
        - PostgreSQL persistence via PostgreSQLManager

    Attributes:
        providers: List of ModelProvider instances in priority order.
        current_provider: Active provider (first available).
        db_manager: PostgreSQLManager for database operations.
        stats: Dict tracking usage statistics (analyzed count, costs).
        interpretable_analyzer: InterpretableAnalyzer for explanations.
        safety_validator: SafetyValidator for reliability checks.

    Example:
        >>> analyzer = MultiModelAnalyzer()
        >>> await analyzer.initialize()
        >>> result = analyzer.analyze_song("Kendrick", "HUMBLE.", lyrics)
        >>> safe_result = analyzer.analyze_song_with_safety("Drake", "God's Plan", lyrics)
        >>> await analyzer.batch_analyze_from_db(limit=100)
        >>> await analyzer.close()
    """

    def __init__(self):
        """Initialize MultiModelAnalyzer with all providers and validators.

        Sets up provider chain (Ollama -> Gemma -> Mock), database manager,
        interpretable analyzer, and safety validator. Initializes usage statistics.

        Note:
            Database connection not established until initialize() is called.
            Providers check their own availability during initialization.
        """
        self.providers = []
        self.current_provider = None
        self.db_manager = PostgreSQLManager()
        self.stats = {
            "total_analyzed": 0,
            "ollama_used": 0,
            "gemma_used": 0,
            "mock_used": 0,
            "total_cost": 0.0,
        }

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        self._init_providers()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        self.interpretable_analyzer = InterpretableAnalyzer(self)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.safety_validator = SafetyValidator()

    async def initialize(self) -> bool:
        """Initialize database connection pool.

        Returns:
            True if database initialized successfully, False otherwise.

        Note:
            Must be called before any database operations (e.g., batch_analyze_from_db).
        """
        return await self.db_manager.initialize()

    async def close(self):
        """Close database connections and cleanup resources.

        Gracefully closes PostgreSQL connection pool.
        """
        await self.db_manager.close()

    def analyze_with_explanations(
        self, artist: str, title: str, lyrics: str
    ) -> ExplainableAnalysisResult | None:
        """Analyze song with AI decision explanations and interpretability.

        Delegates to InterpretableAnalyzer for explainable analysis.

        Args:
            artist: Artist/performer name.
            title: Song title.
            lyrics: Complete song lyrics.

        Returns:
            ExplainableAnalysisResult with analysis, explanations, confidence,
            decision factors, and influential phrases. None on failure.

        Example:
            >>> result = analyzer.analyze_with_explanations("Artist", "Title", lyrics)
            >>> print(f"Confidence: {result.confidence:.2f}")
            >>> print(result.explanation['genre_indicators'])
        """
        return self.interpretable_analyzer.analyze_with_explanation(
            artist, title, lyrics
        )

    async def explain_existing_analysis(self, track_id: int) -> dict | None:
        """–û–±—ä—è—Å–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∞–Ω–∞–ª–∏–∑ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            async with self.db_manager.get_connection() as conn:
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–µ—Å–Ω–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞
                query = """
                    SELECT t.artist, t.title, t.lyrics, ar.*
                    FROM tracks t
                    JOIN analysis_results ar ON t.id = ar.track_id
                    WHERE t.id = $1 AND ar.analyzer_type = 'multi_model_ai'
                """

                row = await conn.fetchrow(query, track_id)
                if not row:
                    logger.warning(
                        f"–ü–µ—Å–Ω—è —Å ID {track_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –Ω–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞"
                    )
                    return None

                # –ü–∞—Ä—Å–∏–º analysis_data
                analysis_data = json.loads(row["analysis_data"])

                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç EnhancedSongData –∏–∑ –¥–∞–Ω–Ω—ã—Ö –ë–î
                metadata = SongMetadata(
                    genre=analysis_data.get("metadata", {}).get("genre", "rap"),
                    mood=analysis_data.get("metadata", {}).get("mood", "neutral"),
                    energy_level=analysis_data.get("metadata", {}).get(
                        "energy_level", "medium"
                    ),
                    explicit_content=analysis_data.get("metadata", {}).get(
                        "explicit_content", False
                    ),
                )

                lyrics_analysis = LyricsAnalysis(
                    structure=analysis_data.get("lyrics_analysis", {}).get(
                        "structure", "verse"
                    ),
                    rhyme_scheme=analysis_data.get("lyrics_analysis", {}).get(
                        "rhyme_scheme", "unknown"
                    ),
                    complexity_level=analysis_data.get("lyrics_analysis", {}).get(
                        "complexity_level", "intermediate"
                    ),
                    main_themes=analysis_data.get("lyrics_analysis", {}).get(
                        "main_themes", []
                    ),
                    emotional_tone=analysis_data.get("lyrics_analysis", {}).get(
                        "emotional_tone", "neutral"
                    ),
                    storytelling_type=analysis_data.get("lyrics_analysis", {}).get(
                        "storytelling_type", "conversational"
                    ),
                    wordplay_quality=analysis_data.get("lyrics_analysis", {}).get(
                        "wordplay_quality", "basic"
                    ),
                )

                quality_metrics = QualityMetrics(
                    authenticity_score=analysis_data.get("quality_metrics", {}).get(
                        "authenticity_score", 0.5
                    ),
                    lyrical_creativity=analysis_data.get("quality_metrics", {}).get(
                        "lyrical_creativity", 0.5
                    ),
                    commercial_appeal=analysis_data.get("quality_metrics", {}).get(
                        "commercial_appeal", 0.5
                    ),
                    uniqueness=analysis_data.get("quality_metrics", {}).get(
                        "uniqueness", 0.5
                    ),
                    overall_quality=analysis_data.get("quality_metrics", {}).get(
                        "overall_quality", "fair"
                    ),
                    ai_likelihood=analysis_data.get("quality_metrics", {}).get(
                        "ai_likelihood", 0.5
                    ),
                )

                enhanced_data = EnhancedSongData(
                    artist=row["artist"],
                    title=row["title"],
                    metadata=metadata,
                    lyrics_analysis=lyrics_analysis,
                    quality_metrics=quality_metrics,
                    model_used=row["model_version"],
                    analysis_date=row["created_at"].isoformat(),
                )

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
                explanation = self.interpretable_analyzer.explain_decision(
                    row["lyrics"], enhanced_data
                )
                confidence = self.interpretable_analyzer.calculate_confidence(
                    enhanced_data, row["lyrics"]
                )
                decision_factors = self.interpretable_analyzer.extract_key_factors(
                    row["lyrics"], enhanced_data
                )
                influential_phrases = (
                    self.interpretable_analyzer.find_influential_phrases(
                        row["lyrics"], enhanced_data
                    )
                )

                return {
                    "song_info": {
                        "id": track_id,
                        "artist": row["artist"],
                        "title": row["title"],
                    },
                    "analysis": enhanced_data.model_dump(),
                    "explanation": explanation,
                    "confidence": confidence,
                    "decision_factors": decision_factors,
                    "influential_phrases": influential_phrases,
                }

        except Exception as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return None

    def _init_providers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞"""
        logger.info("üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤...")

        # 1. Ollama (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - –±–µ—Å–ø–ª–∞—Ç–Ω–æ)
        ollama = OllamaProvider()
        if ollama.available:
            self.providers.append(ollama)
            logger.info("‚úÖ Ollama –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

        # 2. Google Gemma (cloud fallback)
        gemma = GemmaProvider()
        if gemma.available:
            self.providers.append(gemma)
            logger.info("‚úÖ Google Gemma –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

        # 3. Mock Provider (–≤—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
        mock = MockProvider()
        self.providers.append(mock)
        logger.info("‚úÖ Mock –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–æ–±–∞–≤–ª–µ–Ω –∫–∞–∫ fallback")

        if not self.providers:
            logger.error("‚åõ –ù–∏ –æ–¥–∏–Ω AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
            raise Exception("No AI providers available")

        self.current_provider = self.providers[0]
        logger.info(f"üéØ –ê–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.current_provider.name}")

    def analyze_song(
        self, artist: str, title: str, lyrics: str
    ) -> EnhancedSongData | None:
        """Analyze song using multi-provider fallback strategy.

        Attempts analysis with providers in priority order (Ollama -> Gemma -> Mock).
        Returns first successful result. Updates usage statistics.

        Args:
            artist: Artist/performer name.
            title: Song title.
            lyrics: Complete song lyrics text.

        Returns:
            EnhancedSongData with analysis results from first successful provider,
            or None if all providers fail.

        Side Effects:
            - Updates self.stats with usage counts
            - Logs provider attempts and results

        Example:
            >>> result = analyzer.analyze_song("Kendrick", "HUMBLE.", lyrics)
            >>> if result:
            ...     print(f"Analyzed by: {result.model_used}")
            ...     print(f"Genre: {result.metadata.genre}")
        """

        for provider in self.providers:
            try:
                logger.info(f"ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ {provider.name}: {artist} - {title}")

                result = provider.analyze_song(artist, title, lyrics)

                if result:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    self.stats["total_analyzed"] += 1
                    if provider.name == "Ollama":
                        self.stats["ollama_used"] += 1
                    elif provider.name == "Gemma":
                        self.stats["gemma_used"] += 1
                    elif provider.name == "Mock":
                        self.stats["mock_used"] += 1

                    logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —á–µ—Ä–µ–∑ {provider.name}")
                    return result
                logger.warning(f"‚ö†Ô∏è {provider.name} –Ω–µ —Å–º–æ–≥ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å")

            except Exception as e:
                logger.error(f"‚åõ –û—à–∏–±–∫–∞ {provider.name}: {e}")
                continue

        logger.error(
            f"‚åõ –í—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ —Å–º–æ–≥–ª–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å: {artist} - {title}"
        )
        return None

    def get_stats(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        return {
            **self.stats,
            "available_providers": [p.name for p in self.providers],
            "current_provider": self.current_provider.name
            if self.current_provider
            else None,
        }

    # TODO(code_review): [HIGH] Method too long (60+ lines) - extract helper methods
    # Split into: fetch_unanalyzed_tracks(), analyze_single_track(), save_results()
    # TODO(code_review): [MEDIUM] Hardcoded 2 second sleep - make configurable
    # Add rate_limit_delay parameter with default value
    async def batch_analyze_from_db(self, limit: int = 100, offset: int = 0):  # TODO(code_review): [MEDIUM] Add return type hint -> None
        """Batch analyze unanalyzed songs from database.

        Fetches songs without multi_model_ai analysis from database,
        analyzes them using multi-provider strategy, and saves results.
        Includes progress tracking and error handling.

        Args:
            limit: Maximum number of songs to analyze (default: 100).
            offset: Number of songs to skip (default: 0).

        Returns:
            None (logs progress and summary).

        Side Effects:
            - Fetches songs from tracks table
            - Saves analysis results to analysis_results table
            - Updates self.stats with usage counts
            - 2 second delay between analyses to avoid rate limits

        Example:
            >>> analyzer = MultiModelAnalyzer()
            >>> await analyzer.initialize()
            >>> await analyzer.batch_analyze_from_db(limit=50)
            # Logs: "‚úÖ –£—Å–ø–µ—à–Ω–æ: 45, ‚åõ –û—à–∏–±–æ–∫: 5"

        Note:
            Requires initialize() to be called first.
            Only analyzes songs with lyrics longer than 50 characters.
        """

        logger.info(f"üéµ –ù–∞—á–∏–Ω–∞–µ–º batch –∞–Ω–∞–ª–∏–∑: {limit} –ø–µ—Å–µ–Ω —Å offset {offset}")

        try:
            async with self.db_manager.get_connection() as conn:
                # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Å–Ω–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                query = """
                    SELECT t.id, t.artist, t.title, t.lyrics 
                    FROM tracks t
                    LEFT JOIN analysis_results ar ON t.id = ar.track_id 
                        AND ar.analyzer_type = 'multi_model_ai'
                    WHERE t.lyrics IS NOT NULL 
                        AND LENGTH(TRIM(t.lyrics)) > 50
                        AND ar.id IS NULL  -- –¢–æ–ª—å–∫–æ –Ω–µ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
                    ORDER BY t.id
                    LIMIT $1 OFFSET $2
                """

                rows = await conn.fetch(query, limit, offset)
                logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(rows)} –ø–µ—Å–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

                successful = 0
                failed = 0

                for i, row in enumerate(rows, 1):
                    try:
                        logger.info(
                            f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(rows)} - {row['artist']} - {row['title']}"
                        )

                        analysis = self.analyze_song(
                            row["artist"], row["title"], row["lyrics"]
                        )

                        if analysis:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
                            await self._save_analysis_to_db(conn, row["id"], analysis)
                            successful += 1
                            logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω –∞–Ω–∞–ª–∏–∑ #{successful}")
                        else:
                            failed += 1
                            logger.warning("‚åõ –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å")

                        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                        if i < len(rows):  # –ù–µ –¥–µ–ª–∞–µ–º –ø–∞—É–∑—É –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–µ—Å–Ω–∏
                            await asyncio.sleep(2)  # 2 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∞–Ω–∞–ª–∏–∑–∞–º–∏  # TODO(code_review): [MEDIUM] Magic number - extract to constant or parameter

                    except Exception as e:
                        failed += 1
                        logger.error(f"‚åõ –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Å–Ω–∏ {row['id']}: {e}")
                        continue

                logger.info(f"""
                üéâ Batch –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!
                ‚úÖ –£—Å–ø–µ—à–Ω–æ: {successful}
                ‚åõ –û—à–∏–±–æ–∫: {failed}
                üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {self.get_stats()}
                """)

        except Exception as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ batch –∞–Ω–∞–ª–∏–∑–∞: {e}")

    # TODO(code_review): [MEDIUM] Add return type hint -> None
    async def _save_analysis_to_db(
        self, conn: asyncpg.Connection, track_id: int, analysis: EnhancedSongData
    ):  # TODO(code_review): [MEDIUM] Missing return type
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            analysis_data = {
                "metadata": analysis.metadata.model_dump(),
                "lyrics_analysis": analysis.lyrics_analysis.model_dump(),
                "quality_metrics": analysis.quality_metrics.model_dump(),
                "analysis_info": {
                    "analyzer_version": "multi_model_v2",  # TODO(code_review): [MEDIUM] Hardcoded version - use __version__ from module
                    "analysis_timestamp": analysis.analysis_date,
                    "model_used": analysis.model_used,
                },
            }
            # TODO(code_review): [MEDIUM] SQL query embedded in code - extract to constants or SQL file
            # Makes query optimization and testing difficult
            await conn.execute(
                """
                INSERT INTO analysis_results (
                    track_id, analyzer_type, sentiment, confidence,
                    complexity_score, themes, analysis_data,
                    processing_time_ms, model_version, created_at
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
            """,
                track_id,
                "multi_model_ai",  # TODO(code_review): [MEDIUM] Magic string - extract to constant
                analysis.metadata.mood,
                analysis.quality_metrics.authenticity_score,
                analysis.quality_metrics.lyrical_creativity,
                json.dumps(analysis.lyrics_analysis.main_themes),
                json.dumps(analysis_data),
                1000.0,  # placeholder processing time  # TODO(code_review): [HIGH] Fake value 1000.0 - implement actual timing or remove
                analysis.model_used,
                datetime.now(),
            )

        except Exception as e:
            logger.error(f"‚åõ –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
            raise

    def analyze_song_with_safety(
        self, artist: str, title: str, lyrics: str
    ) -> dict | None:
        """Analyze song with AI safety validation and hallucination detection.

        Performs standard multi-provider analysis followed by comprehensive
        safety validation using SafetyValidator to detect hallucinations,
        check consistency, and verify factual accuracy.

        Args:
            artist: Artist/performer name.
            title: Song title.
            lyrics: Complete song lyrics text.

        Returns:
            Dictionary containing:
                - analysis (EnhancedSongData): Full AI analysis result
                - validation (dict): Detailed validation metrics
                - is_safe (bool): Whether analysis passed validation
                - confidence (float): Overall reliability score 0.0-1.0
                - warnings (list): List of warning flag strings
                - summary (str): Human-readable validation summary
            Returns None if initial analysis fails.

        Example:
            >>> result = analyzer.analyze_song_with_safety("Drake", "God's Plan", lyrics)
            >>> if result and result['is_safe']:
            ...     print(f"‚úÖ Reliable: {result['summary']}")
            ...     print(f"Confidence: {result['confidence']:.2f}")
            >>> else:
            ...     print(f"‚ö†Ô∏è Warnings: {result['warnings']}")
            ...     print(f"Risk: {result['validation']['hallucination_risk']:.2f}")

        Note:
            Analysis considered reliable if:
            - hallucination_risk < 0.4
            - consistency_score > 0.6
            - factual_accuracy > 0.5
            - text_alignment > 0.4
            - No critical warning flags
        """

        logger.info(f"üõ°Ô∏è –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {artist} - {title}")

        # 1. –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        analysis_result = self.analyze_song(artist, title, lyrics)

        if not analysis_result:
            logger.error("‚åõ –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
            return None

        # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        analysis_dict = {
            "genre": analysis_result.metadata.genre,
            "mood": analysis_result.metadata.mood,
            "energy_level": analysis_result.metadata.energy_level,
            "explicit_content": analysis_result.metadata.explicit_content,
            "structure": analysis_result.lyrics_analysis.structure,
            "rhyme_scheme": analysis_result.lyrics_analysis.rhyme_scheme,
            "complexity_level": analysis_result.lyrics_analysis.complexity_level,
            "main_themes": analysis_result.lyrics_analysis.main_themes,
            "authenticity_score": analysis_result.quality_metrics.authenticity_score,
            "lyrical_creativity": analysis_result.quality_metrics.lyrical_creativity,
            "commercial_appeal": analysis_result.quality_metrics.commercial_appeal,
            "uniqueness": analysis_result.quality_metrics.uniqueness,
            "overall_quality": analysis_result.quality_metrics.overall_quality,
            "ai_likelihood": analysis_result.quality_metrics.ai_likelihood,
        }

        # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ SafetyValidator
        validation_result = self.safety_validator.validate_analysis(
            lyrics, analysis_dict
        )

        # 4. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        logger.info(
            f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {validation_result['validation_summary']}"
        )

        if not validation_result["is_reliable"]:
            logger.warning("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–Ω –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–º!")
            logger.warning(
                f"   ‚Ä¢ –†–∏—Å–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: {validation_result['hallucination_risk']:.3f}"
            )
            logger.warning(
                f"   ‚Ä¢ –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {validation_result['consistency_score']:.3f}"
            )
            logger.warning(
                f"   ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤: {validation_result['factual_accuracy']:.3f}"
            )

            if validation_result["warning_flags"]:
                logger.warning(
                    f"   ‚Ä¢ –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {', '.join(validation_result['warning_flags'])}"
                )
        else:
            logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
            logger.info(
                f"   ‚Ä¢ –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å: {validation_result['reliability_score']:.3f}"
            )

        # 5. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return {
            "analysis": analysis_result,
            "validation": validation_result,
            "is_safe": validation_result["is_reliable"],
            "confidence": validation_result["reliability_score"],
            "warnings": validation_result["warning_flags"],
            "summary": validation_result["validation_summary"],
        }


# TODO(code_review): [HIGH] main() is 171 lines - too long for a test function
# Split into separate test functions: test_explainable_analysis(), test_safety_validation(), etc.
# TODO(code_review): [CRITICAL] Integration tests in main() instead of proper test suite
# Move to tests/ directory using pytest framework with fixtures and mocks
# Current approach:
# 1. Can't run individual tests
# 2. No test isolation
# 3. Requires live database
# 4. No assertions - just prints
# TODO(code_review): [HIGH] Test data hardcoded in main() - extract to fixtures
async def main():
    """Test multi-model analyzer with interpretability and safety features.

    Comprehensive test suite demonstrating:
        - Multi-provider initialization and fallback
        - Explainable analysis with decision explanations
        - Safety validation and hallucination detection
        - Statistics tracking and cost optimization

    Returns:
        None. Prints test results to stdout and logs to file.

    Raises:
        Exception: Any unhandled errors are logged with traceback.

    Example:
        >>> asyncio.run(main())
        # Outputs test results with analysis examples and validation demos

    Note:
        Uses test lyrics in Russian for demonstration.
        Requires database connection (continues if fails).
    """

    print("ü§ñ –ú–Ω–æ–≥–æ–º–æ–¥–µ–ª—å–Ω—ã–π AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏ —Ä–µ—à–µ–Ω–∏–π")
    print("=" * 70)  # TODO(code_review): [LOW] Magic number 70 - extract to constant

    try:
        analyzer = MultiModelAnalyzer()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        if not await analyzer.initialize():
            print("‚åõ –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
            return

        print(f"üìä –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã: {[p.name for p in analyzer.providers]}")
        print(
            f"üéØ –ê–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {analyzer.current_provider.name if analyzer.current_provider else 'None'}"
        )

        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏...")

        # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –ø–µ—Å–Ω–∏
        test_lyrics = """
        –Ø —Å —É–ª–∏—Ü—ã, —Ä–∞–π–æ–Ω –º–µ–Ω—è –≤–æ—Å–ø–∏—Ç–∞–ª
        –í –ø–æ–¥—ä–µ–∑–¥–∞—Ö —Ç–µ–º–Ω—ã—Ö –ø—Ä–∞–≤–¥—É –ø–æ–∑–Ω–∞–≤–∞–ª
        –ú–æ–ª–æ–¥–æ—Å—Ç—å –ø—Ä–æ—à–ª–∞ –≤ –¥—ã–º—É –∏ –¥—Ä–∞–∫–∞—Ö
        –¢–µ–ø–µ—Ä—å —á–∏—Ç–∞—é –ø—Ä–∞–≤–¥—É –≤ —ç—Ç–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö
        
        –î–µ–Ω—å–≥–∏, —Å–ª–∞–≤–∞ - –≤—Å–µ —ç—Ç–æ –ø—É—Å—Ç–æ—Ç–∞
        –ì–ª–∞–≤–Ω–æ–µ –æ—Å—Ç–∞—Ç—å—Å—è —Å–æ–±–æ–π –¥–æ –∫–æ–Ω—Ü–∞
        –°–µ–º—å—è –∏ –≤–µ—Ä–Ω—ã–µ –¥—Ä—É–∑—å—è —Ä—è–¥–æ–º
        –≠—Ç–æ –±–æ–≥–∞—Ç—Å—Ç–≤–æ, –∞ –Ω–µ —Ñ–∞–ª—å—à–∏–≤—ã–π —è–¥
        """

        # –ê–Ω–∞–ª–∏–∑ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
        explainable_result = analyzer.analyze_with_explanations(
            "–¢–µ—Å—Ç–æ–≤—ã–π –∞—Ä—Ç–∏—Å—Ç", "–¢–µ—Å—Ç–æ–≤—ã–π —Ç—Ä–µ–∫", test_lyrics
        )

        if explainable_result:
            print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê –° –û–ë–™–Ø–°–ù–ï–ù–ò–Ø–ú–ò:")
            print("-" * 50)

            # –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑
            analysis = explainable_result.analysis
            print(f"üéµ –ñ–∞–Ω—Ä: {analysis.metadata.genre}")
            print(f"üòä –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {analysis.metadata.mood}")
            print(f"‚ö° –≠–Ω–µ—Ä–≥–∏—è: {analysis.metadata.energy_level}")
            print(f"üèÜ –ö–∞—á–µ—Å—Ç–≤–æ: {analysis.quality_metrics.overall_quality}")
            print(f"üìù –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {explainable_result.confidence:.2f}")

            # –û–±—ä—è—Å–Ω–µ–Ω–∏—è
            print("\nüí° –û–ë–™–Ø–°–ù–ï–ù–ò–Ø –†–ï–®–ï–ù–ò–ô:")
            for category, explanations in explainable_result.explanation.items():
                if explanations:
                    print(f"  {category.replace('_', ' ').title()}:")
                    for exp in explanations:
                        print(f"    ‚Ä¢ {exp}")

            # –í–ª–∏—è—Ç–µ–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã
            print("\nüîç –í–õ–ò–Ø–¢–ï–õ–¨–ù–´–ï –§–†–ê–ó–´:")
            for category, phrases in explainable_result.influential_phrases.items():
                if phrases:
                    print(f"  {category.replace('_', ' ').title()}:")
                    for phrase in phrases[:2]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 2
                        print(f"    ‚Ä¢ '{phrase}'")

            # –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
            print("\nüìä –ö–õ–Æ–ß–ï–í–´–ï –§–ê–ö–¢–û–†–´ (—Ç–æ–ø-5):")
            top_factors = sorted(
                explainable_result.decision_factors.items(),
                key=lambda x: x[1],
                reverse=True,
            )[:5]
            for factor, value in top_factors:
                print(f"  ‚Ä¢ {factor.replace('_', ' ').title()}: {value:.3f}")

        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è SafetyValidator
        print("\nüõ°Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI Safety & Hallucination Detection...")

        # –¢–µ—Å—Ç —Å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        problematic_lyrics = """
        –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
        """

        safe_result = analyzer.analyze_song_with_safety(
            "Test Artist", "Problematic Track", problematic_lyrics
        )

        if safe_result:
            print("\nüõ°Ô∏è –†–ï–ó–£–õ–¨–¢–ê–¢ –ë–ï–ó–û–ü–ê–°–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:")
            print("-" * 50)
            print(
                f"‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: {'–ù–ê–î–ï–ñ–ï–ù' if safe_result['is_safe'] else '–ù–ï–ù–ê–î–ï–ñ–ï–ù'}"
            )
            print(f"üìù –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {safe_result['confidence']:.3f}")
            print(f"üìÑ –†–µ–∑—é–º–µ: {safe_result['summary']}")

            if safe_result["warnings"]:
                print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
                for warning in safe_result["warnings"]:
                    print(f"   ‚Ä¢ {warning}")

            # –î–µ—Ç–∞–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            validation = safe_result["validation"]
            print("\nüìä –î–ï–¢–ê–õ–ò –í–ê–õ–ò–î–ê–¶–ò–ò:")
            print(f"   ‚Ä¢ –†–∏—Å–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: {validation['hallucination_risk']:.3f}")
            print(f"   ‚Ä¢ –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {validation['consistency_score']:.3f}")
            print(f"   ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤: {validation['factual_accuracy']:.3f}")
            print(f"   ‚Ä¢ –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ–∫—Å—Ç—É: {validation['text_alignment']:.3f}")

        # –¢–µ—Å—Ç —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        print("\nüìÑ –¢–µ—Å—Ç —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º...")
        normal_safe_result = analyzer.analyze_song_with_safety(
            "–¢–µ—Å—Ç–æ–≤—ã–π –∞—Ä—Ç–∏—Å—Ç", "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç—Ä–µ–∫", test_lyrics
        )

        if normal_safe_result:
            print(
                f"‚úÖ –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: {'–ù–ê–î–ï–ñ–ï–ù' if normal_safe_result['is_safe'] else '–ù–ï–ù–ê–î–ï–ñ–ï–ù'}"
            )
            print(f"üìù –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {normal_safe_result['confidence']:.3f}")
            print(f"üìÑ –†–µ–∑—é–º–µ: {normal_safe_result['summary']}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = analyzer.get_stats()
        print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {stats['total_analyzed']}")
        print(f"  ‚Ä¢ Ollama –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω: {stats['ollama_used']} —Ä–∞–∑")
        print(f"  ‚Ä¢ Gemma –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω: {stats['gemma_used']} —Ä–∞–∑")
        print(f"  ‚Ä¢ Mock –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω: {stats['mock_used']} —Ä–∞–∑")
        print(f"  ‚Ä¢ –û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${stats['total_cost']:.4f}")

        print("\n‚úÖ AI Safety & Hallucination Detection - –ì–û–¢–û–í–û!")
        print("üõ°Ô∏è –¢–µ–ø–µ—Ä—å AI –∞–Ω–∞–ª–∏–∑ –≤–∫–ª—é—á–∞–µ—Ç:")
        print("   ‚Ä¢ Interpretability & Model Understanding")
        print("   ‚Ä¢ Safety & Hallucination Detection")
        print("   ‚Ä¢ Consistency Validation")
        print("   ‚Ä¢ Factual Accuracy Checking")
        print("üéØ –ü—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏!")

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
        await analyzer.close()

    except Exception as e:
        logger.error(f"‚åõ –û—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
