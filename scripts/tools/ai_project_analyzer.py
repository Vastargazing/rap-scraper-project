#!/usr/bin/env python3
"""
üß† AI Project Analyzer ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ AST-–ø–∞—Ä—Å–∏–Ω–≥ –≤–º–µ—Å—Ç–æ –ø—Ä–∏–º–∏—Ç–∏–≤–Ω–æ–≥–æ grep
- –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º PostgreSQL –º–∏–≥—Ä–∞—Ü–∏–∏
- –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–π –ø–æ–∏—Å–∫ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
- –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
python scripts/tools/ai_project_analyzer.py --analyze

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- Python 3.8+
- ast, dataclasses, pathlib

–†–ï–ó–£–õ–¨–¢–ê–¢:
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
- –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

–ê–í–¢–û–†: Vastargazing
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""

import os
import ast
import json
import sqlite3
import subprocess
import sys
from pathlib import Path
from collections import defaultdict, Counter
from dataclasses import dataclass
from typing import Dict, List, Set, Tuple, Optional
import hashlib

@dataclass
class CodeMetrics:
    file_path: str
    lines_of_code: int
    functions: List[str]
    classes: List[str]
    imports: List[str]
    complexity_score: float
    last_modified: float
    is_test: bool
    is_legacy: bool

@dataclass
class DuplicationResult:
    similarity_score: float
    file1: str
    file2: str
    duplicate_lines: List[str]
    suggestion: str

class ProjectIntelligence:
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ results/ —Å–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø—Ä–æ–µ–∫—Ç–∞
        self.results_dir = self.project_root / "results"
        self.results_dir.mkdir(exist_ok=True)
        
        self.metrics = {}
        self.duplicates = []
        self.unused_files = set()
        self.architecture_violations = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Å–ª–æ–∏ –ø—Ä–æ–µ–∫—Ç–∞
        self.architecture_layers = {
            "database": ["src/database/", "postgres_adapter.py"],
            "analyzers": ["src/analyzers/"],
            "cli": ["src/cli/", "main.py"],
            "models": ["src/models/"],
            "scripts": ["scripts/"],
            "legacy": ["scripts/archive/", "_sqlite.py", "data_backup_"],
            "tests": ["tests/", "test_"]
        }
    
    def analyze_project(self) -> Dict:
        """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
        print("üîç –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–µ–∫—Ç–∞...")
        
        # 1. –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –ø–æ —Ñ–∞–π–ª–∞–º
        self._collect_file_metrics()
        
        # 2. –ü–æ–∏—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        self._find_semantic_duplicates()
        
        # 3. –ü–æ–∏—Å–∫ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
        self._find_unused_files()
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π
        self._check_architecture_violations()
        
        # 5. –ê–Ω–∞–ª–∏–∑ PostgreSQL migration status
        self._analyze_postgresql_migration()
        
        return self._generate_report()
    
    def _collect_file_metrics(self):
        """–°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º Python —Ñ–∞–π–ª–∞–º"""
        for py_file in self.project_root.rglob("*.py"):
            if self._should_skip_file(py_file):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                tree = ast.parse(content)
                
                metrics = CodeMetrics(
                    file_path=str(py_file),
                    lines_of_code=len(content.splitlines()),
                    functions=self._extract_functions(tree),
                    classes=self._extract_classes(tree),
                    imports=self._extract_imports(tree),
                    complexity_score=self._calculate_complexity(tree),
                    last_modified=py_file.stat().st_mtime,
                    is_test="test" in str(py_file).lower(),
                    is_legacy=self._is_legacy_file(py_file)
                )
                
                self.metrics[str(py_file)] = metrics
                
            except (SyntaxError, UnicodeDecodeError) as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å {py_file}: {e}")
    
    def _find_semantic_duplicates(self):
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ –±–ª–æ–∫–∏ –∫–æ–¥–∞"""
        function_signatures = defaultdict(list)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ —Å–∏–≥–Ω–∞—Ç—É—Ä–∞–º
        for file_path, metrics in self.metrics.items():
            for func in metrics.functions:
                signature = self._normalize_function_signature(func)
                function_signatures[signature].append((file_path, func))
        
        # –ò—â–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        for signature, occurrences in function_signatures.items():
            if len(occurrences) > 1:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç—ã
                for i, (file1, func1) in enumerate(occurrences):
                    for file2, func2 in occurrences[i+1:]:
                        similarity = self._calculate_similarity(file1, file2, func1, func2)
                        if similarity > 0.7:  # 70% –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
                            self.duplicates.append(DuplicationResult(
                                similarity_score=similarity,
                                file1=file1,
                                file2=file2,
                                duplicate_lines=[func1, func2],
                                suggestion=self._suggest_refactoring(file1, file2)
                            ))
    
    def _find_unused_files(self):
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∞–π–ª—ã"""
        all_imports = set()
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–º–ø–æ—Ä—Ç—ã
        for metrics in self.metrics.values():
            all_imports.update(metrics.imports)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
        for file_path in self.metrics.keys():
            module_name = self._file_to_module_name(file_path)
            
            # –ï—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–∏–≥–¥–µ –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è entry point
            if not self._is_imported(module_name, all_imports) and not self._is_entry_point(file_path):
                self.unused_files.add(file_path)
    
    def _check_architecture_violations(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
        for file_path, metrics in self.metrics.items():
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: Legacy –∫–æ–¥ –Ω–µ –¥–æ–ª–∂–µ–Ω –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ –Ω–æ–≤–æ–º –∫–æ–¥–µ
            if not metrics.is_legacy:
                for imp in metrics.imports:
                    if "sqlite" in imp.lower() or "archive" in imp:
                        self.architecture_violations.append(
                            f"‚ùå {file_path}: –ò–º–ø–æ—Ä—Ç legacy –∫–æ–¥–∞ '{imp}'"
                        )
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –°–ª–æ–∏ –Ω–µ –¥–æ–ª–∂–Ω—ã –Ω–∞—Ä—É—à–∞—Ç—å—Å—è
            layer = self._determine_layer(file_path)
            for imp in metrics.imports:
                imp_layer = self._determine_layer_by_import(imp)
                if not self._is_allowed_dependency(layer, imp_layer):
                    self.architecture_violations.append(
                        f"‚ùå {file_path}: –ù–∞—Ä—É—à–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã - {layer} -> {imp_layer}"
                    )
    
    def _analyze_postgresql_migration(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—É—Å –º–∏–≥—Ä–∞—Ü–∏–∏ –Ω–∞ PostgreSQL"""
        sqlite_usage = []
        postgresql_usage = []
        
        for file_path, metrics in self.metrics.items():
            if metrics.is_legacy:
                continue
                
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read().lower()
                
                if "sqlite3" in content or "sqlite" in content:
                    sqlite_usage.append(file_path)
                
                if "postgresql" in content or "asyncpg" in content or "psycopg2" in content:
                    postgresql_usage.append(file_path)
                    
            except Exception:
                continue
        
        self.migration_status = {
            "sqlite_files": sqlite_usage,
            "postgresql_files": postgresql_usage,
            "migration_complete": len(sqlite_usage) == 0
        }
    
    def _generate_report(self) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –¥–ª—è AI"""
        total_files = len(self.metrics)
        legacy_files = sum(1 for m in self.metrics.values() if m.is_legacy)
        
        report = {
            "summary": {
                "total_files": total_files,
                "active_files": total_files - legacy_files,
                "legacy_files": legacy_files,
                "average_complexity": sum(m.complexity_score for m in self.metrics.values()) / total_files,
                "migration_status": self.migration_status.get("migration_complete", False)
            },
            "duplicates": [{
                "similarity": d.similarity_score,
                "files": [d.file1, d.file2],
                "suggestion": d.suggestion
            } for d in self.duplicates],
            "unused_files": list(self.unused_files),
            "architecture_violations": self.architecture_violations,
            "ai_recommendations": self._generate_ai_recommendations()
        }
        
        return report
    
    def _generate_ai_recommendations(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥—É–±–ª–∏–∫–∞—Ç–∞–º
        if len(self.duplicates) > 5:
            recommendations.append(
                f"üîÑ –ù–∞–π–¥–µ–Ω–æ {len(self.duplicates)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∫–æ–¥–∞. "
                "–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–æ–∑–¥–∞—Ç—å –æ–±—â–∏–µ —É—Ç–∏–ª–∏—Ç—ã –≤ src/utils/"
            )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–º —Ñ–∞–π–ª–∞–º
        if len(self.unused_files) > 3:
            recommendations.append(
                f"üóëÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(self.unused_files)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤. "
                "–†–µ–∫–æ–º–µ–Ω–¥—É—é –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –≤ scripts/archive/"
            )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
        if self.architecture_violations:
            recommendations.append(
                "‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–∞—Ä—É—à–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. "
                "–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –∏–º–ø–æ—Ä—Ç–æ–≤ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
            )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–∏–≥—Ä–∞—Ü–∏–∏
        if not self.migration_status.get("migration_complete", True):
            recommendations.append(
                "üîÑ –ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ PostgreSQL –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. "
                "–ù–∞–π–¥–µ–Ω—ã –æ—Å—Ç–∞—Ç–∫–∏ SQLite –∫–æ–¥–∞ –≤ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö"
            )
        
        return recommendations
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _should_skip_file(self, file_path: Path) -> bool:
        skip_patterns = ['.venv', '__pycache__', '.git', 'node_modules']
        return any(pattern in str(file_path) for pattern in skip_patterns)
    
    def _extract_functions(self, tree: ast.AST) -> List[str]:
        functions = []
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                functions.append(node.name)
        return functions
    
    def _extract_classes(self, tree: ast.AST) -> List[str]:
        classes = []
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                classes.append(node.name)
        return classes
    
    def _extract_imports(self, tree: ast.AST) -> List[str]:
        imports = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                imports.append(module)
        return imports
    
    def _calculate_complexity(self, tree: ast.AST) -> float:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        complexity = 1
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.Try, ast.With)):
                complexity += 1
        return complexity / 10  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    
    def _is_legacy_file(self, file_path: Path) -> bool:
        path_str = str(file_path)
        return any(pattern in path_str for pattern in ["archive", "sqlite", "backup"])
    
    def _normalize_function_signature(self, func_name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–∏–≥–Ω–∞—Ç—É—Ä—É —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        return func_name.lower().replace("_", "")
    
    def _calculate_similarity(self, file1: str, file2: str, func1: str, func2: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –ø–æ—Ö–æ–∂–µ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–π"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AST-–∞–Ω–∞–ª–∏–∑
        if func1 == func2:
            return 0.9
        return 0.5 if self._normalize_function_signature(func1) == self._normalize_function_signature(func2) else 0.0
    
    def _suggest_refactoring(self, file1: str, file2: str) -> str:
        """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–ø–æ—Å–æ–± —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞"""
        if "analyzer" in file1 and "analyzer" in file2:
            return "–í—ã–Ω–µ—Å—Ç–∏ –æ–±—â—É—é –ª–æ–≥–∏–∫—É –≤ BaseAnalyzer"
        elif "script" in file1 and "script" in file2:
            return "–°–æ–∑–¥–∞—Ç—å –æ–±—â—É—é —É—Ç–∏–ª–∏—Ç—É –≤ src/utils/"
        return "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –º–æ–¥—É–ª—è"
    
    def _file_to_module_name(self, file_path: str) -> str:
        return file_path.replace("/", ".").replace("\\", ".").replace(".py", "")
    
    def _is_imported(self, module_name: str, all_imports: Set[str]) -> bool:
        return any(module_name in imp or imp in module_name for imp in all_imports)
    
    def _is_entry_point(self, file_path: str) -> bool:
        entry_points = ["main.py", "__init__.py", "api.py", "app.py"]
        return any(ep in file_path for ep in entry_points)
    
    def _determine_layer(self, file_path: str) -> str:
        for layer, patterns in self.architecture_layers.items():
            if any(pattern in file_path for pattern in patterns):
                return layer
        return "unknown"
    
    def _determine_layer_by_import(self, import_name: str) -> str:
        for layer, patterns in self.architecture_layers.items():
            if any(pattern.replace("/", ".") in import_name for pattern in patterns):
                return layer
        return "external"
    
    def _is_allowed_dependency(self, from_layer: str, to_layer: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Å–ª–æ—è–º–∏"""
        allowed_deps = {
            "cli": ["models", "analyzers", "database", "external"],
            "analyzers": ["models", "database", "external"],
            "database": ["models", "external"],
            "models": ["external"],
            "scripts": ["cli", "analyzers", "database", "models", "external"],
            "tests": ["cli", "analyzers", "database", "models", "external"]
        }
        return to_layer in allowed_deps.get(from_layer, [])

def generate_vscode_config(project_analysis: Dict) -> Dict:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é VS Code –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    exclude_patterns = {
        "**/__pycache__": True,
        "**/.pytest_cache": True,
        "**/venv": True,
        "**/.venv": True,
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º legacy —Ñ–∞–π–ª—ã –≤ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    if project_analysis["summary"]["legacy_files"] > 0:
        exclude_patterns.update({
            "scripts/archive/**": True,
            "data/data_backup_*.db": True,
            "**/*sqlite*.py": True
        })
    
    config = {
        "python.defaultInterpreterPath": "./venv/bin/python",
        "python.linting.enabled": True,
        "python.formatting.provider": "black",
        "python.analysis.autoImportCompletions": True,
        "python.analysis.typeCheckingMode": "basic",
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∞–π–ª–æ–≤
        "files.exclude": exclude_patterns,
        "search.exclude": exclude_patterns,
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è AI
        "github.copilot.enable": {
            "*": True,
            "yaml": True,
            "plaintext": False,
            "markdown": True
        },
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤
        "search.maxResults": 2000,
        "search.smartCase": True,
        
        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –≤–∞–∂–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        "workbench.colorCustomizations": {
            "tab.activeBorder": "#ff6b6b",
            "tab.unfocusedActiveBorder": "#ff6b6b50"
        },
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        "todo-tree.tree.showScanModeButton": False,
        "todo-tree.highlights.defaultHighlight": {
            "icon": "alert",
            "type": "tag",
            "foreground": "red",
            "background": "white",
            "opacity": 50,
            "iconColour": "blue"
        }
    }
    
    return config

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –∑–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–µ–∫—Ç–∞...")
    
    analyzer = ProjectIntelligence()
    analysis_result = analyzer.analyze_project()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
    print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {analysis_result['summary']['total_files']}")
    print(f"–ê–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {analysis_result['summary']['active_files']}")
    print(f"Legacy —Ñ–∞–π–ª–æ–≤: {analysis_result['summary']['legacy_files']}")
    print(f"–°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {analysis_result['summary']['average_complexity']:.2f}")
    print(f"PostgreSQL –º–∏–≥—Ä–∞—Ü–∏—è: {'‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞' if analysis_result['summary']['migration_status'] else '‚ùå –í –ø—Ä–æ—Ü–µ—Å—Å–µ'}")
    
    if analysis_result['duplicates']:
        print(f"\nüîÑ –ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(analysis_result['duplicates'])}")
        for dup in analysis_result['duplicates'][:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3
            print(f"  - {dup['similarity']:.1%} —Å—Ö–æ–∂–µ—Å—Ç—å: {Path(dup['files'][0]).name} <-> {Path(dup['files'][1]).name}")
    
    if analysis_result['unused_files']:
        print(f"\nüóëÔ∏è –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∞–π–ª—ã: {len(analysis_result['unused_files'])}")
        for file in list(analysis_result['unused_files'])[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3
            print(f"  - {Path(file).name}")
    
    if analysis_result['architecture_violations']:
        print(f"\n‚ö†Ô∏è –ù–∞—Ä—É—à–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: {len(analysis_result['architecture_violations'])}")
        for violation in analysis_result['architecture_violations'][:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3
            print(f"  - {violation}")
    
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø AI:")
    for rec in analysis_result['ai_recommendations']:
        print(f"  {rec}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º VS Code –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    vscode_config = generate_vscode_config(analysis_result)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø—Ä–æ–µ–∫—Ç–∞
    os.makedirs('.vscode', exist_ok=True)
    
    with open('.vscode/settings.json', 'w', encoding='utf-8') as f:
        json.dump(vscode_config, f, indent=2, ensure_ascii=False)
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ results/
    with open(analyzer.results_dir / 'project_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, indent=2, ensure_ascii=False)
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {analyzer.results_dir / 'project_analysis.json'}")
    print("‚öôÔ∏è VS Code –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –≤ .vscode/settings.json")

if __name__ == "__main__":
    main()
