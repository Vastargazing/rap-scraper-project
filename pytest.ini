[tool:pytest]
# ============================================================================
# Pytest Configuration for Rap Analyzer ML Platform
# Production-grade testing setup for AI/ML backend systems
# ============================================================================

# Test Discovery
# ============================================================================
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Python Path Configuration
# ============================================================================
pythonpath = src

# Async Support (для FastAPI и async database operations)
# ============================================================================
asyncio_mode = auto

# Test Markers
# ============================================================================
markers =
    # Performance Testing
    benchmark: benchmark performance tests
    quick: quick tests for CI/CD (< 1 second)
    slow: slow running tests (> 5 seconds)
    stress: stress and load tests
    memory: memory usage tests
    
    # Test Types
    unit: unit tests (isolated, no external dependencies)
    integration: integration tests (database, APIs)
    e2e: end-to-end tests (full workflow)
    smoke: smoke tests (basic functionality check)
    
    # AI/ML Components (СПЕЦИФИЧНО ДЛЯ ТВОЕГО ПРОЕКТА!)
    analyzer: tests for AI analyzers (GPT, Claude, Gemini, etc)
    llm: tests involving LLM API calls (expensive!)
    vector: tests for pgvector and semantic search
    embedding: tests for text embeddings
    rag: tests for RAG pipeline components
    
    # Backend Components
    api: tests for FastAPI endpoints
    database: tests for PostgreSQL/pgvector
    cache: tests for caching layer
    auth: tests for authentication/authorization
    
    # Resource Markers
    expensive: tests that cost money (LLM API calls)
    network: tests requiring network access
    external_api: tests calling external APIs
    
    # Environment Specific
    dev: development environment only
    staging: staging environment only
    prod: production environment tests

# Pytest Options
# ============================================================================
addopts = 
    # Strict mode
    --strict-markers
    --strict-config
    
    # Output formatting
    -v
    --tb=short
    --show-capture=no
    
    # Coverage (uncomment if using pytest-cov)
    # --cov=src
    # --cov-report=html
    # --cov-report=term-missing
    # --cov-fail-under=80
    
    # Benchmark defaults
    --benchmark-skip
    --benchmark-disable-gc
    --benchmark-warmup=on
    --benchmark-warmup-iterations=2
    
    # Warnings
    --disable-warnings
    
    # Performance
    -n auto
    --maxfail=5

# Benchmark Configuration (для pytest-benchmark)
# ============================================================================
benchmark-min-rounds = 3
benchmark-max-time = 10.0
benchmark-min-time = 0.05
benchmark-timer = time.perf_counter
benchmark-disable-gc = true
benchmark-warmup = true

# Warning Filters
# ============================================================================
filterwarnings =
    # Ignore deprecation warnings from dependencies
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::pytest.PytestUnraisableExceptionWarning
    
    # Ignore specific warnings (uncomment and customize as needed)
    # ignore::UserWarning:langchain
    # ignore::FutureWarning:transformers

# Test Execution
# ============================================================================
# Minimum pytest version
minversion = 7.0

# Console output format
console_output_style = progress

# Log configuration
log_cli = false
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Test timeout (requires pytest-timeout)
# timeout = 300
# timeout_method = thread

# ============================================================================
# Usage Examples:
# ============================================================================
# 
# Basic test runs:
#   pytest                              # Run all tests
#   pytest tests/                       # Run tests in directory
#   pytest tests/test_analyzers.py      # Run specific file
#   pytest -k "test_gpt"                # Run tests matching pattern
#
# By markers:
#   pytest -m unit                      # Only unit tests
#   pytest -m "not slow"                # All except slow tests
#   pytest -m "not expensive"           # Skip LLM API calls
#   pytest -m "quick and not llm"       # Fast tests without LLM
#   pytest -m "analyzer and not expensive"  # Analyzer tests without API costs
#
# For CI/CD:
#   pytest -m "not slow and not expensive"  # Fast and free tests
#   pytest -m smoke                     # Smoke tests only
#   pytest -m "integration and not external_api"  # Integration without APIs
#
# Performance testing:
#   pytest --benchmark-only             # Run only benchmarks
#   pytest -m benchmark                 # Benchmarks (marked explicitly)
#   pytest --benchmark-compare          # Compare with previous results
#
# Parallel execution (requires pytest-xdist):
#   pytest -n auto                      # Auto-detect CPU count
#   pytest -n 4                         # Use 4 workers
#
# With coverage:
#   pytest --cov=src --cov-report=html  # Generate coverage report
#
# Debug mode:
#   pytest -vv -s                       # Very verbose with output
#   pytest --pdb                        # Drop into debugger on failure
#   pytest --lf                         # Run last failed tests
#   pytest --ff                         # Run failures first
#
# ============================================================================