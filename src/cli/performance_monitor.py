#!/usr/bin/env python3
"""
üìä CLI-—É—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
- –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏, —Ç–æ—á–Ω–æ—Å—Ç–∏, —Ä–µ—Å—É—Ä—Å–æ–≤ —Ä–∞–∑–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
python src/cli/performance_monitor.py --analyzer qwen      # –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Qwen
python src/cli/performance_monitor.py --all                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- Python 3.8+
- src/core/app.py, src/interfaces/analyzer_interface.py
- psutil, statistics

–†–ï–ó–£–õ–¨–¢–ê–¢:
- –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏

–ê–í–¢–û–†: AI Assistant
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025

üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π CLI-–º–æ–Ω–∏—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤

–ù–û–í–´–ï –§–ò–ß–ò:
- pytest-benchmark –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
- py-spy –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
- Prometheus –º–µ—Ç—Ä–∏–∫–∏
- hyperfine CLI —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
- Memory profiling
- OpenTelemetry tracing

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
python enhanced_monitor.py --analyzer qwen --mode benchmark    # –ë–∞–∑–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫
python enhanced_monitor.py --analyzer qwen --mode profile      # –ì–ª—É–±–æ–∫–æ–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
python enhanced_monitor.py --all --mode compare                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö
python enhanced_monitor.py --analyzer qwen --mode load         # –ù–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

–ê–í–¢–û–†: AI Assistant + Human
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025
"""

import argparse
import asyncio
import cProfile
import json
import logging
import pstats
import statistics
import subprocess
import sys
import tempfile
import time
from dataclasses import asdict, dataclass
from datetime import datetime
from io import StringIO
from pathlib import Path
from typing import Any

import psutil

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env
try:
    from dotenv import load_dotenv

    load_dotenv()
    print("‚úÖ .env —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω")
except ImportError:
    print("‚ö†Ô∏è python-dotenv –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. API –∫–ª—é—á–∏ –∏–∑ .env –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.app import create_app

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ñ–∏—á
try:
    from prometheus_client import REGISTRY, Counter, Gauge, Histogram, start_http_server

    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False

try:
    from memory_profiler import profile as memory_profile

    MEMORY_PROFILER_AVAILABLE = True
except ImportError:
    MEMORY_PROFILER_AVAILABLE = False

try:
    import pytest

    PYTEST_AVAILABLE = True
except ImportError:
    PYTEST_AVAILABLE = False

# Rich –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
try:
    from rich import box
    from rich.console import Console
    from rich.layout import Layout
    from rich.live import Live
    from rich.panel import Panel
    from rich.progress import Progress, TaskID, track
    from rich.table import Table
    from rich.text import Text

    RICH_AVAILABLE = True
    console = Console()
except ImportError:
    RICH_AVAILABLE = False
    console = None

# Tabulate –¥–ª—è —Ç–∞–±–ª–∏—Ü
try:
    from tabulate import tabulate

    TABULATE_AVAILABLE = True
except ImportError:
    TABULATE_AVAILABLE = False

# Click –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ CLI
try:
    import click

    CLICK_AVAILABLE = True
except ImportError:
    CLICK_AVAILABLE = False


@dataclass
class EnhancedMetrics:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

    analyzer_name: str
    test_count: int

    # –ë–∞–∑–æ–≤—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    median_time: float

    # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    avg_cpu_percent: float
    avg_memory_mb: float
    peak_memory_mb: float

    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    success_rate: float
    error_count: int
    items_per_second: float

    # –ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    memory_growth_mb: float = 0.0
    cpu_efficiency: float = 0.0  # items per cpu percent
    latency_p95: float = 0.0
    latency_p99: float = 0.0

    # –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    hottest_function: str = ""
    profile_data: dict | None = None

    def to_dict(self) -> dict[str, Any]:
        return asdict(self)


class PytestBenchmarkIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å pytest-benchmark –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞"""

    def __init__(self, monitor: "EnhancedPerformanceMonitor"):
        self.monitor = monitor
        self.available = PYTEST_AVAILABLE

    def generate_benchmark_test(
        self, analyzer_type: str, test_texts: list[str], output_file: str
    ):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è pytest benchmark —Ç–µ—Å—Ç–æ–≤"""
        if not self.available:
            self.monitor.display.print_warning(
                "pytest –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è benchmark –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
            )
            return

        test_content = f'''#!/usr/bin/env python3
"""
üß™ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ pytest-benchmark —Ç–µ—Å—Ç—ã
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {analyzer_type}
–¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã: {len(test_texts)}
–î–∞—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {datetime.now().isoformat()}
"""
import pytest
import asyncio
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ Python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.core.app import create_app

class TestAnalyzerPerformance:
    """Benchmark —Ç–µ—Å—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ {analyzer_type}"""
    
    @pytest.fixture(scope="class")
    def analyzer(self):
        """–§–∏–∫—Å—Ç—É—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        app = create_app()
        return app.get_analyzer("{analyzer_type}")
    
    @pytest.fixture(scope="class") 
    def test_texts(self):
        """–¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã"""
        return {test_texts[:10]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö —Ç–µ—Å—Ç–æ–≤
    
    def test_single_analysis_benchmark(self, benchmark, analyzer, test_texts):
        """Benchmark –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        def run_analysis():
            text = test_texts[0]
            return asyncio.run(analyzer.analyze_song("Test Artist", "Test Song", text))
        
        result = benchmark(run_analysis)
        assert result is not None
        assert hasattr(result, 'confidence')
    
    def test_batch_analysis_benchmark(self, benchmark, analyzer, test_texts):
        """Benchmark –±–∞—Ç—á–µ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        async def batch_analysis():
            results = []
            for i, text in enumerate(test_texts):
                result = await analyzer.analyze_song("Test Artist", f"Test Song {{i}}", text)
                results.append(result)
            return results
        
        def run_batch():
            return asyncio.run(batch_analysis())
        
        results = benchmark(run_batch)
        assert len(results) == len(test_texts)
        assert all(hasattr(r, 'confidence') for r in results)
    
    @pytest.mark.parametrize("text_length", [50, 200, 500, 1000])
    def test_text_length_scaling_benchmark(self, benchmark, analyzer, text_length):
        """Benchmark –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞"""
        test_text = "word " * (text_length // 5)  # –ü—Ä–∏–º–µ—Ä–Ω–æ text_length —Å–∏–º–≤–æ–ª–æ–≤
        
        def run_analysis():
            return asyncio.run(analyzer.analyze_song("Test Artist", "Test Song", test_text))
        
        result = benchmark(run_analysis)
        assert result is not None

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ benchmark —Ç–µ—Å—Ç–æ–≤:
    # pytest {output_file} --benchmark-only --benchmark-sort=mean
    # pytest {output_file} --benchmark-only --benchmark-histogram
    # pytest {output_file} --benchmark-only --benchmark-json=benchmark_results.json
    pass
'''

        try:
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(test_content)

            self.monitor.display.print_success(
                f"Benchmark —Ç–µ—Å—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {output_file}"
            )
            self.monitor.display.print_progress_info(
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ: pytest " + output_file + " --benchmark-only"
            )

        except Exception as e:
            self.monitor.display.print_error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ benchmark —Ç–µ—Å—Ç–æ–≤: {e}")

    def run_benchmark_tests(
        self, test_file: str, json_output: str | None = None
    ) -> dict | None:
        """–ó–∞–ø—É—Å–∫ pytest benchmark —Ç–µ—Å—Ç–æ–≤"""
        if not self.available:
            self.monitor.display.print_warning("pytest –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
            return None

        try:
            import subprocess

            cmd = [
                "python",
                "-m",
                "pytest",
                test_file,
                "--benchmark-only",
                "--benchmark-sort=mean",
            ]

            if json_output:
                cmd.extend(["--benchmark-json", json_output])

            self.monitor.display.print_progress_info(
                f"–ó–∞–ø—É—Å–∫ benchmark —Ç–µ—Å—Ç–æ–≤: {test_file}"
            )

            result = subprocess.run(
                cmd, check=False, capture_output=True, text=True, timeout=300
            )

            if result.returncode == 0:
                self.monitor.display.print_success("Benchmark —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")

                # –ß–∏—Ç–∞–µ–º JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å
                if json_output:
                    try:
                        with open(json_output, encoding="utf-8") as f:
                            return json.load(f)
                    except Exception as e:
                        self.monitor.display.print_warning(
                            f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {e}"
                        )

                return {"status": "success", "output": result.stdout}
            self.monitor.display.print_error(
                f"Benchmark —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å —Å –æ—à–∏–±–∫–æ–π: {result.stderr}"
            )
            return None

        except subprocess.TimeoutExpired:
            self.monitor.display.print_warning("Benchmark —Ç–µ—Å—Ç—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ —Ç–∞–π–º–∞—É—Ç")
            return None
        except Exception as e:
            self.monitor.display.print_error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ benchmark —Ç–µ—Å—Ç–æ–≤: {e}")
            return None

    def compare_benchmark_results(
        self, results1: dict, results2: dict, analyzer1: str, analyzer2: str
    ):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ benchmark —Ç–µ—Å—Ç–æ–≤"""
        if not self.monitor.display.use_rich or not self.monitor.display.console:
            # Fallback –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∞
            print(f"\\nComparison: {analyzer1} vs {analyzer2}")
            return

        table = Table(
            title=f"üìä Benchmark Comparison: {analyzer1} vs {analyzer2}",
            box=box.ROUNDED,
        )
        table.add_column("Test", style="cyan", width=25)
        table.add_column(f"{analyzer1}", style="green", width=15)
        table.add_column(f"{analyzer2}", style="blue", width=15)
        table.add_column("Difference", style="yellow", width=15)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º benchmark –¥–∞–Ω–Ω—ã–µ
        benchmarks1 = results1.get("benchmarks", [])
        benchmarks2 = results2.get("benchmarks", [])

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        bench1_dict = {b["name"]: b for b in benchmarks1}
        bench2_dict = {b["name"]: b for b in benchmarks2}

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ–±—â–∏–µ —Ç–µ—Å—Ç—ã
        common_tests = set(bench1_dict.keys()) & set(bench2_dict.keys())

        for test_name in sorted(common_tests):
            b1 = bench1_dict[test_name]
            b2 = bench2_dict[test_name]

            mean1 = b1["stats"]["mean"]
            mean2 = b2["stats"]["mean"]

            diff_pct = ((mean2 - mean1) / mean1) * 100

            diff_text = f"{diff_pct:+.1f}%"
            if diff_pct < -5:
                diff_style = "bright_green"  # –£–ª—É—á—à–µ–Ω–∏–µ
            elif diff_pct > 5:
                diff_style = "bright_red"  # –£—Ö—É–¥—à–µ–Ω–∏–µ
            else:
                diff_style = "dim"  # –ù–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π

            table.add_row(
                test_name.split("::")[-1],  # –¢–æ–ª—å–∫–æ –∏–º—è —Ç–µ—Å—Ç–∞
                f"{mean1:.3f}s",
                f"{mean2:.3f}s",
                Text(diff_text, style=diff_style),
            )

        self.monitor.display.console.print(table)


class RichDisplayManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å Rich"""

    def __init__(self):
        self.console = console if RICH_AVAILABLE else None
        self.use_rich = RICH_AVAILABLE and console is not None

    def print_header(self, title: str, subtitle: str | None = None):
        """–ö—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
        if self.use_rich and self.console:
            header_text = Text(title, style="bold cyan")
            if subtitle:
                header_text.append(f"\n{subtitle}", style="dim")

            panel = Panel(
                header_text,
                box=box.DOUBLE,
                padding=(1, 2),
                title="üöÄ Enhanced Performance Monitor",
                title_align="center",
            )
            self.console.print(panel)
        else:
            print(f"üöÄ {title}")
            if subtitle:
                print(f"   {subtitle}")
            print("=" * 60)

    def print_analyzer_info(self, analyzer_type: str, info: dict):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ"""
        if self.use_rich and self.console:
            table = Table(
                title=f"üìä Analyzer: {analyzer_type}", show_header=False, box=box.SIMPLE
            )
            table.add_column("Property", style="cyan", width=20)
            table.add_column("Value", style="green")

            table.add_row("Type", analyzer_type)
            table.add_row(
                "Available", "‚úÖ Yes" if info.get("available", True) else "‚ùå No"
            )
            table.add_row("Features", ", ".join(info.get("supported_features", [])))

            self.console.print(table)
        else:
            print(f"üìä Analyzer: {analyzer_type}")
            print(
                f"   Available: {'‚úÖ Yes' if info.get('available', True) else '‚ùå No'}"
            )
            print(f"   Features: {', '.join(info.get('supported_features', []))}")

    def print_metrics_table(self, metrics: EnhancedMetrics):
        """–¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if self.use_rich and self.console:
            table = Table(
                title=f"üìà Performance Metrics: {metrics.analyzer_name}",
                box=box.ROUNDED,
            )
            table.add_column("Metric", style="cyan bold", width=25)
            table.add_column("Value", style="green", width=15)
            table.add_column("Unit", style="dim", width=10)

            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            table.add_row("‚è±Ô∏è Average Time", f"{metrics.avg_time:.3f}", "seconds")
            table.add_row("‚ö° Min Time", f"{metrics.min_time:.3f}", "seconds")
            table.add_row("üî• Max Time", f"{metrics.max_time:.3f}", "seconds")
            table.add_row("üìä Median Time", f"{metrics.median_time:.3f}", "seconds")
            table.add_row("üìà 95th Percentile", f"{metrics.latency_p95:.3f}", "seconds")
            table.add_row("üìà 99th Percentile", f"{metrics.latency_p99:.3f}", "seconds")

            # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            table.add_row("üöÄ Throughput", f"{metrics.items_per_second:.1f}", "items/s")
            table.add_row("‚úÖ Success Rate", f"{metrics.success_rate:.1f}", "%")
            table.add_row("‚ùå Errors", f"{metrics.error_count}", "count")

            # –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
            table.add_row("üíæ Avg Memory", f"{metrics.avg_memory_mb:.1f}", "MB")
            table.add_row("üî∫ Peak Memory", f"{metrics.peak_memory_mb:.1f}", "MB")
            table.add_row("üìà Memory Growth", f"{metrics.memory_growth_mb:.1f}", "MB")
            table.add_row("üñ•Ô∏è Avg CPU", f"{metrics.avg_cpu_percent:.1f}", "%")
            table.add_row(
                "‚ö° CPU Efficiency", f"{metrics.cpu_efficiency:.2f}", "items/cpu%"
            )

            # –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            if metrics.hottest_function:
                table.add_row("üî• Hottest Function", metrics.hottest_function, "")

            self.console.print(table)
        else:
            # Fallback –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∞
            print(f"\nüìä {metrics.analyzer_name.upper()} Enhanced Metrics:")
            print("=" * 50)
            print(f"‚è±Ô∏è  Average time: {metrics.avg_time:.3f}s")
            print(f"üìà 95th percentile: {metrics.latency_p95:.3f}s")
            print(f"üìà 99th percentile: {metrics.latency_p99:.3f}s")
            print(f"üöÄ Throughput: {metrics.items_per_second:.1f} items/s")
            print(f"üíæ Memory growth: {metrics.memory_growth_mb:.1f} MB")
            print(f"‚ö° CPU efficiency: {metrics.cpu_efficiency:.2f} items/cpu%")
            if metrics.hottest_function:
                print(f"üî• Hottest function: {metrics.hottest_function}")

    def print_load_test_results(self, results: dict[str, Any]):
        """–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if self.use_rich and self.console:
            table = Table(
                title=f"üî• Load Test Results: {results['analyzer_type']}", box=box.HEAVY
            )
            table.add_column("Metric", style="red bold", width=20)
            table.add_column("Value", style="bright_yellow", width=15)
            table.add_column("Unit", style="dim", width=10)

            table.add_row(
                "üöÄ Requests/sec", f"{results['requests_per_second']:.1f}", "req/s"
            )
            table.add_row(
                "‚úÖ Success Rate", f"{100 - results['error_rate_percent']:.1f}", "%"
            )
            table.add_row("‚ùå Error Rate", f"{results['error_rate_percent']:.1f}", "%")
            table.add_row(
                "‚è±Ô∏è Avg Response", f"{results['avg_response_time']:.3f}", "seconds"
            )
            table.add_row(
                "üë• Concurrent Users", f"{results['concurrent_users']}", "users"
            )
            table.add_row("üì¶ Total Requests", f"{results['total_requests']}", "count")
            table.add_row("üíæ Memory Usage", f"{results['avg_memory_mb']:.1f}", "MB")
            table.add_row("üñ•Ô∏è CPU Usage", f"{results['avg_cpu_percent']:.1f}", "%")

            self.console.print(table)
        else:
            print(f"\nüî• Load Test Results for {results['analyzer_type']}:")
            print("=" * 50)
            print(f"üöÄ Requests/sec: {results['requests_per_second']:.1f}")
            print(f"‚úÖ Success rate: {100 - results['error_rate_percent']:.1f}%")
            print(f"‚è±Ô∏è  Avg response: {results['avg_response_time']:.3f}s")
            print(f"üíæ Memory usage: {results['avg_memory_mb']:.1f} MB")

    def print_hyperfine_comparison(self, results: dict):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Hyperfine"""
        if self.use_rich and self.console and results:
            table = Table(title="‚ö° Hyperfine Comparison", box=box.SIMPLE_HEAVY)
            table.add_column("Command", style="cyan", width=30)
            table.add_column("Mean Time", style="green", width=12)
            table.add_column("Std Dev", style="yellow", width=12)
            table.add_column("Min", style="blue", width=10)
            table.add_column("Max", style="red", width=10)

            for result in results.get("results", []):
                table.add_row(
                    result["command"][-30:],  # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
                    f"{result['mean']:.3f}s",
                    f"¬±{result['stddev']:.3f}s",
                    f"{result['min']:.3f}s",
                    f"{result['max']:.3f}s",
                )

            self.console.print(table)
        else:
            print("\n‚ö° Hyperfine Comparison:")
            if results:
                for result in results.get("results", []):
                    print(
                        f"  {result['command']}: {result['mean']:.3f}s ¬± {result['stddev']:.3f}s"
                    )

    def print_progress_info(
        self, message: str, current: int | None = None, total: int | None = None
    ):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ"""
        if self.use_rich and self.console:
            if current is not None and total is not None:
                progress_text = f"{message} [{current}/{total}]"
            else:
                progress_text = message

            self.console.print(f"üîÑ {progress_text}", style="bright_blue")
        elif current is not None and total is not None:
            print(f"üîÑ {message} [{current}/{total}]")
        else:
            print(f"üîÑ {message}")

    def print_error(self, message: str):
        """–û—à–∏–±–∫–∞"""
        if self.use_rich and self.console:
            self.console.print(f"‚ùå {message}", style="bright_red")
        else:
            print(f"‚ùå {message}")

    def print_success(self, message: str):
        """–£—Å–ø–µ—Ö"""
        if self.use_rich and self.console:
            self.console.print(f"‚úÖ {message}", style="bright_green")
        else:
            print(f"‚úÖ {message}")

    def print_warning(self, message: str):
        """–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ"""
        if self.use_rich and self.console:
            self.console.print(f"‚ö†Ô∏è  {message}", style="bright_yellow")
        else:
            print(f"‚ö†Ô∏è  {message}")

    def print_analyzer_comparison(self, comparison_results: dict[str, dict]):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        if not comparison_results or len(comparison_results) < 2:
            self.print_warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return

        if self.use_rich and self.console:
            table = Table(title="üî• Analyzer Performance Comparison", box=box.HEAVY)
            table.add_column("Analyzer", style="cyan bold", width=20)
            table.add_column("Avg Time", style="green", width=12)
            table.add_column("Throughput", style="yellow", width=12)
            table.add_column("Success %", style="blue", width=10)
            table.add_column("Memory", style="magenta", width=12)
            table.add_column("CPU Eff", style="red", width=12)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (throughput)
            sorted_results = sorted(
                comparison_results.items(),
                key=lambda x: x[1].get("items_per_second", 0)
                if not x[1].get("error")
                else 0,
                reverse=True,
            )

            for analyzer_name, metrics in sorted_results:
                if metrics.get("error"):
                    table.add_row(analyzer_name, "‚ùå Error", "-", "-", "-", "-")
                else:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∏–ª—å –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    is_fastest = analyzer_name == sorted_results[0][0]
                    name_style = "bright_green bold" if is_fastest else "cyan"

                    table.add_row(
                        Text(analyzer_name, style=name_style),
                        f"{metrics.get('avg_time', 0):.3f}s",
                        f"{metrics.get('items_per_second', 0):.1f}/s",
                        f"{metrics.get('success_rate', 0):.1f}%",
                        f"{metrics.get('avg_memory_mb', 0):.1f}MB",
                        f"{metrics.get('cpu_efficiency', 0):.2f}",
                    )

            self.console.print(table)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            winner = sorted_results[0]
            if not winner[1].get("error"):
                winner_panel = Panel(
                    f"üèÜ Winner: [bold green]{winner[0]}[/bold green]\n"
                    f"‚ö° {winner[1]['items_per_second']:.1f} items/s\n"
                    f"‚è±Ô∏è {winner[1]['avg_time']:.3f}s avg time\n"
                    f"üíæ {winner[1]['avg_memory_mb']:.1f}MB memory",
                    title="Performance Champion",
                    border_style="green",
                )
                self.console.print(winner_panel)
        else:
            # Fallback –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∞
            print("\nüî• Analyzer Performance Comparison:")
            print("=" * 60)

            sorted_results = sorted(
                comparison_results.items(),
                key=lambda x: x[1].get("items_per_second", 0)
                if not x[1].get("error")
                else 0,
                reverse=True,
            )

            for i, (analyzer_name, metrics) in enumerate(sorted_results):
                if metrics.get("error"):
                    print(f"{i + 1}. ‚ùå {analyzer_name}: Error - {metrics['error']}")
                else:
                    trophy = "üèÜ " if i == 0 else f"{i + 1}. "
                    print(f"{trophy}{analyzer_name}:")
                    print(f"   ‚ö° {metrics.get('items_per_second', 0):.1f} items/s")
                    print(f"   ‚è±Ô∏è  {metrics.get('avg_time', 0):.3f}s avg")
                    print(f"   üíæ {metrics.get('avg_memory_mb', 0):.1f}MB")
                    print(f"   ‚úÖ {metrics.get('success_rate', 0):.1f}% success")
                    print()


class PrometheusMetrics:
    """Prometheus –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""

    def __init__(self):
        if not PROMETHEUS_AVAILABLE:
            return

        self.request_counter = Counter(
            "analyzer_requests_total",
            "Total analyzer requests",
            ["analyzer_type", "status"],
        )

        self.request_duration = Histogram(
            "analyzer_request_duration_seconds",
            "Request duration in seconds",
            ["analyzer_type"],
        )

        self.memory_usage = Gauge(
            "analyzer_memory_usage_mb", "Memory usage in MB", ["analyzer_type"]
        )

        self.cpu_usage = Gauge(
            "analyzer_cpu_usage_percent", "CPU usage percentage", ["analyzer_type"]
        )

    def record_request(self, analyzer_type: str, duration: float, success: bool):
        if not PROMETHEUS_AVAILABLE:
            return

        status = "success" if success else "error"
        self.request_counter.labels(analyzer_type=analyzer_type, status=status).inc()
        self.request_duration.labels(analyzer_type=analyzer_type).observe(duration)

    def update_system_metrics(
        self, analyzer_type: str, memory_mb: float, cpu_percent: float
    ):
        if not PROMETHEUS_AVAILABLE:
            return

        self.memory_usage.labels(analyzer_type=analyzer_type).set(memory_mb)
        self.cpu_usage.labels(analyzer_type=analyzer_type).set(cpu_percent)


class EnhancedPerformanceMonitor:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–Ω–∏—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

    def __init__(
        self, monitoring_interval: float = 0.1, enable_prometheus: bool = False
    ):
        self.app = create_app()
        self.monitoring_interval = monitoring_interval
        self.logger = logging.getLogger(__name__)

        # Rich display manager
        self.display = RichDisplayManager()

        # Pytest benchmark –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
        self.pytest_benchmark = PytestBenchmarkIntegration(self)

        # Prometheus –º–µ—Ç—Ä–∏–∫–∏
        self.prometheus_metrics = PrometheusMetrics() if PROMETHEUS_AVAILABLE else None
        if enable_prometheus and PROMETHEUS_AVAILABLE:
            # –ó–∞–ø—É—Å–∫–∞–µ–º Prometheus HTTP —Å–µ—Ä–≤–µ—Ä
            start_http_server(8000)
            self.logger.info("üìä Prometheus metrics server started on :8000")

        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        self.cpu_measurements = []
        self.memory_measurements = []
        self.is_monitoring = False

    async def benchmark_with_profiling(
        self,
        analyzer_type: str,
        test_texts: list[str],
        enable_profiling: bool = True,
        enable_memory_profiling: bool = True,
        timeout_per_text: float = 30.0,  # –¢–∞–π–º–∞—É—Ç –Ω–∞ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
    ) -> EnhancedMetrics:
        """–ë–µ–Ω—á–º–∞—Ä–∫ —Å –≥–ª—É–±–æ–∫–∏–º –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""

        analyzer = self.app.get_analyzer(analyzer_type)
        if not analyzer:
            available = self.app.list_analyzers()
            raise ValueError(
                f"Unknown analyzer type: {analyzer_type}. Available: {available}"
            )

        self.logger.info(f"üî¨ Enhanced benchmarking {analyzer_type} with profiling")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä–∞
        profiler = cProfile.Profile() if enable_profiling else None

        # –°–±—Ä–æ—Å –º–µ—Ç—Ä–∏–∫
        self.cpu_measurements.clear()
        self.memory_measurements.clear()

        # –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        monitoring_task = asyncio.create_task(self._monitor_system_resources())

        # –û—Å–Ω–æ–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        execution_times = []
        error_count = 0
        memory_start = psutil.Process().memory_info().rss / 1024 / 1024

        if profiler:
            profiler.enable()

        start_time = time.time()

        for i, text in enumerate(test_texts):
            text_start_time = time.time()
            success = False

            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–µ—Ç–æ–¥ async
                import functools
                import inspect

                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
                if inspect.iscoroutinefunction(analyzer.analyze_song):
                    result = await asyncio.wait_for(
                        analyzer.analyze_song("Unknown", f"Test_{i}", text),
                        timeout=timeout_per_text,
                    )
                else:
                    # –î–ª—è sync –º–µ—Ç–æ–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º run_in_executor —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º functools.partial –≤–º–µ—Å—Ç–æ lambda –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
                    loop = asyncio.get_event_loop()
                    sync_call = functools.partial(
                        analyzer.analyze_song, "Unknown", f"Test_{i}", text
                    )
                    result = await asyncio.wait_for(
                        loop.run_in_executor(None, sync_call), timeout=timeout_per_text
                    )

                text_time = time.time() - text_start_time
                execution_times.append(text_time)
                success = True

                # Prometheus –º–µ—Ç—Ä–∏–∫–∏
                if self.prometheus_metrics:
                    self.prometheus_metrics.record_request(
                        analyzer_type, text_time, True
                    )

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
                if text_time > 5.0:
                    self.display.print_progress_info(
                        f"Text {i + 1}/{len(test_texts)} completed",
                        current=i + 1,
                        total=len(test_texts),
                    )

            except asyncio.TimeoutError:
                error_count += 1
                text_time = time.time() - text_start_time
                self.logger.warning(
                    f"‚è∞ Timeout processing text {i + 1} after {timeout_per_text}s"
                )

                if self.prometheus_metrics:
                    self.prometheus_metrics.record_request(
                        analyzer_type, text_time, False
                    )

            except Exception as e:
                error_count += 1
                text_time = time.time() - text_start_time
                self.logger.warning(f"‚ùå Error processing text {i + 1}: {e}")

                if self.prometheus_metrics:
                    self.prometheus_metrics.record_request(
                        analyzer_type, text_time, False
                    )

        total_time = time.time() - start_time

        if profiler:
            profiler.disable()

        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        self.is_monitoring = False
        try:
            await monitoring_task
        except asyncio.CancelledError:
            pass

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        success_count = len(execution_times)
        memory_end = psutil.Process().memory_info().rss / 1024 / 1024
        memory_growth = memory_end - memory_start

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if execution_times:
            avg_time = statistics.mean(execution_times)
            min_time = min(execution_times)
            max_time = max(execution_times)
            median_time = statistics.median(execution_times)
            # –ü—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏
            sorted_times = sorted(execution_times)
            p95_idx = int(len(sorted_times) * 0.95)
            p99_idx = int(len(sorted_times) * 0.99)
            latency_p95 = (
                sorted_times[p95_idx] if p95_idx < len(sorted_times) else max_time
            )
            latency_p99 = (
                sorted_times[p99_idx] if p99_idx < len(sorted_times) else max_time
            )
        else:
            avg_time = min_time = max_time = median_time = 0
            latency_p95 = latency_p99 = 0

        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        avg_cpu = statistics.mean(self.cpu_measurements) if self.cpu_measurements else 0
        avg_memory = (
            statistics.mean(self.memory_measurements) if self.memory_measurements else 0
        )
        peak_memory = max(self.memory_measurements) if self.memory_measurements else 0

        # CPU —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        cpu_efficiency = (success_count / avg_cpu) if avg_cpu > 0 else 0

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        hottest_function = ""
        profile_data = None

        if profiler:
            s = StringIO()
            ps = pstats.Stats(profiler, stream=s)
            ps.sort_stats("cumulative")
            ps.print_stats(10)  # –¢–æ–ø 10 —Ñ—É–Ω–∫—Ü–∏–π

            profile_output = s.getvalue()
            lines = profile_output.split("\n")
            for line in lines:
                if (
                    "function calls" not in line
                    and line.strip()
                    and not line.startswith("Ordered by")
                ):
                    parts = line.split()
                    if len(parts) > 5:
                        hottest_function = parts[-1]
                        break

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            profile_data = {
                "total_calls": getattr(ps, "total_calls", 0),
                "total_time": getattr(ps, "total_tt", 0.0),
                "profile_summary": profile_output[:1000],  # –ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤
            }

        # –û–±–Ω–æ–≤–ª—è–µ–º Prometheus –º–µ—Ç—Ä–∏–∫–∏
        if self.prometheus_metrics:
            self.prometheus_metrics.update_system_metrics(
                analyzer_type, avg_memory, avg_cpu
            )

        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics = EnhancedMetrics(
            analyzer_name=analyzer_type,
            test_count=len(test_texts),
            total_time=total_time,
            avg_time=avg_time,
            min_time=min_time,
            max_time=max_time,
            median_time=median_time,
            avg_cpu_percent=avg_cpu,
            avg_memory_mb=avg_memory,
            peak_memory_mb=peak_memory,
            success_rate=(success_count / len(test_texts) * 100) if test_texts else 0,
            error_count=error_count,
            items_per_second=(success_count / total_time) if total_time > 0 else 0,
            memory_growth_mb=memory_growth,
            cpu_efficiency=cpu_efficiency,
            latency_p95=latency_p95,
            latency_p99=latency_p99,
            hottest_function=hottest_function,
            profile_data=profile_data,
        )

        self.logger.info("‚úÖ Enhanced benchmarking completed")
        return metrics

    async def py_spy_analysis(
        self, analyzer_type: str, test_texts: list[str], duration: int = 30
    ) -> str | None:
        """–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å py-spy"""

        if not test_texts:
            return None

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è py-spy
        script_content = f'''
import asyncio
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from src.core.app import create_app

async def run_analyzer():
    app = create_app()
    analyzer = app.get_analyzer("{analyzer_type}")
    texts = {test_texts[:10]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è py-spy
    
    for i in range(100):  # –ú–Ω–æ–≥–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è py-spy
        for j, text in enumerate(texts):
            try:
                import inspect
                if inspect.iscoroutinefunction(analyzer.analyze_song):
                    await analyzer.analyze_song("Unknown", f"Text_{{i}}_{{j}}", text)
                else:
                    analyzer.analyze_song("Unknown", f"Text_{{i}}_{{j}}", text)
            except Exception:
                pass

if __name__ == "__main__":
    asyncio.run(run_analyzer())
'''

        with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
            f.write(script_content)
            temp_script = f.name

        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º py-spy
            output_file = f"pyspy_profile_{analyzer_type}.svg"
            cmd = [
                "py-spy",
                "record",
                "-d",
                str(duration),
                "-o",
                output_file,
                "--",
                "python",
                temp_script,
            ]

            self.logger.info(f"üîç Running py-spy profiling for {duration}s...")

            try:
                result = subprocess.run(
                    cmd,
                    check=False,
                    capture_output=True,
                    text=True,
                    timeout=duration + 10,
                )
                if result.returncode == 0:
                    self.logger.info(f"üìä py-spy profile saved to: {output_file}")
                    return output_file
                self.logger.warning(f"py-spy failed: {result.stderr}")
                return None
            except subprocess.TimeoutExpired:
                self.logger.warning("py-spy timed out")
                return None
            except FileNotFoundError:
                self.logger.warning(
                    "py-spy not found. Install with: pip install py-spy"
                )
                return None

        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            Path(temp_script).unlink(missing_ok=True)

        return None

    def hyperfine_comparison(
        self, analyzer_types: list[str], test_text: str = "Test text for hyperfine"
    ) -> dict | None:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å hyperfine"""

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        temp_scripts = []
        commands = []

        for analyzer_type in analyzer_types:
            script_content = f'''
import asyncio
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from src.core.app import create_app

async def main():
    app = create_app()
    analyzer = app.get_analyzer("{analyzer_type}")
    text = "{test_text}"
    
    try:
        import inspect
        if inspect.iscoroutinefunction(analyzer.analyze_song):
            await analyzer.analyze_song("Unknown", "Test", text)
        else:
            analyzer.analyze_song("Unknown", "Test", text)
    except Exception as e:
        print(f"Error: {{e}}")

if __name__ == "__main__":
    asyncio.run(main())
'''

            with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
                f.write(script_content)
                temp_scripts.append(f.name)
                commands.append(f"python {f.name}")

        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º hyperfine
            cmd = ["hyperfine", "--export-json", "hyperfine_results.json"] + commands

            self.logger.info("üèÉ Running hyperfine comparison...")

            try:
                result = subprocess.run(
                    cmd, check=False, capture_output=True, text=True, timeout=120
                )
                if result.returncode == 0:
                    # –ß–∏—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    try:
                        with open("hyperfine_results.json") as f:
                            hyperfine_data = json.load(f)

                        self.logger.info("‚ö° Hyperfine comparison completed")
                        return hyperfine_data
                    except FileNotFoundError:
                        self.logger.warning("Hyperfine results file not found")
                        return None
                else:
                    self.logger.warning(f"Hyperfine failed: {result.stderr}")
                    return None
            except subprocess.TimeoutExpired:
                self.logger.warning("Hyperfine timed out")
                return None
            except FileNotFoundError:
                self.logger.warning(
                    "Hyperfine not found. Install with: pip install hyperfine"
                )
                return None

        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            for script in temp_scripts:
                Path(script).unlink(missing_ok=True)

        return None

    async def load_test(
        self,
        analyzer_type: str,
        test_texts: list[str],
        concurrent_users: int = 10,
        duration_seconds: int = 60,
    ) -> dict[str, Any]:
        """–ù–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""

        analyzer = self.app.get_analyzer(analyzer_type)
        if not analyzer:
            raise ValueError(f"Unknown analyzer type: {analyzer_type}")

        self.logger.info(
            f"üî• Load testing {analyzer_type} with {concurrent_users} concurrent users for {duration_seconds}s"
        )

        # –°—á–µ—Ç—á–∏–∫–∏
        successful_requests = 0
        failed_requests = 0
        response_times = []
        start_time = time.time()

        async def worker(worker_id: int):
            nonlocal successful_requests, failed_requests, response_times

            while time.time() - start_time < duration_seconds:
                text = test_texts[worker_id % len(test_texts)]
                request_start = time.time()

                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–µ—Ç–æ–¥ async
                    import inspect

                    if inspect.iscoroutinefunction(analyzer.analyze_song):
                        result = await analyzer.analyze_song(
                            "Unknown", f"Worker_{worker_id}", text
                        )
                    else:
                        result = analyzer.analyze_song(
                            "Unknown", f"Worker_{worker_id}", text
                        )

                    response_time = time.time() - request_start
                    response_times.append(response_time)
                    successful_requests += 1

                except Exception:
                    failed_requests += 1

                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(0.01)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–æ—Ä–∫–µ—Ä—ã
        tasks = [asyncio.create_task(worker(i)) for i in range(concurrent_users)]

        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
        monitoring_task = asyncio.create_task(self._monitor_system_resources())

        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        await asyncio.gather(*tasks, return_exceptions=True)

        self.is_monitoring = False
        try:
            await monitoring_task
        except asyncio.CancelledError:
            pass

        total_time = time.time() - start_time
        total_requests = successful_requests + failed_requests

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        avg_response_time = statistics.mean(response_times) if response_times else 0
        rps = successful_requests / total_time if total_time > 0 else 0
        error_rate = (
            (failed_requests / total_requests * 100) if total_requests > 0 else 0
        )

        avg_cpu = statistics.mean(self.cpu_measurements) if self.cpu_measurements else 0
        avg_memory = (
            statistics.mean(self.memory_measurements) if self.memory_measurements else 0
        )

        load_test_results = {
            "analyzer_type": analyzer_type,
            "duration_seconds": total_time,
            "concurrent_users": concurrent_users,
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "failed_requests": failed_requests,
            "requests_per_second": rps,
            "error_rate_percent": error_rate,
            "avg_response_time": avg_response_time,
            "avg_cpu_percent": avg_cpu,
            "avg_memory_mb": avg_memory,
        }

        self.logger.info(
            f"‚úÖ Load test completed: {rps:.1f} RPS, {error_rate:.1f}% errors"
        )
        return load_test_results

    async def _monitor_system_resources(self) -> None:
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.is_monitoring = True
        process = psutil.Process()

        while self.is_monitoring:
            try:
                cpu_percent = process.cpu_percent()
                memory_info = process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024

                self.cpu_measurements.append(cpu_percent)
                self.memory_measurements.append(memory_mb)

                await asyncio.sleep(self.monitoring_interval)

            except asyncio.CancelledError:
                break
            except Exception:
                pass

        self.is_monitoring = False


def generate_test_texts(count: int = 50) -> list[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
    base_texts = [
        "Happy song",
        "Sad lyrics about lost love and broken dreams",
        "Energetic rap with political messages and social commentary",
        "Calm meditation on nature and human existence in modern world",
        """This is a comprehensive analysis of modern culture and its impact 
        on society, exploring themes of justice, growth, and artistic expression 
        through complex metaphors and intricate wordplay that challenges thinking""",
    ]

    texts = []
    for i in range(count):
        base_text = base_texts[i % len(base_texts)]
        variation = (
            f" (test variation {i // len(base_texts) + 1})"
            if i >= len(base_texts)
            else ""
        )
        texts.append(base_text + variation)

    return texts


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å CLI –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏"""
    parser = argparse.ArgumentParser(description="üöÄ Enhanced Performance Monitor")
    parser.add_argument("--analyzer", type=str, help="Analyzer type to test")
    parser.add_argument(
        "--all", action="store_true", help="Test all available analyzers"
    )
    parser.add_argument(
        "--mode",
        choices=["benchmark", "profile", "compare", "load", "pyspy", "pytest"],
        default="benchmark",
        help="Testing mode",
    )
    parser.add_argument(
        "--prometheus", action="store_true", help="Enable Prometheus metrics"
    )
    parser.add_argument("--output", type=str, help="Output file for results")
    parser.add_argument("--texts", type=int, default=20, help="Number of test texts")
    parser.add_argument(
        "--duration", type=int, default=30, help="Duration for py-spy profiling"
    )
    parser.add_argument(
        "--users", type=int, default=10, help="Concurrent users for load test"
    )
    parser.add_argument(
        "--timeout",
        type=float,
        default=30.0,
        help="Timeout per text analysis (seconds)",
    )

    args = parser.parse_args()

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    monitor = EnhancedPerformanceMonitor(enable_prometheus=args.prometheus)
    test_texts = generate_test_texts(args.texts)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º rich –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
    monitor.display.print_header(
        "Enhanced Performance Monitor",
        f"Mode: {args.mode} | Dataset: {len(test_texts)} texts",
    )

    try:
        if args.mode == "benchmark":
            analyzer_type = args.analyzer or "advanced_algorithmic"

            monitor.display.print_progress_info(
                f"Starting benchmark for {analyzer_type}"
            )

            metrics = await monitor.benchmark_with_profiling(
                analyzer_type,
                test_texts,
                enable_profiling=True,
                timeout_per_text=args.timeout,
            )

            # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
            monitor.display.print_metrics_table(metrics)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if args.output:
                with open(args.output, "w", encoding="utf-8") as f:
                    json.dump(metrics.to_dict(), f, indent=2, ensure_ascii=False)
                monitor.display.print_success(f"Results saved to: {args.output}")

        elif args.mode == "pyspy":
            analyzer_type = args.analyzer or "advanced_algorithmic"

            monitor.display.print_progress_info(
                f"Starting py-spy profiling for {analyzer_type}"
            )

            profile_file = await monitor.py_spy_analysis(
                analyzer_type, test_texts, args.duration
            )
            if profile_file:
                monitor.display.print_success(
                    f"py-spy profile saved to: {profile_file}"
                )
            else:
                monitor.display.print_warning(
                    "py-spy profiling failed or not available"
                )

        elif args.mode == "load":
            analyzer_type = args.analyzer or "advanced_algorithmic"

            monitor.display.print_progress_info(
                f"Starting load test for {analyzer_type}"
            )

            results = await monitor.load_test(analyzer_type, test_texts, args.users, 60)

            # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            monitor.display.print_load_test_results(results)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if args.output:
                with open(args.output, "w", encoding="utf-8") as f:
                    json.dump(results, f, indent=2, ensure_ascii=False)
                monitor.display.print_success(
                    f"Load test results saved to: {args.output}"
                )

        elif args.mode == "compare":
            analyzers = (
                ["advanced_algorithmic"]
                if not args.all
                else ["advanced_algorithmic", "qwen", "emotion_analyzer"]
            )

            monitor.display.print_progress_info(
                f"Starting comparison of {len(analyzers)} analyzers"
            )

            # –í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
            comparison_results = {}

            for analyzer_type in analyzers:
                try:
                    monitor.display.print_progress_info(f"Benchmarking {analyzer_type}")

                    # –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–∞–π–º–∞—É—Ç
                    timeout = 10.0 if analyzer_type == "qwen" else 5.0

                    metrics = await monitor.benchmark_with_profiling(
                        analyzer_type,
                        test_texts[:5],
                        enable_profiling=False,  # –ë—ã—Å—Ç—Ä–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
                        timeout_per_text=timeout,
                    )

                    comparison_results[analyzer_type] = metrics.to_dict()

                except Exception as e:
                    monitor.display.print_warning(
                        f"Failed to benchmark {analyzer_type}: {e}"
                    )
                    comparison_results[analyzer_type] = {"error": str(e)}

            # –ö—Ä–∞—Å–∏–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            if len(comparison_results) > 1:
                monitor.display.print_analyzer_comparison(comparison_results)

            # Hyperfine —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            hyperfine_results = monitor.hyperfine_comparison(analyzers)
            if hyperfine_results:
                monitor.display.print_hyperfine_comparison(hyperfine_results)
            else:
                monitor.display.print_warning(
                    "Hyperfine comparison failed or not available"
                )
                monitor.display.print_success(
                    "‚ú® Internal comparison completed! Install hyperfine for external benchmarks:"
                )

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ
                if monitor.display.use_rich and monitor.display.console:
                    install_panel = Panel(
                        "üöÄ To install Hyperfine:\n\n"
                        "[cyan]Windows:[/cyan]\n"
                        "‚Ä¢ choco install hyperfine\n"
                        "‚Ä¢ scoop install hyperfine\n"
                        "‚Ä¢ Download from GitHub releases\n\n"
                        "[cyan]Linux/Mac:[/cyan]\n"
                        "‚Ä¢ cargo install hyperfine\n"
                        "‚Ä¢ brew install hyperfine",
                        title="üì¶ Hyperfine Installation",
                        border_style="yellow",
                    )
                    monitor.display.console.print(install_panel)
                else:
                    print("\nüì¶ To install Hyperfine:")
                    print("  Windows: choco install hyperfine")
                    print("  Linux:   cargo install hyperfine")
                    print("  Mac:     brew install hyperfine")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if args.output:
                full_results = {
                    "internal_comparison": comparison_results,
                    "hyperfine_results": hyperfine_results,
                }
                with open(args.output, "w", encoding="utf-8") as f:
                    json.dump(full_results, f, indent=2, ensure_ascii=False)
                monitor.display.print_success(
                    f"Comparison results saved to: {args.output}"
                )

        elif args.mode == "pytest":
            analyzer_type = args.analyzer or "advanced_algorithmic"

            monitor.display.print_progress_info(
                f"Generating pytest benchmark tests for {analyzer_type}"
            )

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º benchmark —Ç–µ—Å—Ç—ã
            test_file = f"test_benchmark_{analyzer_type}.py"
            monitor.pytest_benchmark.generate_benchmark_test(
                analyzer_type, test_texts, test_file
            )

            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
            if args.output:
                json_output = args.output.replace(".json", "_benchmark.json")
                results = monitor.pytest_benchmark.run_benchmark_tests(
                    test_file, json_output
                )

                if results:
                    monitor.display.print_success(
                        f"Pytest benchmark —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {json_output}"
                    )

        monitor.display.print_success("Enhanced monitoring completed successfully!")

    except Exception as e:
        monitor.display.print_error(f"Enhanced monitoring failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
