#!/usr/bin/env python3
"""
ü§ñ AI Context Manager PRO ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å ML –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è–º–∏

–ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
üéØ –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á —Å AI-powered –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏
üìä Git-based –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –∫–æ–º–º–∏—Ç–æ–≤ –∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
üß† ML —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
üîÑ –£–º–Ω–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–æ–≤
üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å ai_project_analyzer –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞
üìà –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥—É–ª–µ–π

–ù–û–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò PRO:
‚ú® Git-based –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è - —Ñ–∞–π–ª—ã —Å —á–∞—Å—Ç—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –ø–æ–ª—É—á–∞—é—Ç –≤—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ - –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
ü§ñ –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏ - –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –≤—ã–±–∏—Ä–∞–µ—Ç debug/develop/analyze/refactor
üíæ –£–º–Ω–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ - MD5 —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ + –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
üìä Enterprise –º–µ—Ç—Ä–∏–∫–∏ - —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ–¥–∞, —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å –º–æ–¥—É–ª–µ–π, git —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
üîó Project Analyzer –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è - –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–¥–∞, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è
üé® –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π CLI - —É–¥–æ–±–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤–º–µ—Å—Ç–æ —Ç–æ–ª—å–∫–æ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
python scripts/tools/ai_context_manager.py --interactive

# –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
python scripts/tools/ai_context_manager.py --query "fix database connection error"

# –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
python scripts/tools/ai_context_manager.py --semantic-search "analyzer performance"

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ —Å ML –º–µ—Ç—Ä–∏–∫–∞–º–∏
python scripts/tools/ai_context_manager.py --stats

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Project Analyzer
python scripts/tools/ai_context_manager.py --integrate

–ó–ê–í–ò–°–ò–ú–û–°–¢–ò:
- Python 3.8+
- dataclasses, pathlib, ast, subprocess (–±–∞–∑–æ–≤—ã–µ)
- scikit-learn, numpy (–¥–ª—è ML —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞)
- pyperclip (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –±—É—Ñ–µ—Ä)

–†–ï–ó–£–õ–¨–¢–ê–¢:
üéØ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å ML –∏–Ω—Å–∞–π—Ç–∞–º–∏
‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ legacy –∫–æ–¥–µ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Ä–∏—Å–∫–∞—Ö
üìä –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ git –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–¥–æ–≤–æ–π –±–∞–∑–µ
üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞

–≠–í–û–õ–Æ–¶–ò–Ø –û–¢ –ë–ê–ó–û–í–û–ô –í–ï–†–°–ò–ò:
üîÑ –ë—ã–ª–æ: –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã (int 1-5) ‚Üí –°—Ç–∞–ª–æ: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ (float 0-5.0)
üîÑ –ë—ã–ª–æ: –ü—Ä–æ—Å—Ç–æ–π grep –ø–æ–∏—Å–∫ ‚Üí –°—Ç–∞–ª–æ: ML —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å TF-IDF
üîÑ –ë—ã–ª–æ: –†—É—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è ‚Üí –°—Ç–∞–ª–æ: Git-based –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è
üîÑ –ë—ã–ª–æ: –ë–∞–∑–æ–≤—ã–π CLI ‚Üí –°—Ç–∞–ª–æ: –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º + –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á
üîÑ –ë—ã–ª–æ: –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç ‚Üí –°—Ç–∞–ª–æ: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Project Analyzer

–ê–í–¢–û–†: Vastargazing
–î–ê–¢–ê: –°–µ–Ω—Ç—è–±—Ä—å 2025 (PRO upgrade)
–í–ï–†–°–ò–Ø: 2.0 PRO
"""

import ast
import hashlib
import json
import pickle
import subprocess
from collections import defaultdict
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta
from pathlib import Path

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è ML features
try:
    import numpy as np
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("‚ö†Ô∏è sklearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –±—É–¥–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º")

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è advanced features
try:
    import httpx

    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

try:
    import uvicorn
    from fastapi import FastAPI, HTTPException, Query
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel

    API_AVAILABLE = True
except ImportError:
    API_AVAILABLE = False


@dataclass
class EnhancedFileContext:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ñ–∞–π–ª–∞ —Å ML –º–µ—Ç—Ä–∏–∫–∞–º–∏"""

    path: str
    priority: float  # –¢–µ–ø–µ—Ä—å float –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
    category: str
    description: str
    last_modified: str
    size_lines: int
    dependencies: list[str]

    # –ù–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è ML
    git_commits_count: int = 0
    git_last_commit: str = ""
    git_authors: list[str] = field(default_factory=list)
    complexity_score: float = 0.0
    coupling_score: float = 0.0  # –°–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥—É–ª—è–º–∏
    usage_frequency: int = 0  # –ö–∞–∫ —á–∞—Å—Ç–æ —Ñ–∞–π–ª –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø—Ä–æ–µ–∫—Ç–µ
    semantic_embedding: list[float] | None = None  # –î–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞


@dataclass
class ContextCache:
    """–ö–µ—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã"""

    file_hashes: dict[str, str] = field(default_factory=dict)
    embeddings: dict[str, list[float]] = field(default_factory=dict)
    git_data: dict[str, dict] = field(default_factory=dict)
    last_update: float = 0


class GitAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä git –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏–∏"""

    def __init__(self, repo_path: Path):
        self.repo_path = repo_path

    def get_file_stats(self, file_path: str) -> dict:
        """–ü–æ–ª—É—á–∞–µ—Ç git —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è —Ñ–∞–π–ª–∞"""
        try:
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–º–∏—Ç–æ–≤
            commits = (
                subprocess.run(
                    ["git", "log", "--oneline", file_path],
                    check=False,
                    capture_output=True,
                    text=True,
                    cwd=self.repo_path,
                )
                .stdout.strip()
                .split("\n")
            )
            commit_count = len([c for c in commits if c])

            # –ü–æ—Å–ª–µ–¥–Ω–∏–π –∫–æ–º–º–∏—Ç
            last_commit = subprocess.run(
                ["git", "log", "-1", "--format=%ar", file_path],
                check=False,
                capture_output=True,
                text=True,
                cwd=self.repo_path,
            ).stdout.strip()

            # –ê–≤—Ç–æ—Ä—ã
            authors = (
                subprocess.run(
                    ["git", "log", "--format=%an", file_path],
                    check=False,
                    capture_output=True,
                    text=True,
                    cwd=self.repo_path,
                )
                .stdout.strip()
                .split("\n")
            )
            unique_authors = list(set([a for a in authors if a]))

            return {
                "commits": commit_count,
                "last_commit": last_commit,
                "authors": unique_authors,
            }
        except Exception:
            return {"commits": 0, "last_commit": "unknown", "authors": []}

    def get_recent_changes(self, days: int = 7) -> list[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–µ–¥–∞–≤–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            since = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
            result = subprocess.run(
                ["git", "log", f"--since={since}", "--name-only", "--format="],
                check=False,
                capture_output=True,
                text=True,
                cwd=self.repo_path,
            )
            files = [
                f for f in result.stdout.strip().split("\n") if f and f.endswith(".py")
            ]
            return list(set(files))
        except:
            return []


class SemanticSearchEngine:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–¥—É"""

    def __init__(self):
        self.vectorizer = None
        self.file_contents = {}
        self.embeddings = None

    def build_index(self, files: dict[str, EnhancedFileContext]) -> None:
        """–°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        if not ML_AVAILABLE:
            return

        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Ñ–∞–π–ª–æ–≤
        contents = []
        file_paths = []

        for path, context in files.items():
            if Path(path).exists():
                try:
                    with open(path, encoding="utf-8") as f:
                        content = f.read()
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º docstrings –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
                        content = self._extract_semantic_content(content)
                        contents.append(content)
                        file_paths.append(path)
                        self.file_contents[path] = content
                except:
                    pass

        if contents:
            # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä—ã
            self.vectorizer = TfidfVectorizer(
                max_features=500, stop_words="english", ngram_range=(1, 2)
            )
            self.embeddings = self.vectorizer.fit_transform(contents)
            self.file_paths = file_paths

    def search(self, query: str, top_k: int = 10) -> list[tuple[str, float]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not ML_AVAILABLE or self.vectorizer is None:
            return []

        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_vec = self.vectorizer.transform([query])

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        similarities = cosine_similarity(query_vec, self.embeddings).flatten()

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        top_indices = similarities.argsort()[-top_k:][::-1]

        results = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                results.append((self.file_paths[idx], float(similarities[idx])))

        return results

    def _extract_semantic_content(self, code: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –∫–æ–¥–∞"""
        try:
            tree = ast.parse(code)
            semantic_parts = []

            for node in ast.walk(tree):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º docstrings
                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                    docstring = ast.get_docstring(node)
                    if docstring:
                        semantic_parts.append(docstring)
                    semantic_parts.append(node.name)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                for line in code.split("\n"):
                    if "#" in line:
                        comment = line.split("#")[1].strip()
                        if len(comment) > 10:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                            semantic_parts.append(comment)

            return " ".join(semantic_parts)
        except:
            return code[:1000]  # Fallback –∫ –ø–µ—Ä–≤—ã–º 1000 —Å–∏–º–≤–æ–ª–∞–º


class DynamicPrioritizer:
    """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤"""

    def __init__(self, git_analyzer: GitAnalyzer):
        self.git_analyzer = git_analyzer
        self.usage_patterns = defaultdict(int)

    def calculate_priority(self, context: EnhancedFileContext) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–∞–π–ª–∞"""

        base_priority = self._get_base_priority(context.category)

        # –§–∞–∫—Ç–æ—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        factors = []

        # 1. –ß–∞—Å—Ç–æ—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π (git commits)
        if context.git_commits_count > 0:
            commit_factor = min(context.git_commits_count / 50, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 1
            factors.append(commit_factor * 0.3)

        # 2. –ù–µ–¥–∞–≤–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–π
        if "hour" in context.git_last_commit or "minute" in context.git_last_commit:
            factors.append(0.5)  # –û—á–µ–Ω—å –Ω–µ–¥–∞–≤–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        elif "day" in context.git_last_commit:
            factors.append(0.3)
        elif "week" in context.git_last_commit:
            factors.append(0.1)

        # 3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–≤—Ç–æ—Ä–æ–≤ (–ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞)
        if len(context.git_authors) > 1:
            author_factor = min(len(context.git_authors) / 5, 1.0)
            factors.append(author_factor * 0.2)

        # 4. –°–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ–¥–∞
        if context.complexity_score > 0:
            complexity_factor = min(context.complexity_score / 100, 1.0)
            factors.append(complexity_factor * 0.2)

        # 5. –°–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥—É–ª—è–º–∏
        if context.coupling_score > 0:
            coupling_factor = min(context.coupling_score / 10, 1.0)
            factors.append(coupling_factor * 0.3)

        # 6. –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (–±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã —á–∞—Å—Ç–æ –≤–∞–∂–Ω–µ–µ)
        if context.size_lines > 200:
            size_factor = min(context.size_lines / 1000, 1.0)
            factors.append(size_factor * 0.1)

        # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        dynamic_boost = sum(factors)
        final_priority = base_priority + dynamic_boost

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        return min(final_priority, 5.0)

    def _get_base_priority(self, category: str) -> float:
        """–ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        priorities = {
            "database": 4.0,
            "cli": 4.0,
            "analyzer": 3.5,
            "config": 3.5,
            "models": 3.0,
            "docs": 3.0,
            "tests": 2.5,
            "scripts": 2.0,
            "legacy": 1.0,
        }
        return priorities.get(category, 2.5)


class LLMDescriptionGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —É–º–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ LLM"""

    def __init__(self, provider="ollama", model="codellama"):
        self.provider = provider
        self.model = model
        self.cache_dir = Path("results/.llm_cache")
        self.cache_dir.mkdir(exist_ok=True)

    async def generate_file_description(self, file_path: Path) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–º–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ LLM"""

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        cache_key = hashlib.md5(str(file_path).encode()).hexdigest()
        cache_file = self.cache_dir / f"{cache_key}.txt"

        if cache_file.exists():
            return cache_file.read_text()

        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()[:2000]  # –ü–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤

            description = await self._generate_description(file_path.name, content)

            # –ö–µ—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            cache_file.write_text(description)
            return description

        except Exception:
            return self._fallback_description(file_path, "")

    async def _generate_description(self, filename: str, content: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –¥–æ—Å—Ç—É–ø–Ω—ã–π LLM"""

        prompt = f"""Analyze this Python file and provide a concise description (max 100 chars):
Filename: {filename}
Content preview:
{content[:500]}

Describe the main purpose and key functionality:"""

        if self.provider == "ollama" and HTTPX_AVAILABLE:
            return await self._call_ollama(prompt)
        return self._fallback_description(Path(filename), content)

    async def _call_ollama(self, prompt: str) -> str:
        """–í—ã–∑–æ–≤ Ollama API"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {"temperature": 0.1},
                    },
                    timeout=30,
                )
                if response.status_code == 200:
                    result = response.json()["response"]
                    return result.strip()[:100]
        except Exception as e:
            print(f"‚ö†Ô∏è Ollama error: {e}")

        return "AI-generated description unavailable"

    def _fallback_description(self, file_path: Path, content: str) -> str:
        """Fallback –æ–ø–∏—Å–∞–Ω–∏–µ –±–µ–∑ LLM"""
        if "class" in content and "def" in content:
            return f"Python module with classes and functions: {file_path.name}"
        if "class" in content:
            return f"Class definitions in {file_path.name}"
        if "def" in content:
            return f"Function definitions in {file_path.name}"
        if "import" in content:
            return f"Python module with imports: {file_path.name}"
        return f"Python file: {file_path.name}"

    def generate_descriptions(self, file_contexts: dict) -> dict[str, str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"""
        descriptions = {}
        print(f"ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º AI –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è {len(file_contexts)} —Ñ–∞–π–ª–æ–≤...")

        for file_path, context in file_contexts.items():
            try:
                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –Ω—É–∂–Ω–æ –æ–±–µ—Ä–Ω—É—Ç—å
                import asyncio

                description = asyncio.run(
                    self.generate_file_description(Path(file_path))
                )
                descriptions[file_path] = description
                print(f"‚úÖ {file_path}: {description[:50]}...")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {file_path}: {e}")
                descriptions[file_path] = self._fallback_description(
                    Path(file_path), context.content[:500]
                )

        return descriptions


class DependencyVisualizer:
    """–ü—Ä–æ—Å—Ç–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""

    def __init__(self, file_contexts: dict):
        self.file_contexts = file_contexts
        self.output_dir = Path("results/visualizations")
        self.output_dir.mkdir(exist_ok=True)

    def generate_dependency_graph(self, focus_files: list[str] | None = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ DOT —Ñ–æ—Ä–º–∞—Ç–µ"""

        dot_lines = [
            "digraph Dependencies {",
            "  rankdir=LR;",
            "  node [shape=box, style=rounded, fontname=Arial];",
            "  edge [color=gray50];",
            "",
        ]

        # –¶–≤–µ—Ç–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        category_colors = {
            "database": "#3498db",
            "analyzer": "#2ecc71",
            "cli": "#e74c3c",
            "models": "#f39c12",
            "config": "#9b59b6",
            "tests": "#95a5a6",
            "legacy": "#7f8c8d",
        }

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∞–π–ª—ã
        files_to_show = (
            focus_files if focus_files else list(self.file_contexts.keys())[:20]
        )

        # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã
        for file_path in files_to_show:
            if file_path not in self.file_contexts:
                continue

            ctx = self.file_contexts[file_path]
            node_name = Path(file_path).name.replace(".", "_").replace("-", "_")
            color = category_colors.get(ctx.category, "#34495e")

            # –†–∞–∑–º–µ—Ä –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
            width = 0.5 + (ctx.priority * 0.2)

            dot_lines.append(
                f'  "{node_name}" [fillcolor="{color}", style=filled, '
                f'width={width:.1f}, tooltip="{ctx.description[:50]}"];'
            )

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∏
        for file_path in files_to_show:
            if file_path not in self.file_contexts:
                continue

            ctx = self.file_contexts[file_path]
            node_name = Path(file_path).name.replace(".", "_").replace("-", "_")

            for dep in ctx.dependencies[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–≤—è–∑–∏
                dep_files = [
                    f
                    for f in files_to_show
                    if dep.lower() in Path(f).name.lower() and f != file_path
                ]

                for dep_file in dep_files[:1]:  # –ú–∞–∫—Å–∏–º—É–º 1 —Å–≤—è–∑—å –Ω–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
                    dep_name = Path(dep_file).name.replace(".", "_").replace("-", "_")
                    dot_lines.append(f'  "{node_name}" -> "{dep_name}";')

        dot_lines.append("}")
        return "\n".join(dot_lines)

    def save_graph(self, focus_category: str | None = None) -> Path:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ –≤ —Ñ–∞–π–ª"""

        focus_files = None
        if focus_category:
            focus_files = [
                path
                for path, ctx in self.file_contexts.items()
                if ctx.category == focus_category
            ]

        dot_content = self.generate_dependency_graph(focus_files)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º DOT —Ñ–∞–π–ª
        suffix = f"_{focus_category}" if focus_category else ""
        dot_file = self.output_dir / f"dependencies{suffix}.dot"
        dot_file.write_text(dot_content, encoding="utf-8")

        print(f"‚úÖ –ì—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {dot_file}")
        print(
            f"üí° –î–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞: dot -Tsvg {dot_file} -o {dot_file.with_suffix('.svg')}"
        )

        return dot_file


class SimpleAPI:
    """–ü—Ä–æ—Å—Ç–æ–π REST API –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å IDE"""

    def __init__(self, context_manager):
        if not API_AVAILABLE:
            print("‚ö†Ô∏è FastAPI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return

        self.context_manager = context_manager
        self.app = FastAPI(title="AI Context Manager API", version="2.0")

        # CORS –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å IDE
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_methods=["*"],
            allow_headers=["*"],
        )

        self._setup_routes()

    def _setup_routes(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ API endpoints"""

        @self.app.get("/health")
        async def health_check():
            return {
                "status": "healthy",
                "files_indexed": len(self.context_manager.file_contexts),
                "ml_available": ML_AVAILABLE,
                "version": "2.0",
            }

        @self.app.post("/context")
        async def generate_context(
            task_type: str = "develop", query: str = "", max_files: int = 15
        ):
            """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–¥–∞—á–∏"""
            context = self.context_manager.generate_ai_context(task_type, query)

            return {
                "relevant_files": context["relevant_files"][:max_files],
                "context_summary": context["context_summary"],
                "suggested_commands": context["suggested_commands"],
                "warnings": context["warnings"],
                "ml_insights": context.get("ml_insights", []),
            }

        @self.app.get("/files")
        async def list_files(category: str | None = None, min_priority: float = 0.0):
            """–°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
            files = []
            for path, ctx in self.context_manager.file_contexts.items():
                if category and ctx.category != category:
                    continue
                if ctx.priority < min_priority:
                    continue

                files.append(
                    {
                        "path": path,
                        "name": Path(path).name,
                        "category": ctx.category,
                        "priority": round(ctx.priority, 2),
                        "description": ctx.description,
                        "git_commits": ctx.git_commits_count,
                    }
                )

            return {"files": files, "total": len(files)}

        @self.app.get("/stats")
        async def get_stats():
            """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
            categories = defaultdict(int)
            total_complexity = 0
            high_priority = 0

            for ctx in self.context_manager.file_contexts.values():
                categories[ctx.category] += 1
                total_complexity += ctx.complexity_score
                if ctx.priority >= 4.0:
                    high_priority += 1

            return {
                "total_files": len(self.context_manager.file_contexts),
                "high_priority_files": high_priority,
                "avg_complexity": round(
                    total_complexity / len(self.context_manager.file_contexts), 1
                ),
                "categories": dict(categories),
            }

        @self.app.get("/visualize/{category}")
        async def visualize_category(category: str):
            """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
            visualizer = DependencyVisualizer(self.context_manager.file_contexts)
            dot_content = visualizer.generate_dependency_graph(
                focus_files=[
                    path
                    for path, ctx in self.context_manager.file_contexts.items()
                    if ctx.category == category
                ]
            )
            return {"dot": dot_content, "category": category}

    def run(self, host: str = "0.0.0.0", port: int = 8000):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç API —Å–µ—Ä–≤–µ—Ä"""
        if not API_AVAILABLE:
            print("‚ùå FastAPI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install fastapi uvicorn")
            return

        print(f"üöÄ API —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ http://{host}:{port}")
        print(f"üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://{host}:{port}/docs")
        print(f"üíö Health check: http://{host}:{port}/health")
        uvicorn.run(self.app, host=host, port=port)


class AIContextManagerPro:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å ML –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è–º–∏"""

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.results_dir = self.project_root / "results"
        self.results_dir.mkdir(exist_ok=True)

        # –§–∞–π–ª—ã –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.context_file = self.results_dir / ".ai_context_pro.json"
        self.cache_file = self.results_dir / ".ai_context_cache.pkl"

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.git_analyzer = GitAnalyzer(self.project_root)
        self.semantic_engine = SemanticSearchEngine()
        self.prioritizer = DynamicPrioritizer(self.git_analyzer)

        # –î–∞–Ω–Ω—ã–µ
        self.file_contexts = {}
        self.cache = self._load_cache()

        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å project analyzer
        self.project_analyzer = None
        self.analyzer_metrics = {}

        # Advanced features
        self.llm_generator = None
        self.visualizer = None
        self.api_server = None

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.load_context()

    def setup_advanced_features(
        self, enable_llm: bool = False, enable_api: bool = False
    ):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"""

        if enable_llm:
            self.llm_generator = LLMDescriptionGenerator()
            print("‚úÖ LLM –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–ø–∏—Å–∞–Ω–∏–π –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

        if enable_api:
            self.api_server = SimpleAPI(self)
            print("‚úÖ REST API –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ç–æ—Ä –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω
        self.visualizer = DependencyVisualizer(self.file_contexts)
        print("‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ç–æ—Ä –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

    def integrate_with_project_analyzer(self) -> bool:
        """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å ai_project_analyzer –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º project analyzer
            import sys

            sys.path.append(str(self.project_root / "scripts" / "tools"))

            try:
                from ai_project_analyzer import ProjectIntelligence

                self.project_analyzer = ProjectIntelligence(str(self.project_root))

                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
                analyzer_results_file = self.results_dir / "project_analysis.json"
                if analyzer_results_file.exists():
                    with open(analyzer_results_file, encoding="utf-8") as f:
                        self.analyzer_metrics = json.load(f)
                else:
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
                    print("üîÑ –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Project Analyzer...")
                    self.analyzer_metrics = self.project_analyzer.analyze_project()

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    with open(analyzer_results_file, "w", encoding="utf-8") as f:
                        json.dump(
                            self.analyzer_metrics, f, indent=2, ensure_ascii=False
                        )

                return True

            except ImportError as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å ProjectIntelligence: {e}")
                return False

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å project analyzer: {e}")
            return False

    def _enhance_context_with_analyzer_data(
        self, file_path: str, context: EnhancedFileContext
    ):
        """–û–±–æ–≥–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ project analyzer"""
        if not self.analyzer_metrics:
            return

        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ñ–∞–π–ª–∞ –∏–∑ project analyzer
        file_metrics = self.analyzer_metrics.get("file_metrics", {})
        if file_path in file_metrics:
            metrics = file_metrics[file_path]

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            if "complexity_score" in metrics:
                context.complexity_score = max(
                    context.complexity_score, metrics["complexity_score"]
                )

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
            duplicates = self.analyzer_metrics.get("duplicates", [])
            for dup in duplicates:
                if file_path in [dup.get("file1"), dup.get("file2")]:
                    context.coupling_score += 1  # –§–∞–π–ª —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –¥—É–±–ª–∏–∫–∞—Ü–∏–∏

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è
        violations = self.analyzer_metrics.get("architecture_violations", [])
        for violation in violations:
            if file_path in violation.get("description", ""):
                context.priority += 0.5  # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–∞–π–ª–æ–≤ —Å –Ω–∞—Ä—É—à–µ–Ω–∏—è–º–∏

    def _generate_enhanced_insights(self, relevant_files: list[str]) -> list[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ project analyzer"""
        insights = []

        if not self.analyzer_metrics:
            return self._generate_ml_insights(relevant_files, "")

        # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
        duplicates = self.analyzer_metrics.get("duplicates", [])
        relevant_duplicates = [
            dup
            for dup in duplicates
            if any(rf in [dup.get("file1"), dup.get("file2")] for rf in relevant_files)
        ]

        if relevant_duplicates:
            insights.append(
                f"üîç –ù–∞–π–¥–µ–Ω–æ {len(relevant_duplicates)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö - "
                f"–≤–æ–∑–º–æ–∂–Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"
            )

        # –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π
        violations = self.analyzer_metrics.get("architecture_violations", [])
        relevant_violations = [
            v
            for v in violations
            if any(rf in v.get("description", "") for rf in relevant_files)
        ]

        if relevant_violations:
            insights.append(
                f"‚ö†Ô∏è {len(relevant_violations)} –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π - "
                f"–ø—Ä–æ–≤–µ—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ PostgreSQL –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ"
            )

        # –ê–Ω–∞–ª–∏–∑ legacy —Ñ–∞–π–ª–æ–≤
        unused_files = self.analyzer_metrics.get("unused_files", [])
        relevant_unused = [f for f in unused_files if f in relevant_files]

        if relevant_unused:
            insights.append(
                f"üóëÔ∏è {len(relevant_unused)} –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ - "
                f"–≤–æ–∑–º–æ–∂–Ω–æ —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫–æ–¥"
            )

        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞
        summary = self.analyzer_metrics.get("summary", {})
        if summary:
            avg_complexity = summary.get("average_complexity", 0)
            if avg_complexity > 10:
                insights.append(
                    f"üî• –í—ã—Å–æ–∫–∞—è —Å—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å ({avg_complexity:.1f}) - "
                    f"—Ä–∞—Å—Å–º–æ—Ç—Ä–∏ —É–ø—Ä–æ—â–µ–Ω–∏–µ"
                )

        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ ML –∏–Ω—Å–∞–π—Ç—ã
        base_insights = self._generate_ml_insights(relevant_files, "")
        insights.extend(base_insights)

        return insights

    def _load_cache(self) -> ContextCache:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–µ—à –∏–∑ —Ñ–∞–π–ª–∞"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, "rb") as f:
                    return pickle.load(f)
            except:
                pass
        return ContextCache()

    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–µ—à –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.cache_file, "wb") as f:
                pickle.dump(self.cache, f)
        except:
            pass

    def load_context(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É—á–µ—Ç–æ–º –∫–µ—à–∞"""
        if self.context_file.exists():
            with open(self.context_file, encoding="utf-8") as f:
                data = json.load(f)
                self.file_contexts = {
                    path: EnhancedFileContext(**ctx) for path, ctx in data.items()
                }
        else:
            self._build_initial_context()

        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        self._update_dynamic_data()

        # –°—Ç—Ä–æ–∏–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å
        self.semantic_engine.build_index(self.file_contexts)

    def _update_dynamic_data(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (git, –º–µ—Ç—Ä–∏–∫–∏ –∏ —Ç.–¥.)"""

        # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–¥–∞–≤–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏–∏
        recent_files = set(self.git_analyzer.get_recent_changes(days=30))

        for path, context in self.file_contexts.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
            file_path = Path(path)
            if not file_path.exists():
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö–µ—à —Ñ–∞–π–ª–∞
            current_hash = self._get_file_hash(file_path)
            cached_hash = self.cache.file_hashes.get(path)

            if current_hash != cached_hash or path in recent_files:
                # –§–∞–π–ª –∏–∑–º–µ–Ω–∏–ª—Å—è, –æ–±–Ω–æ–≤–ª—è–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏

                # Git —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                git_stats = self.git_analyzer.get_file_stats(path)
                context.git_commits_count = git_stats["commits"]
                context.git_last_commit = git_stats["last_commit"]
                context.git_authors = git_stats["authors"]

                # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                context.complexity_score = self._calculate_complexity(file_path)

                # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç–∏
                context.coupling_score = self._calculate_coupling(file_path)

                # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
                self.cache.file_hashes[path] = current_hash

            # –û–±–æ–≥–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ project analyzer
            self._enhance_context_with_analyzer_data(path, context)

            # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            context.priority = self.prioritizer.calculate_priority(context)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–µ—à
        self._save_cache()

    def _get_file_hash(self, file_path: Path) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        try:
            with open(file_path, "rb") as f:
                return hashlib.md5(f.read()).hexdigest()
        except:
            return ""

    def _calculate_complexity(self, file_path: Path) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            tree = ast.parse(content)
            complexity = 0

            for node in ast.walk(tree):
                if isinstance(node, (ast.If, ast.While, ast.For, ast.Try)):
                    complexity += 1
                elif isinstance(node, ast.FunctionDef):
                    complexity += 0.5
                elif isinstance(node, ast.ClassDef):
                    complexity += 2

            return complexity
        except:
            return 0.0

    def _calculate_coupling(self, file_path: Path) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å –º–æ–¥—É–ª—è —Å –¥—Ä—É–≥–∏–º–∏"""
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            tree = ast.parse(content)
            imports = set()

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.add(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.add(node.module)

            # –°—á–∏—Ç–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∏–º–ø–æ—Ä—Ç—ã
            internal_imports = sum(
                1 for imp in imports if not imp.startswith(("python", "sys", "os"))
            )

            return float(internal_imports)
        except:
            return 0.0

    def generate_ai_context(self, task_type: str, query: str = "") -> dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å ML —É–ª—É—á—à–µ–Ω–∏—è–º–∏"""

        # –ë–∞–∑–æ–≤–∞—è —Å–µ–ª–µ–∫—Ü–∏—è —Ñ–∞–π–ª–æ–≤
        relevant_files = self._select_relevant_files_smart(task_type, query)

        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –µ—Å–ª–∏ –µ—Å—Ç—å –∑–∞–ø—Ä–æ—Å
        if query and ML_AVAILABLE:
            semantic_results = self.semantic_engine.search(query, top_k=5)
            for file_path, score in semantic_results:
                if file_path not in relevant_files and score > 0.3:
                    relevant_files.append(file_path)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_summary = self._generate_smart_summary(relevant_files, task_type, query)
        commands = self._suggest_commands_smart(task_type, relevant_files, query)
        warnings = self._generate_warnings_smart(task_type, relevant_files)

        # –î–æ–±–∞–≤–ª—è–µ–º ML insights —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π project analyzer
        if self.analyzer_metrics:
            insights = self._generate_enhanced_insights(relevant_files)
        else:
            insights = self._generate_ml_insights(relevant_files, query)

        return {
            "task_type": task_type,
            "relevant_files": relevant_files[:20],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20
            "context_summary": context_summary,
            "suggested_commands": commands,
            "warnings": warnings,
            "ml_insights": insights,
            "semantic_matches": semantic_results[:3] if query and ML_AVAILABLE else [],
        }

    def _select_relevant_files_smart(self, task_type: str, query: str) -> list[str]:
        """–£–º–Ω–∞—è —Å–µ–ª–µ–∫—Ü–∏—è —Ñ–∞–π–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤"""

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        sorted_files = sorted(
            self.file_contexts.items(), key=lambda x: x[1].priority, reverse=True
        )

        relevant = []

        # –î–æ–±–∞–≤–ª—è–µ–º –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç >= 4)
        for path, ctx in sorted_files:
            if ctx.priority >= 4.0:
                relevant.append(path)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ç–∏–ø—É –∑–∞–¥–∞—á–∏
        task_categories = {
            "debug": ["database", "analyzer", "cli", "tests"],
            "develop": ["analyzer", "cli", "models", "database"],
            "analyze": ["analyzer", "database", "models"],
            "refactor": ["analyzer", "cli", "models", "database", "scripts"],
        }

        for path, ctx in sorted_files:
            if ctx.category in task_categories.get(task_type, []):
                if path not in relevant:
                    relevant.append(path)

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–∞–≤–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        recent_files = self.git_analyzer.get_recent_changes(days=7)
        for file in recent_files[:5]:  # –¢–æ–ø 5 –Ω–µ–¥–∞–≤–Ω–∏—Ö
            full_path = str(self.project_root / file)
            if full_path in self.file_contexts and full_path not in relevant:
                relevant.append(full_path)

        return relevant

    def _generate_smart_summary(
        self, files: list[str], task_type: str, query: str
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–º–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""

        summary_parts = [
            f"üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è: {task_type.upper()}",
            f"üìÅ –§–∞–π–ª–æ–≤: {len(files)}",
        ]

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = defaultdict(list)
        total_complexity = 0
        recent_count = 0

        for file_path in files:
            ctx = self.file_contexts.get(file_path)
            if ctx:
                categories[ctx.category].append(ctx)
                total_complexity += ctx.complexity_score
                if "hour" in ctx.git_last_commit or "day" in ctx.git_last_commit:
                    recent_count += 1

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if recent_count > 0:
            summary_parts.append(f"üî• –ù–µ–¥–∞–≤–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–æ: {recent_count} —Ñ–∞–π–ª–æ–≤")

        avg_complexity = total_complexity / len(files) if files else 0
        if avg_complexity > 50:
            summary_parts.append(
                f"‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ–¥–∞ (avg: {avg_complexity:.1f})"
            )

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
        for category, contexts in categories.items():
            high_priority = sum(1 for ctx in contexts if ctx.priority >= 4.0)
            summary_parts.append(
                f"‚Ä¢ {category}: {len(contexts)} —Ñ–∞–π–ª–æ–≤ ({high_priority} –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö)"
            )

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        if query:
            summary_parts.append(f"\nüîç –ü–æ–∏—Å–∫: '{query[:50]}...'")

        return "\n".join(summary_parts)

    def _suggest_commands_smart(
        self, task_type: str, files: list[str], query: str
    ) -> list[str]:
        """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫–æ–º–∞–Ω–¥—ã —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""

        commands = []

        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã –≤—Å–µ–≥–¥–∞ –ø–æ–ª–µ–∑–Ω—ã
        commands.extend(
            [
                "python main.py --info  # –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã",
                "python main.py --test  # –í–∞–ª–∏–¥–∞—Ü–∏—è",
            ]
        )

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º, –∫–∞–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã
        has_db = any("database" in f for f in files)
        has_analyzer = any("analyzer" in f for f in files)
        has_tests = any("test" in f for f in files)

        if task_type == "debug":
            if has_db:
                commands.append("python check_stats.py  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ë–î")
            if has_analyzer:
                commands.append("python main.py --benchmark  # –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤")
            if not has_tests:
                commands.append("pytest tests/ -v  # –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã")

        elif task_type == "develop":
            if has_analyzer:
                commands.append(
                    f"python main.py --analyze '{query or 'test'}' --analyzer hybrid"
                )
            commands.append(
                "python scripts/tools/ai_project_analyzer.py  # –ê—É–¥–∏—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"
            )

        elif task_type == "analyze":
            commands.append("python scripts/mass_qwen_analysis.py --test")
            if has_db:
                commands.append("python scripts/db_browser.py  # –ë—Ä–∞—É–∑–µ—Ä –ë–î")

        elif task_type == "refactor":
            commands.append("grep -r 'TODO\\|FIXME' src/  # –ù–∞–π—Ç–∏ TODO")
            commands.append("python scripts/tools/ai_project_analyzer.py --duplicates")

        return commands

    def _generate_warnings_smart(self, task_type: str, files: list[str]) -> list[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–º–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"""

        warnings = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ legacy –∫–æ–¥–∞
        legacy_count = sum(
            1
            for f in files
            if "legacy"
            in self.file_contexts.get(
                f, EnhancedFileContext("", 0, "", "", "", 0, [])
            ).category
        )
        if legacy_count > 0:
            warnings.append(
                f"‚ö†Ô∏è {legacy_count} legacy —Ñ–∞–π–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ - –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏"
            )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        high_complexity_files = [
            f
            for f in files
            if self.file_contexts.get(
                f, EnhancedFileContext("", 0, "", "", "", 0, [])
            ).complexity_score
            > 100
        ]
        if high_complexity_files:
            warnings.append(
                f"üî• {len(high_complexity_files)} —Ñ–∞–π–ª–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é - –±—É–¥—å –≤–Ω–∏–º–∞—Ç–µ–ª–µ–Ω"
            )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–¥–∞–≤–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        recent_critical = [
            f
            for f in files
            if self.file_contexts.get(f)
            and self.file_contexts[f].priority >= 4.0
            and "hour" in self.file_contexts[f].git_last_commit
        ]
        if recent_critical:
            warnings.append(
                f"üö® {len(recent_critical)} –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑–º–µ–Ω–µ–Ω—ã –Ω–µ–¥–∞–≤–Ω–æ - –ø—Ä–æ–≤–µ—Ä—å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å"
            )

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        warnings.extend(
            [
                "üîÑ –ò—Å–ø–æ–ª—å–∑—É–π PostgreSQL, –Ω–µ SQLite",
                "üéØ main.py - –µ–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞",
                "üß™ –¢–µ—Å—Ç–∏—Ä—É–π —á–µ—Ä–µ–∑ python main.py --test",
            ]
        )

        return warnings

    def _generate_ml_insights(self, files: list[str], query: str) -> list[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç ML-–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã"""

        insights = []

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if len(files) > 10:
            # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏
            most_coupled = sorted(
                [
                    (f, self.file_contexts[f].coupling_score)
                    for f in files
                    if f in self.file_contexts
                ],
                key=lambda x: x[1],
                reverse=True,
            )[:3]

            if most_coupled and most_coupled[0][1] > 10:
                insights.append(
                    f"üîó –í—ã—Å–æ–∫–∞—è —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å: {Path(most_coupled[0][0]).name} "
                    f"–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç {int(most_coupled[0][1])} –º–æ–¥—É–ª–µ–π"
                )

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º git –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        active_authors = set()
        for f in files:
            if f in self.file_contexts:
                active_authors.update(self.file_contexts[f].git_authors)

        if len(active_authors) > 3:
            insights.append(
                f"üë• –ù–∞–¥ —ç—Ç–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏ —Ä–∞–±–æ—Ç–∞–ª–∏ {len(active_authors)} —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤"
            )

        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã
        if query and ML_AVAILABLE:
            insights.append(f"üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∞–∫—Ç–∏–≤–µ–Ω –¥–ª—è '{query[:30]}...'")

        return insights

    def save_context(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        data = {path: asdict(ctx) for path, ctx in self.file_contexts.items()}
        with open(self.context_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def _build_initial_context(self):
        """–°—Ç—Ä–æ–∏—Ç –Ω–∞—á–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""

        # –°–∫–∞–Ω–∏—Ä—É–µ–º –ø—Ä–æ–µ–∫—Ç –±–æ–ª–µ–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ
        python_files = list(self.project_root.rglob("*.py"))
        config_files = list(self.project_root.glob("*.yaml")) + list(
            self.project_root.glob("*.yml")
        )
        doc_files = list(self.project_root.glob("docs/*.md"))

        for file_path in python_files + config_files + doc_files:
            if self._should_skip(file_path):
                continue

            category = self._determine_category(file_path)

            context = EnhancedFileContext(
                path=str(file_path),
                priority=2.5,  # –ù–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                category=category,
                description=self._generate_description(file_path),
                last_modified="",
                size_lines=self._count_lines(file_path),
                dependencies=self._extract_dependencies(file_path),
            )

            self.file_contexts[str(file_path)] = context

        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
        self._update_dynamic_data()
        self.save_context()

    def _should_skip(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∞–π–ª"""
        skip_patterns = [
            "__pycache__",
            ".git",
            "venv",
            ".venv",
            "node_modules",
            ".pytest_cache",
            "*.pyc",
        ]
        return any(pattern in str(file_path) for pattern in skip_patterns)

    def _determine_category(self, file_path: Path) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ñ–∞–π–ª–∞"""
        path_str = str(file_path).lower()

        if "database" in path_str or "postgres" in path_str:
            return "database"
        if "analyzer" in path_str:
            return "analyzer"
        if "cli" in path_str or file_path.name == "main.py":
            return "cli"
        if "model" in path_str:
            return "models"
        if "test" in path_str:
            return "tests"
        if "archive" in path_str or "sqlite" in path_str:
            return "legacy"
        if "script" in path_str:
            return "scripts"
        if file_path.suffix in [".yaml", ".yml", ".json", ".env"]:
            return "config"
        if file_path.suffix == ".md":
            return "docs"
        return "other"

    def _generate_description(self, file_path: Path) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, encoding="utf-8") as f:
                first_lines = f.read(500)

            # –ò—â–µ–º docstring –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            if '"""' in first_lines:
                docstring = (
                    first_lines.split('"""')[1]
                    if len(first_lines.split('"""')) > 1
                    else ""
                )
                return docstring.strip()[:100]
            if "#" in first_lines:
                comment = first_lines.split("\n")[0].replace("#", "").strip()
                return comment[:100]
        except:
            pass

        return f"–§–∞–π–ª {file_path.name}"

    def _count_lines(self, file_path: Path) -> int:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ"""
        try:
            with open(file_path, encoding="utf-8") as f:
                return len(f.readlines())
        except:
            return 0

    def _extract_dependencies(self, file_path: Path) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        if file_path.suffix != ".py":
            return []

        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            tree = ast.parse(content)
            imports = set()

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.add(alias.name.split(".")[0])
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.add(node.module.split(".")[0])

            return list(imports)
        except:
            return []

    def generate_llm_descriptions(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è LLM –æ–ø–∏—Å–∞–Ω–∏–π –¥–ª—è —Ñ–∞–π–ª–æ–≤"""

        if not self.llm_generator:
            print("‚ùå LLM –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --llm-descriptions")
            return

        print("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º AI –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è —Ñ–∞–π–ª–æ–≤...")
        descriptions = self.llm_generator.generate_descriptions(self.file_contexts)
        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–π: {len(descriptions)}")

    def create_dependency_graph(self, output_path: str = "dependency_graph.dot"):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""

        if not self.visualizer:
            self.visualizer = DependencyVisualizer(self.file_contexts)

        graph_content = self.visualizer.generate_dependency_graph()
        output_file = self.visualizer.save_graph()
        print(f"üìä –ì—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–æ–∑–¥–∞–Ω: {output_file}")
        return output_file

    def start_api_server(self, host: str = "127.0.0.1", port: int = 8000):
        """–ó–∞–ø—É—Å–∫ REST API —Å–µ—Ä–≤–µ—Ä–∞"""

        if not self.api_server:
            print("‚ùå API —Å–µ—Ä–≤–µ—Ä –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --api")
            return

        print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º API —Å–µ—Ä–≤–µ—Ä –Ω–∞ http://{host}:{port}")
        self.api_server.run(host, port)


def auto_detect_task_type(query: str) -> str:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–¥–∞—á–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
    query_lower = query.lower()

    debug_keywords = [
        "error",
        "bug",
        "fix",
        "broken",
        "crash",
        "timeout",
        "exception",
        "fail",
    ]
    develop_keywords = ["add", "create", "implement", "feature", "new", "build"]
    analyze_keywords = ["analyze", "stats", "metrics", "report", "check", "examine"]
    refactor_keywords = ["refactor", "optimize", "improve", "clean", "restructure"]

    if any(kw in query_lower for kw in debug_keywords):
        return "debug"
    if any(kw in query_lower for kw in develop_keywords):
        return "develop"
    if any(kw in query_lower for kw in analyze_keywords):
        return "analyze"
    if any(kw in query_lower for kw in refactor_keywords):
        return "refactor"
    return "debug"  # Default


def interactive_mode():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    print("ü§ñ AI Context Manager PRO - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º")
    print("=" * 50)

    manager = AIContextManagerPro()

    while True:
        print("\nüéØ –í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:")
        print("1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É")
        print("2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤")
        print("3. –ê–Ω–∞–ª–∏–∑ git –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏")
        print("4. –û–±–Ω–æ–≤–∏—Ç—å –∫–µ—à")
        print("5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞")
        print("6. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Project Analyzer")
        print("0. –í—ã—Ö–æ–¥")

        choice = input("\n–í—ã–±–æ—Ä: ").strip()

        if choice == "0":
            break
        if choice == "1":
            query = input("üîç –û–ø–∏—à–∏—Ç–µ –∑–∞–¥–∞—á—É: ").strip()
            if query:
                task_type = auto_detect_task_type(query)
                print(f"üìù –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ç–∏–ø –∑–∞–¥–∞—á–∏: {task_type.upper()}")

                context = manager.generate_ai_context(task_type, query)
                print_context_pretty(context)

                export = input("\nüíæ –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ñ–∞–π–ª? (y/N): ").strip().lower()
                if export == "y":
                    export_context(
                        context,
                        f"context_{task_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    )

        elif choice == "2":
            if not ML_AVAILABLE:
                print("‚ùå –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ scikit-learn")
                continue

            query = input("üß† –ó–∞–ø—Ä–æ—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: ").strip()
            if query:
                results = manager.semantic_engine.search(query, top_k=10)
                print(f"\nüîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:")
                for i, (path, score) in enumerate(results, 1):
                    filename = Path(path).name
                    print(f"{i}. {filename} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f})")

        elif choice == "3":
            recent_files = manager.git_analyzer.get_recent_changes(days=7)
            print(f"\nüìÖ –ù–µ–¥–∞–≤–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (7 –¥–Ω–µ–π): {len(recent_files)}")
            for i, file in enumerate(recent_files[:10], 1):
                print(f"{i}. {file}")

        elif choice == "4":
            print("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–µ—à–∞...")
            manager._update_dynamic_data()
            manager.save_context()
            print("‚úÖ –ö–µ—à –æ–±–Ω–æ–≤–ª–µ–Ω!")

        elif choice == "5":
            print_project_stats(manager)

        elif choice == "6":
            print("üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Project Analyzer...")
            success = manager.integrate_with_project_analyzer()
            if success:
                print(
                    "‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞! –î–∞–Ω–Ω—ã–µ Project Analyzer –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç"
                )
                manager._update_dynamic_data()
                manager.save_context()
            else:
                print(
                    "‚ùå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ ai_project_analyzer.py"
                )


def print_context_pretty(context: dict):
    """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    print("\n" + "=" * 60)
    print(f"üéØ {context['task_type'].upper()} –ö–û–ù–¢–ï–ö–°–¢")
    print("=" * 60)

    print(f"\nüìã {context['context_summary']}")

    print(f"\nüìÅ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã ({len(context['relevant_files'])}):")
    for i, file_path in enumerate(context["relevant_files"][:15], 1):
        filename = Path(file_path).name
        print(f"  {i:2d}. {filename}")

    if context.get("semantic_matches"):
        print("\nüß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è:")
        for path, score in context["semantic_matches"]:
            filename = Path(path).name
            print(f"  ‚Ä¢ {filename} (relevance: {score:.3f})")

    print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫–æ–º–∞–Ω–¥—ã:")
    for i, cmd in enumerate(context["suggested_commands"][:5], 1):
        print(f"  {i}. {cmd}")

    print("\n‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
    for warning in context["warnings"][:3]:
        print(f"  ‚Ä¢ {warning}")

    if context.get("ml_insights"):
        print("\nüöÄ ML –ò–Ω—Å–∞–π—Ç—ã:")
        for insight in context["ml_insights"]:
            print(f"  ‚Ä¢ {insight}")


def print_project_stats(manager: AIContextManagerPro):
    """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–µ–∫—Ç–∞"""
    print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–ï–ö–¢–ê")
    print("=" * 40)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    categories = defaultdict(list)
    total_complexity = 0
    high_priority_count = 0
    recent_count = 0

    for path, ctx in manager.file_contexts.items():
        categories[ctx.category].append(ctx)
        total_complexity += ctx.complexity_score
        if ctx.priority >= 4.0:
            high_priority_count += 1
        if "hour" in ctx.git_last_commit or "day" in ctx.git_last_commit:
            recent_count += 1

    print(f"üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(manager.file_contexts)}")
    print(f"üî• –ö—Ä–∏—Ç–∏—á–Ω—ã—Ö (priority >= 4): {high_priority_count}")
    print(f"‚è∞ –ù–µ–¥–∞–≤–Ω–æ –∏–∑–º–µ–Ω–µ–Ω—ã: {recent_count}")
    print(f"üßÆ –°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {total_complexity / len(manager.file_contexts):.1f}")

    print("\nüìÇ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    for category, contexts in categories.items():
        avg_priority = sum(ctx.priority for ctx in contexts) / len(contexts)
        print(
            f"  ‚Ä¢ {category}: {len(contexts)} —Ñ–∞–π–ª–æ–≤ (avg priority: {avg_priority:.1f})"
        )


def export_context(context: dict, filename: str):
    """–≠–∫—Å–ø–æ—Ä—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ñ–∞–π–ª"""
    try:
        output_path = Path("results") / filename
        output_path.parent.mkdir(exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(context, f, indent=2, ensure_ascii=False)

        print(f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {output_path}")

        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –±—É—Ñ–µ—Ä –æ–±–º–µ–Ω–∞
        try:
            import pyperclip

            context_text = json.dumps(context, indent=2, ensure_ascii=False)
            pyperclip.copy(context_text)
            print("üìã –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –≤ –±—É—Ñ–µ—Ä –æ–±–º–µ–Ω–∞")
        except ImportError:
            pass

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="ü§ñ AI Context Manager PRO - –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å ML",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
üéØ –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø:

# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
python scripts/tools/ai_context_manager_pro.py --interactive

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏
python scripts/tools/ai_context_manager_pro.py --query "fix database connection error"

# –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –∑–∞–¥–∞—á–∞
python scripts/tools/ai_context_manager_pro.py --task debug --query "postgres timeout"

# –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
python scripts/tools/ai_context_manager_pro.py --semantic-search "analyzer performance"

# –û–±–Ω–æ–≤–∏—Ç—å –∫–µ—à
python scripts/tools/ai_context_manager_pro.py --update-cache

# –≠–∫—Å–ø–æ—Ä—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
python scripts/tools/ai_context_manager_pro.py --task develop --export context.json

üöÄ –ù–û–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò PRO:
‚Ä¢ Git-based –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤
‚Ä¢ ML —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å TF-IDF
‚Ä¢ –£–º–Ω–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
‚Ä¢ –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
‚Ä¢ –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        """,
    )

    parser.add_argument(
        "--interactive",
        "-i",
        action="store_true",
        help="–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Å CLI –º–µ–Ω—é",
    )
    parser.add_argument(
        "--task",
        "-t",
        choices=["debug", "develop", "analyze", "refactor"],
        help="–¢–∏–ø –∑–∞–¥–∞—á–∏",
    )
    parser.add_argument(
        "--query", "-q", type=str, help="–û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞"
    )
    parser.add_argument(
        "--semantic-search", "-s", type=str, help="–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Ñ–∞–π–ª–∞–º"
    )
    parser.add_argument(
        "--update-cache",
        "-u",
        action="store_true",
        help="–û–±–Ω–æ–≤–∏—Ç—å –∫–µ—à –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
    )
    parser.add_argument("--export", "-e", type=str, help="–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Ñ–∞–π–ª")
    parser.add_argument(
        "--integrate", action="store_true", help="–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å ai_project_analyzer"
    )
    parser.add_argument(
        "--stats", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–µ–∫—Ç–∞"
    )

    # Advanced features
    parser.add_argument(
        "--llm-descriptions",
        action="store_true",
        help="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è AI –æ–ø–∏—Å–∞–Ω–∏–π —á–µ—Ä–µ–∑ LLM (Ollama)",
    )
    parser.add_argument(
        "--visualize", "-v", action="store_true", help="–°–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
    )
    parser.add_argument("--api", action="store_true", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å REST API —Å–µ—Ä–≤–µ—Ä")
    parser.add_argument(
        "--api-host",
        default="127.0.0.1",
        help="–•–æ—Å—Ç –¥–ª—è API —Å–µ—Ä–≤–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 127.0.0.1)",
    )
    parser.add_argument(
        "--api-port",
        type=int,
        default=8000,
        help="–ü–æ—Ä—Ç –¥–ª—è API —Å–µ—Ä–≤–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8000)",
    )

    args = parser.parse_args()

    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    if args.interactive:
        interactive_mode()
        exit()

    # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä
    try:
        manager = AIContextManagerPro()
        print("ü§ñ AI Context Manager PRO –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        if ML_AVAILABLE:
            print("‚úÖ ML –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—ã (scikit-learn)")
        else:
            print("‚ö†Ô∏è ML –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        exit(1)

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–µ—à–∞
    if args.update_cache:
        print("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–µ—à–∞...")
        manager._update_dynamic_data()
        manager.save_context()
        print("‚úÖ –ö–µ—à –æ–±–Ω–æ–≤–ª–µ–Ω!")
        exit()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞
    if args.stats:
        print_project_stats(manager)
        exit()

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    if args.semantic_search:
        if not ML_AVAILABLE:
            print("‚ùå –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ scikit-learn")
            exit(1)

        print(f"üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: '{args.semantic_search}'")
        results = manager.semantic_engine.search(args.semantic_search, top_k=10)

        if results:
            print(f"\nüîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:")
            for i, (path, score) in enumerate(results, 1):
                filename = Path(path).name
                print(f"  {i:2d}. {filename} (relevance: {score:.3f})")
        else:
            print("ü§∑ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        exit()

    # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å project analyzer
    if args.integrate:
        print("üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Project Analyzer...")
        success = manager.integrate_with_project_analyzer()
        if success:
            print("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞!")
            manager._update_dynamic_data()
            manager.save_context()
        else:
            print("‚ùå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å")
        exit()

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
    if args.llm_descriptions or args.api:
        enable_llm = args.llm_descriptions
        enable_api = args.api
        manager.setup_advanced_features(enable_llm=enable_llm, enable_api=enable_api)

    # LLM –æ–ø–∏—Å–∞–Ω–∏—è
    if args.llm_descriptions:
        print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è AI –æ–ø–∏—Å–∞–Ω–∏–π...")
        manager.generate_llm_descriptions()
        exit()

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    if args.visualize:
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
        output_file = manager.create_dependency_graph()
        print(f"üìÅ –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        exit()

    # –ó–∞–ø—É—Å–∫ API —Å–µ—Ä–≤–µ—Ä–∞
    if args.api:
        print("üöÄ –ó–∞–ø—É—Å–∫ API —Å–µ—Ä–≤–µ—Ä–∞...")
        manager.start_api_server(args.api_host, args.api_port)
        exit()

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    if args.query:
        task_type = args.task or auto_detect_task_type(args.query)
        print(f"üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è: {task_type.upper()}")
        print(f"üîç –ó–∞–ø—Ä–æ—Å: '{args.query}'")

        context = manager.generate_ai_context(task_type, args.query)
        print_context_pretty(context)

        # –≠–∫—Å–ø–æ—Ä—Ç
        if args.export:
            export_context(context, args.export)

    elif args.task:
        print(f"üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è: {args.task.upper()}")
        context = manager.generate_ai_context(args.task, "")
        print_context_pretty(context)

        # –≠–∫—Å–ø–æ—Ä—Ç
        if args.export:
            export_context(context, args.export)

    else:
        print("ü§ñ AI Context Manager PRO")
        print(
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --help –¥–ª—è —Å–ø–∏—Å–∫–∞ –∫–æ–º–∞–Ω–¥ –∏–ª–∏ --interactive –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞"
        )
        print("\nüöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç:")
        print("  python scripts/tools/ai_context_manager_pro.py --interactive")
